<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>networkop on networkop</title>
    <link>https://networkop.co.uk/</link>
    <description>Recent content in networkop on networkop</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Michael Kashin 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The problem of unpredictable interface order in multi-network Docker containers</title>
      <link>https://networkop.co.uk/post/2018-03-03-docker-multinet/</link>
      <pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2018-03-03-docker-multinet/</guid>
      <description>

&lt;p&gt;Whether we like it or not, the era of DevOps is upon us, fellow network engineers, and with it come opportunities to approach and solve common networking problems
in new, innovative ways. One such problem is automated network change validation and testing in virtual environments, something I&amp;rsquo;ve already &lt;a href=&#34;https://networkop.co.uk/blog/2016/02/19/network-ci-intro/&#34;&gt;written about&lt;/a&gt; a few years ago. The biggest problem with my original approach was that I had to create a custom &lt;a href=&#34;https://networkop.co.uk/blog/2016/01/01/rest-for-neteng/&#34;&gt;REST API SDK&lt;/a&gt; to work with a network simulation environment (UnetLab) that was never designed to be interacted with in a programmatic way. On the other hand, technologies like Docker have been very interesting since they were built around the idea of non-interactive lifecycle management and came with all &lt;a href=&#34;http://docker-py.readthedocs.io/en/stable/containers.html&#34; target=&#34;_blank&#34;&gt;API batteries&lt;/a&gt; already included. However, Docker was never intended to be used for network simulations and its support for multiple network interfaces is&amp;hellip; somewhat problematic.&lt;/p&gt;

&lt;h1 id=&#34;problem-demonstration&#34;&gt;Problem demonstration&lt;/h1&gt;

&lt;p&gt;The easiest way to understand the problem is to see it. Let&amp;rsquo;s start with a blank Docker host and create a few networks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker network create net1
docker network create net2
docker network create net3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s see what prefixes have been allocated to those networks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker network inspect -f &amp;quot;{{range .IPAM.Config }}{{.Subnet}}{{end}}&amp;quot; net1
172.17.0.0/16
docker network inspect -f &amp;quot;{{range .IPAM.Config }}{{.Subnet}}{{end}}&amp;quot; net2
172.18.0.0/16
docker network inspect -f &amp;quot;{{range .IPAM.Config }}{{.Subnet}}{{end}}&amp;quot; net3
172.19.0.0/16
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, let&amp;rsquo;s create a container and attach it to these networks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker create --name test -it alpine sh
docker network connect net1 test
docker network connect net2 test
docker network connect net3 test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now obviously you would expect for networks to appear in the same order as they were attached, right? Let&amp;rsquo;s see if it&amp;rsquo;s true:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker start test
docker exec -it test sh -c &amp;quot;ip a | grep &#39;inet&#39;&amp;quot;
inet 127.0.0.1/8 scope host lo
inet 172.26.0.2/16 brd 172.26.255.255 scope global eth0
inet 172.17.0.2/16 brd 172.17.255.255 scope global eth1
inet 172.18.0.2/16 brd 172.18.255.255 scope global eth2
inet 172.19.0.2/16 brd 172.19.255.255 scope global eth3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks good so far. The first interface (172.26.0.2/16) is the docker bridge that was attached by default in &lt;code&gt;docker create&lt;/code&gt; command. Now let&amp;rsquo;s add another network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker network create net4
docker stop test
docker network connect net4 test
docker start test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s examine our interfaces again:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker exec -it test sh -c &amp;quot;ip a | egrep &#39;eth\d|inet&#39;&amp;quot;
inet 127.0.0.1/8 scope host lo
inet 172.26.0.2/16 brd 172.26.255.255 scope global eth0
inet 172.20.0.2/16 brd 172.20.255.255 scope global eth3
inet 172.17.0.2/16 brd 172.17.255.255 scope global eth2
inet 172.18.0.2/16 brd 172.18.255.255 scope global eth1
inet 172.19.0.2/16 brd 172.19.255.255 scope global eth4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;re seeing that networks are in a completely different order. Looks like net1 is connected to eth2, net2 to eth1, net3 to eth4 and net4 to eth3. In fact, this issue should manifest itself even with 2 or 3 networks, however, I&amp;rsquo;ve found that it doesn&amp;rsquo;t always reorder them in that case.&lt;/p&gt;

&lt;h1 id=&#34;cnm-and-libnetwork-architecture&#34;&gt;CNM and libnetwork architecture&lt;/h1&gt;

&lt;p&gt;In order to better understand the issue, it help to know the CNM terminology and network lifecycle events which are explained in libnetwork&amp;rsquo;s &lt;a href=&#34;https://github.com/docker/libnetwork/blob/master/docs/design.md&#34; target=&#34;_blank&#34;&gt;design document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/docker/libnetwork/raw/master/docs/cnm-model.jpg?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each time we run a &lt;code&gt;docker network create&lt;/code&gt; command a new &lt;strong&gt;CNM network&lt;/strong&gt; object is created. This object has a specific network type (&lt;code&gt;bridge&lt;/code&gt; by default) which identifies the driver to be used for the actual network implementation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;network, err := controller.NewNetwork(&amp;quot;bridge&amp;quot;, &amp;quot;net1&amp;quot;, &amp;quot;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When container gets attached to its networks, first time in &lt;code&gt;docker create&lt;/code&gt; and subsequently in &lt;code&gt;docket network connect&lt;/code&gt; commands, an &lt;strong&gt;endpoint object&lt;/strong&gt; is created on each of the networks being connected. This endpoint object represents container&amp;rsquo;s point of attachment (similar to a switch port) to docker networks and may allocate IP settings for a future network interface.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;ep, err := network.CreateEndpoint(&amp;quot;ep1&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At the time when container gets attached to its first network, a &lt;strong&gt;sandbox object&lt;/strong&gt; is created. This object represents a container inside CNM object model and stores pointers to all attached network endpoints.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;sbx, err := controller.NewSandbox(&amp;quot;test&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, when we start a container using &lt;code&gt;docker start&lt;/code&gt; command, the corresponding &lt;strong&gt;sandbox gets attached&lt;/strong&gt; to all associated network endpoints using the &lt;code&gt;ep.Join(sandbox)&lt;/code&gt; call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;for _, ep := range epList {
	if err := ep.Join(sb); err != nil {
		logrus.Warnf(&amp;quot;Failed attach sandbox %s to endpoint %s: %v\n&amp;quot;, sb.ID(), ep.ID(), err)
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;going-down-the-rabbit-hole&#34;&gt;Going down the rabbit hole&lt;/h1&gt;

&lt;p&gt;Looking at the above snippet from &lt;code&gt;sandbox.go&lt;/code&gt;, we can assume that the order in which networks will be attached to a container will depend on the order of elements inside the &lt;code&gt;epList&lt;/code&gt; array, which gets built earlier in the function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;epList := sb.getConnectedEndpoints()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s see what happens inside that method call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (sb *sandbox) getConnectedEndpoints() []*endpoint {
	sb.Lock()
	defer sb.Unlock()

	eps := make([]*endpoint, len(sb.endpoints))
	for i, ep := range sb.endpoints {
		eps[i] = ep
	}

	return eps
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So &lt;code&gt;epList&lt;/code&gt; is just an array of endpoints that gets built by copying values from &lt;code&gt;sb.endoints&lt;/code&gt;, which itself is an attribute (or field) inside the &lt;code&gt;sb&lt;/code&gt; struct.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type epHeap []*endpoint

type sandbox struct {
  id                 string
  containerID        string
...
  endpoints          epHeap
...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point it looks like &lt;code&gt;endpoints&lt;/code&gt; is just an array of pointers to endpoint objects, which still doesn&amp;rsquo;t explain the issue we&amp;rsquo;re investigating. Perhaps it would make more sense if we saw how a sandbox object gets created.&lt;/p&gt;

&lt;p&gt;Since sandbox object gets created by calling &lt;code&gt;controller.NewSandbox()&lt;/code&gt; method, let&amp;rsquo;s see exactly how this is done by looking at the code inside the &lt;code&gt;controller.go&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (c *controller) NewSandbox(containerID string, options ...SandboxOption) (Sandbox, error) {
...
  // Create sandbox and process options first. Key generation depends on an option
  if sb == nil {
    sb = &amp;amp;sandbox{
      id:                 sandboxID,
      containerID:        containerID,
      endpoints:          epHeap{},
...
    }
  }

  heap.Init(&amp;amp;sb.endpoints)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last statement explains why sandbox connects networks in random order. The &lt;code&gt;endpoints&lt;/code&gt; array is, in fact, a &lt;a href=&#34;https://golang.org/pkg/container/heap/&#34; target=&#34;_blank&#34;&gt;heap&lt;/a&gt; - an ordered tree, where parent node is always smaller than (or equal to) its children (minheap). Heap is used to implement a priority queue, which should be familiar to every network engineer who knows QoS. One of heap&amp;rsquo;s properties is that it re-orders elements every time an element gets added or removed, in order to maintain the heap invariant (parent &amp;lt;= child).&lt;/p&gt;

&lt;h1 id=&#34;problem-solution&#34;&gt;Problem solution&lt;/h1&gt;

&lt;p&gt;It turns out the problem demonstrated above is a very well-known problem with multiple opened issues on Github [&lt;a href=&#34;https://github.com/moby/moby/issues/25181&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;,&lt;a href=&#34;https://github.com/moby/moby/issues/23742&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;,&lt;a href=&#34;https://github.com/moby/moby/issues/35221&#34; target=&#34;_blank&#34;&gt;3&lt;/a&gt;]. I was lucky enough to have discovered this problem right after &lt;a href=&#34;https://github.com/docker/libnetwork/issues/2093&#34; target=&#34;_blank&#34;&gt;this pull request&lt;/a&gt; got submitted, which is what helped me understand what the issue was in the first place. This pull request reference a &lt;a href=&#34;https://github.com/cziebuhr/libnetwork/commit/d047825d4d156bc4cf01bfe410cb61b3bc33f572&#34; target=&#34;_blank&#34;&gt;patch&lt;/a&gt; that swaps the heapified array with a normal one. Below I&amp;rsquo;ll show how to build a custom docker daemon binary using this patch. We&amp;rsquo;ll start with a privileged centos-based Docker container:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --privileged -it centos bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inside this container we need to install all the dependencies along with Docker. Yes, you need Docker to build Docker:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yum install -y git iptables \
            make &amp;quot;Development Tools&amp;quot; \
            yum-utils device-mapper-persistent-data \
            lvm2

yum-config-manager --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

yum install docker-ce -y

# Start docker in the background
/usr/bin/dockerd &amp;gt;/dev/null &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next let&amp;rsquo;s clone the Docker master branch and the patched fork of libnetwork:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --depth=1 https://github.com/docker/docker.git /tmp/docker-repo
git clone https://github.com/cziebuhr/libnetwork.git /tmp/libnetwork-patch
cd /tmp/libnetwork-patch
git checkout d047825d4d156bc4cf01bfe410cb61b3bc33f572
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I tried using &lt;a href=&#34;https://github.com/LK4D4/vndr&#34; target=&#34;_blank&#34;&gt;VNDR&lt;/a&gt; to update the libnetwork files inside the Docker repository, however I ran into problems with incompatible git options on CentOS. So instead I&amp;rsquo;ll update libnetwork manually, with just the files that are different from the original repo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /tmp/libnetwork-patch
/usr/bin/cp controller.go endpoint.go sandbox.go sandbox_store.go /tmp/docker-repo/vendor/github.com/docker/libnetwork/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Final step is to build docker binaries. This step may require up to 100G of free disk space and may take up to 60 minutes depending on your network speed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /tmp/docker-repo
make build
make binary
...
Created binary: bundles/binary-daemon/dockerd-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;verification&#34;&gt;Verification&lt;/h1&gt;

&lt;p&gt;Once done, we can retrieve the binaries outside of the build container:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;find /var/lib/docker -name dockerd
/var/lib/docker/overlay2/ac310ef5172acac7e8cb748092a9c9d1ddc3c25a91e636ab581cfde0869f5d76/diff/tmp/docker-repo/bundles/binary-daemon/dockerd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can swap the current docker daemon with the patched one:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yum install which -y
systemctl stop docker.service
DOCKERD=$(which dockerd)
mv $DOCKERD $DOCKERD-old
cp /tmp/docker-repo/bundles/latest/binary-daemon/dockerd $DOCKERD
systemctl start docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we re-run our tests now, the interfaces are returned in the same exact order they were added:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker start test
docker exec -it test sh -c &amp;quot;ip a | grep &#39;inet&#39;&amp;quot;
inet 127.0.0.1/8 scope host lo
inet 172.26.0.2/16 brd 172.26.255.255 scope global eth0
inet 172.17.0.2/16 brd 172.17.255.255 scope global eth1
inet 172.18.0.2/16 brd 172.18.255.255 scope global eth2
inet 172.19.0.2/16 brd 172.19.255.255 scope global eth3
inet 172.20.0.2/16 brd 172.20.255.255 scope global eth4
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Huge kudos to the original &lt;a href=&#34;https://github.com/cziebuhr&#34; target=&#34;_blank&#34;&gt;author&lt;/a&gt; of the &lt;a href=&#34;https://github.com/cziebuhr/libnetwork/commit/d047825d4d156bc4cf01bfe410cb61b3bc33f572&#34; target=&#34;_blank&#34;&gt;libnetwork patch&lt;/a&gt; which is the sole reason this blogpost exists. I really hope that this issue will get resolved, in this form or another (could it be possible to keep track of the order in which endpoints are added to a sandbox and use that as a criteria for heap sort?), as this will make automated network testing much more approachable.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN</title>
      <link>https://networkop.co.uk/tags/openstack-sdn/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/tags/openstack-sdn/</guid>
      <description>

&lt;h2 id=&#34;openstack-sdn-tags-openstack-sdn-learn-through-hands-on-experience-everything-you-need-to-know-about-vanilla-openstack-neutron-implementation-of-virtual-networks-including-custom-sdn-controllers-like-ovn-opendaylight-and-opencontrail&#34;&gt;&lt;a href=&#34;https://networkop.co.uk/tags/openstack-sdn/&#34;&gt;OpenStack SDN&lt;/a&gt; - Learn through hands-on experience everything you need to know about vanilla OpenStack Neutron implementation of virtual networks, including custom SDN controllers like OVN, OpenDaylight and OpenContrail&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN - OpenContrail With BGP VPN</title>
      <link>https://networkop.co.uk/blog/2018/01/02/os-contrail/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2018/01/02/os-contrail/</guid>
      <description>

&lt;p&gt;Continuing on the trend started in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/12/15/os-odl-netvirt/&#34;&gt;previous post about OpenDaylight&lt;/a&gt;, I&amp;rsquo;ll move on to the next open-source product that uses BGP VPNs for optimal North-South traffic forwarding. OpenContrail is one of the most popular SDN solutions for OpenStack. It was one of the first hybrid SDN solutions, offering both pure overlay and overlay/underlay integration. It is the default SDN platform of choice for Mirantis Cloud Platform, it has multiple large-scale deployments in companies like Workday and AT&amp;amp;T. I, personally, don&amp;rsquo;t have any production experience with OpenContrail, however my impression, based on what I&amp;rsquo;ve heard and seen in the last 2-3 years that I&amp;rsquo;ve been following Telco SDN space, is that OpenContrail is the most mature SDN platform for Telco NFVs not least because of its unique feature set.&lt;/p&gt;

&lt;p&gt;During the time of production deployment at AT&amp;amp;T, Contrail has added a lot of features required by Telco NFVs like QoS, VLAN trunking and BGP-as-a-service. My first acquaintance with BGPaaS took place when I started working on Telco DCs and I remember being genuinely shocked when I first saw the requirement for dynamic routing exchange with VNFs. To me this seemed to break one of the main rules of cloud networking - a VM is not to have any knowledge or interaction with the underlay. I gradually went though all stages of grief, all the way to acceptance and although it still feels &amp;ldquo;wrong&amp;rdquo; now, I can at least understand why it&amp;rsquo;s needed and what are the pros/cons of different BGPaaS solutions.&lt;/p&gt;

&lt;h1 id=&#34;bgp-as-a-service&#34;&gt;BGP-as-a-Service&lt;/h1&gt;

&lt;p&gt;There&amp;rsquo;s a certain range of VNFs that may require to advertise a set of IP addresses into the existing VPNs inside Telco network. The most notable example is PGW inside EPC. I won&amp;rsquo;t pretend to be an expert in this field, but based on my limited understanding PGW needs to advertise IP networks into various customer VPNs, for example to connect private APNs to existing customer L3VPNs. Obviously, when this kind of network function gets virtualised, it still retains this requirement which now needs to be fulfilled by DC SDN.&lt;/p&gt;

&lt;p&gt;This requirement catches a lot of big SDN vendors off guard and the best they come up with is connecting those VNFs, through VLANs, directly to underlay TOR switches. Although this solution is easy to implement, it has an incredible amount of drawbacks since a single VNF can now affect the stability of the whole POD or even the whole DC network. Some VNFs vendors also require BFD to monitor liveliness of those BGP sessions which, in case a L3 boundary is higher than the TOR, may create even a bigger number of issues on a POD spine.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a small range of SDN platforms that run a full routing stack on each compute node (e.g. Cumulus, Calico). These solutions are the best fit for this kind of scenarios since they allow BGP sessions to be established over a single hop (VNF &amp;lt;-&amp;gt; virtual switch). However they represent a small fraction of total SDN solutions space with majority of vendors implementing a much simpler OpenFlow or XMPP-based flow push model.&lt;/p&gt;

&lt;p&gt;OpenContrail, as far as I know, is the only SDN controller that doesn&amp;rsquo;t run a full routing stack on compute nodes but still fulfills this requirement in a very elegant way. When &lt;a href=&#34;https://www.juniper.net/documentation/en_US/contrail3.2/topics/concept/bgp-as-a-service-overview.html&#34; target=&#34;_blank&#34;&gt;BGPaaS&lt;/a&gt; is enabled for a particular VM&amp;rsquo;s interface, controller programs vRouter to proxy BGP TCP connections coming to virtual network&amp;rsquo;s default gateway IP and forward them to the controller. This way VNF thinks it peers with a next hop IP, however all BGP state and path computations still happen on the controller.&lt;/p&gt;

&lt;p&gt;The diagram below depicts a sample implementation of BGPaaS using OpenContrail. VNF is connected to a vRouter using a dot1Q trunk interface (to allow multiple VRFs over a single vEth link). Each VRF has its own BGPaaS session setup to advertise network ranges (NET1-3) into customer VPNs. These BGP sessions get proxied to the controller which injects those prefixes into their respective VPNs. These updates are then sent to DC gateways using either a VPNv4/6 or EVPN and the traffic is forwarded through DC underlay with VPN segregation preserved by either an MPLS tag (for MPLSoGRE or MPLSoUDL encapsulation) or a VXLAN VNI.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/contrail-bgpaas.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now let me briefly go over the lab that I&amp;rsquo;ve built to showcase the BGPaaS and DC-GW integration features.&lt;/p&gt;

&lt;h1 id=&#34;lab-setup-overview&#34;&gt;Lab setup overview&lt;/h1&gt;

&lt;p&gt;OpenContrail follows a familiar pattern of DC SDN architecture with central controller orchestrating the work of multiple virtual switches. In case of OpenContrail, these switches are called vRouters and they communicate with controller using XMPP-based extension of BGP as described in &lt;a href=&#34;https://www.ietf.org/archive/id/draft-ietf-l3vpn-end-system-06.txt&#34; target=&#34;_blank&#34;&gt;this RFC draft&lt;/a&gt;. A very detailed description of its internal architecture is available on &lt;a href=&#34;http://www.opencontrail.org/opencontrail-architecture-documentation/&#34; target=&#34;_blank&#34;&gt;OpenContrail&amp;rsquo;s website&lt;/a&gt; so it would be pointless to repeat all of this information here. That&amp;rsquo;s why I&amp;rsquo;ll concentrate on how to get things done rather then on the architectural aspects. However to get things started, I always like to have a clear picture of what I&amp;rsquo;m trying to achieve. The below diagram depicts a high-level architecture of my lab setup. Although OpenContrail supports BGP VPNv4/6 with multiple dataplane encapsulations, in this post I&amp;rsquo;ll use EVPN as the only control plane protocol to communicate with MX80 and use VXLAN encapsulation in the dataplane.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/contrail-lab.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;EVPN as a DC-GW integration protocol is relatively new to OpenContrail and comes with a few limitations. One of them is the absence of EVPN type-5 routes, which means I can&amp;rsquo;t use it in the same way I did in &lt;a href=&#34;https://networkop.co.uk/blog/2017/12/15/os-odl-netvirt/&#34;&gt;OpenDaylight&amp;rsquo;s case&lt;/a&gt;. Instead I&amp;rsquo;ll demonstrate a DC-GW IRB scenario, which extends the existing virtual network to a DC-GW and makes IRB/SVI interface on that DC-GW act as a default gateway for this network. This is a very common scenario for L2 DCI and active-active DC deployment models. To demonstrate this scenario I&amp;rsquo;m going to setup a single OpenStack virtual network with a couple of VMs whose gateway will reside on MX80. Since I only have a single OpenStack instance and a single MX80, I&amp;rsquo;ll setup one half of L2 DCI and setup a mutual redistribution to make our overlay network reachable from MX80&amp;rsquo;s global routing table.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/contrail-overlay.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;all-in-one-vm-setup&#34;&gt;All-in-one VM setup&lt;/h1&gt;

&lt;p&gt;Physically, my lab will consist of a single hypervisor running an all-in-one VM with &lt;a href=&#34;https://docs.openstack.org/kolla/latest/&#34; target=&#34;_blank&#34;&gt;kolla-openstack&lt;/a&gt; and &lt;a href=&#34;https://github.com/Juniper/contrail-docker/wiki/OpenContrail-Kolla&#34; target=&#34;_blank&#34;&gt;kolla-contrail&lt;/a&gt; and a physical Juniper MX80 playing the role of a DC-GW.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/contrail-setup.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;OpenContrail&amp;rsquo;s &lt;a href=&#34;https://github.com/Juniper/contrail-docker/wiki/OpenContrail-Kolla&#34; target=&#34;_blank&#34;&gt;kolla github page&lt;/a&gt; contains a set of instructions to setup the environment. As usual, I have automated all of these steps which can be setup from a hypervisor with the following commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/networkop/kolla-odl-bgpvpn &amp;amp;&amp;amp; cd kolla-odl-bgpvpn
./1-create.sh do 
./2-contrail.sh do
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;openstack-setup&#34;&gt;OpenStack setup&lt;/h1&gt;

&lt;p&gt;Once installation is complete and all docker containers are up and running, we can setup the OpenStack side of our test environment. The script below will do the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download cirros and CumulusVX images and upload them to Glance&lt;/li&gt;
&lt;li&gt;Create a virtual network&lt;/li&gt;
&lt;li&gt;Update security rules to allow inbound ICMP and SSH connections&lt;/li&gt;
&lt;li&gt;Create a pair of VMs - one based on cirros and one based on CumulusVX image&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;curl -L -o ./cirros http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img
curl -L -o ./cumulusVX http://cumulusfiles.s3.amazonaws.com/cumulus-linux-3.5.0-vx-amd64.qcow2

openstack image create --disk-format qcow2 --container-format bare --public \
--property os_type=linux --file ./cirros cirros
rm ./cirros

openstack image create --disk-format qcow2 --container-format bare --public \
--property os_type=linux --file ./cumulusVX cumulus
rm ./cumulusVX

openstack network create --provider-network-type vxlan irb-net

openstack subnet create --subnet-range 10.0.100.160/27 --network irb-net \
      --host-route destination=0.0.0.0/0,gateway=10.0.100.190 \
      --gateway 10.0.100.161 --dns-nameserver 8.8.8.8 irb-subnet

openstack flavor create --id 1 --ram 256 --disk 1 --vcpus 1 m1.nano
openstack flavor create --id 2 --ram 512 --disk 10 --vcpus 1 m1.tiny

ADMIN_PROJECT_ID=$(openstack project show &#39;admin&#39; -f value -c id)
ADMIN_SEC_GROUP=$(openstack security group list --project ${ADMIN_PROJECT_ID} | awk &#39;/ default / {print $2}&#39;)
openstack security group rule create --ingress --ethertype IPv4 \
    --protocol icmp ${ADMIN_SEC_GROUP}
openstack security group rule create --ingress --ethertype IPv4 \
    --protocol tcp --dst-port 22 ${ADMIN_SEC_GROUP}

openstack server create --image cirros --flavor m1.nano --net irb-net VM1
openstack server create --image cumulus --flavor m1.tiny --net irb-net VR1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only thing worth noting in the above script is that a default gateway &lt;code&gt;10.0.100.161&lt;/code&gt; gets overridden by a default host route pointing to &lt;code&gt;10.0.100.190&lt;/code&gt;. Normally, to demonstrate DC-GW IRB scenario, I would have setup a gateway-less L2 only subnet, however in that case I wouldn&amp;rsquo;t have been able to demonstrate BGPaaS on the same network, since this feature relies on having a gateway IP setup (which later acts as a BGP session termination endpoint). So instead of setting up two separate networks I&amp;rsquo;ve decided to implement this hack to minimise the required configuration.&lt;/p&gt;

&lt;h1 id=&#34;evpn-integration-with-mx80&#34;&gt;EVPN integration with MX80&lt;/h1&gt;

&lt;p&gt;DC-GW integration procedure is very simple and requires only a few simple steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Make sure VXLAN VNI is matched on both ends&lt;/li&gt;
&lt;li&gt;Configure import/export route targets&lt;/li&gt;
&lt;li&gt;Setup BGP peering with DC-GW&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All of these steps can be done very easily through OpenContrail&amp;rsquo;s GUI. However as I&amp;rsquo;ve mentioned before, I always prefer to use API when I have a chance and in this case I even have a python library for OpenContrail&amp;rsquo;s REST API available on Juniper&amp;rsquo;s &lt;a href=&#34;https://github.com/Juniper/contrail-python-api&#34; target=&#34;_blank&#34;&gt;github page&lt;/a&gt;, which I&amp;rsquo;m going to use below to implement the above three steps.&lt;/p&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;Before we can begin working with OpenContrail&amp;rsquo;s API, we need to authenticate with the controller and get a REST API connection handler.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pycontrail.client as client
CONTRAIL_API = &#39;http://10.0.100.140:8082&#39;
AUTH_URL = &#39;http://10.0.100.140:5000/v2.0&#39;
AUTH_PARAMS = {
    &#39;type&#39;: &#39;keystone&#39;,
    &#39;username&#39;: &#39;admin&#39;,
    &#39;password&#39;: &#39;mypassword&#39;,
    &#39;tenant_name&#39;: &#39;admin&#39;,
    &#39;auth_url&#39;: AUTH_URL
}
conn = client.Client(url=CONTRAIL_API,auth_params=AUTH_PARAMS)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first thing I&amp;rsquo;m going to do is override the default VNI setup by OpenContrail for &lt;code&gt;irb-net&lt;/code&gt; to a pre-defined value of &lt;code&gt;5001&lt;/code&gt;. To do that I first need to get a handler for &lt;code&gt;irb-net&lt;/code&gt; object and extract the &lt;code&gt;virtual_network_properties&lt;/code&gt; object containing a &lt;code&gt;vxlan_network_identifier&lt;/code&gt; property. Once it&amp;rsquo;s overridden, I just need to update the parent &lt;code&gt;irb-net&lt;/code&gt; object to apply the change to the running configuration on the controller.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;irb_net = conn.virtual_network_read(fq_name = [ &#39;default-domain&#39;, &#39;admin&#39; ,&#39;irb-net&#39;] )
vni_props=irb_net.get_virtual_network_properties()
vni_props.set_vxlan_network_identifier(5001)
irb_net.set_virtual_network_properties(vni_props)
conn.virtual_network_update(irb_net)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next thing I need to do is explicitly set the import/export route-target properties for the &lt;code&gt;irb-net&lt;/code&gt; object. This will require a new &lt;code&gt;RouteTargetList&lt;/code&gt; object which then gets referenced by a &lt;code&gt;route_target_list&lt;/code&gt; property of the &lt;code&gt;irb-net&lt;/code&gt; object.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pycontrail import types as t
new_rtl = t.RouteTargetList([&#39;target:200:200&#39;])
irb_net.set_route_target_list(new_rtl)
conn.virtual_network_update(irb_net)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final step is setting up a peering with MX80. The main object that needs to be created is &lt;code&gt;BgpRouter&lt;/code&gt;, which contains a pointer to BGP session parameters object with session-specific values like ASN and remote peer IP. BGP router is defined in a global context (default domain and default project) which will make it available to all configured virtual networks.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pycontrail import types as t
ctx = [&#39;default-domain&#39;, &#39;default-project&#39;, &#39;ip-fabric&#39;, &#39;__default__&#39;]
af = t.AddressFamilies(family=[&#39;inet-vpn&#39;, &#39;e-vpn&#39;])
bgp_params = t.BgpRouterParams(vendor=&#39;Juniper&#39;, \
                               autonomous_system=65411, \
                               address=&#39;10.0.101.15&#39;, \
                               address_families=af)
vrf = conn.routing_instance_read(fq_name = ctx)
bgp_router = t.BgpRouter(name=&#39;MX80&#39;, display_name=&#39;MX80&#39;, \
                         bgp_router_parameters=bgp_params,
                         parent_obj=vrf)
contrail = conn.bgp_router_read(fq_name = ctx + [&#39;controller-1&#39;])
bgp_router.set_bgp_router(contrail,t.BgpPeeringAttributes())
conn.bgp_router_create(bgp_router)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the sake of brevity, I will not cover MX80&amp;rsquo;s configuration in details and simply include it here for reference with some minor explanatory comments.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Interface and global settings configuration
set interfaces irb unit 5001 family inet address 10.0.100.190/27
set interfaces lo0 unit 0 family inet address 10.0.101.15/32
set routing-options router-id 10.0.101.15
set routing-options autonomous-system 65411

# Setup BGP peering with OpenContrail
set protocols bgp group CONTRAIL multihop
set protocols bgp group CONTRAIL local-address 10.0.101.15
set protocols bgp group CONTRAIL family inet-vpn unicast
set protocols bgp group CONTRAIL family evpn signaling
set protocols bgp group CONTRAIL peer-as 64512
set protocols bgp group CONTRAIL neighbor 10.0.100.140

# Setup EVPN instance type with IRB interface and matching RT and VNI
set routing-instances EVPN-L2-IRB vtep-source-interface lo0.0
set routing-instances EVPN-L2-IRB instance-type evpn
set routing-instances EVPN-L2-IRB vlan-id 501
set routing-instances EVPN-L2-IRB routing-interface irb.5001
set routing-instances EVPN-L2-IRB vxlan vni 5001
set routing-instances EVPN-L2-IRB route-distinguisher 200:200
set routing-instances EVPN-L2-IRB vrf-target target:200:200
set routing-instances EVPN-L2-IRB protocols evpn encapsulation vxlan

# Setup VRF instance with IRB interface
set routing-instances EVPN-L3-IRB instance-type vrf
set routing-instances EVPN-L3-IRB interface irb.5001
set routing-instances EVPN-L3-IRB route-distinguisher 201:200
set routing-instances EVPN-L3-IRB vrf-target target:200:200

# Setup route redistribution between EVPN and Global VRFs
set routing-options rib-groups CONTRAIL-TO-GLOBAL import-rib EVPN-L3-IRB.inet.0
set routing-options rib-groups CONTRAIL-TO-GLOBAL import-rib inet.0
set routing-options rib-groups GLOBAL-TO-CONTRAIL import-rib inet.0
set routing-options rib-groups GLOBAL-TO-CONTRAIL import-rib EVPN-L3-IRB.inet.0
set routing-options interface-routes rib-group inet CONTRAIL-TO-GLOBAL
set routing-instances EVPN-L3-IRB routing-options interface-routes rib-group inet CONTRAIL-TO-GLOBAL
set protocols bgp group EXTERNAL-BGP family inet unicast rib-group GLOBAL-TO-CONTRAIL
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;verification&#34;&gt;Verification&lt;/h2&gt;

&lt;p&gt;The easiest way to verify that BGP peering has been established is to query OpenContrail&amp;rsquo;s introspection API:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl  -s http://10.0.100.140:8083/Snh_BgpNeighborReq?ip_address=10.0.101.15 | \
  xmllint --xpath &#39;/BgpNeighborListResp/neighbors[1]/list/BgpNeighborResp/state&#39; -
&amp;lt;state type=&amp;quot;string&amp;quot; identifier=&amp;quot;8&amp;quot;&amp;gt;Established&amp;lt;/state&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Datapath verification can be done from either side, in this case I&amp;rsquo;m showing a ping from MX80&amp;rsquo;s global VRF towards one of the OpenStack VMs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;admin@MX80&amp;gt; ping 10.0.100.164 count 2 
PING 10.0.100.164 (10.0.100.164): 56 data bytes
64 bytes from 10.0.100.164: icmp_seq=0 ttl=64 time=3.836 ms
64 bytes from 10.0.100.164: icmp_seq=1 ttl=64 time=3.907 ms

--- 10.0.100.164 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max/stddev = 3.836/3.872/3.907/0.035 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;bgp-as-a-service-1&#34;&gt;BGP-as-a-Service&lt;/h1&gt;

&lt;p&gt;To keep things simple I will not use multiple dot1Q interfaces and setup a BGP peering with CumulusVX over a normal, non-trunk interface. From CumulusVX I will inject a loopback IP &lt;code&gt;1.1.1.1/32&lt;/code&gt; into the &lt;code&gt;irb-net&lt;/code&gt; network. Since REST API python library I&amp;rsquo;ve used above is two major releases behind the current version of OpenContrail, it cannot be used to setup BGPaaS feature. Instead I will demonstrate how to use REST API directly from the command line of all-in-one VM using cURL.&lt;/p&gt;

&lt;h2 id=&#34;configuration-1&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;In order to start working with OpenContrail&amp;rsquo;s API, I first need to obtain an authentication token from OpenStack&amp;rsquo;s keystone. With that token I can now query the list of IPs assigned to all OpenStack instances and pick the one assigned to CumulusVX. I need the UUID of that particular IP address in order to extract the ID of the VM interface this IP is assigned to.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;source /etc/kolla/admin-openrc.sh 
TOKEN=$(openstack token issue -f value -c id)
CONTRAIL_AUTH=&amp;quot;X-AUTH-TOKEN: $TOKEN&amp;quot;
CTYPE=&amp;quot;Content-Type: application/json; charset=UTF-8&amp;quot;
curl -H &amp;quot;$CONTRAIL_AUTH&amp;quot; http://10.0.100.140:8082/instance-ips | jq
VMI_ID=$(curl -H &amp;quot;$CONTRAIL_AUTH&amp;quot; http://10.0.100.140:8082/instance-ip/2e7987be-3f53-4296-905a-0c64793307a9 | \
         jq &#39;.[&amp;quot;instance-ip&amp;quot;] .virtual_machine_interface_refs[0].uuid&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With VM interface ID saved in a &lt;code&gt;VMI_ID&lt;/code&gt; variable I can create a BGPaaS service and link it to that particular VM interface.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./bgpaas.json
{
    &amp;quot;bgp-as-a-service&amp;quot;:
    {
        &amp;quot;fq_name&amp;quot;: [&amp;quot;default-domain&amp;quot;, &amp;quot;admin&amp;quot;, &amp;quot;cumulusVX-bgp&amp;quot; ],
        &amp;quot;autonomous_system&amp;quot;: 321,
        &amp;quot;bgpaas_session_attributes&amp;quot;: {
            &amp;quot;address_families&amp;quot;: {&amp;quot;family&amp;quot;: [&amp;quot;inet&amp;quot;] }
            },
        &amp;quot;parent_type&amp;quot;: &amp;quot;project&amp;quot;,
        &amp;quot;virtual_machine_interface_refs&amp;quot;: [{
            &amp;quot;attr&amp;quot;: null,
            &amp;quot;to&amp;quot;: [&amp;quot;default-domain&amp;quot;, &amp;quot;admin&amp;quot;, ${VMI_ID}]
            }],
        &amp;quot;bgpaas-shared&amp;quot;: false,
        &amp;quot;bgpaas-ip-address&amp;quot;: &amp;quot;10.0.100.164&amp;quot;
    }
}
EOF

curl -X POST -H &amp;quot;$CONTRAIL_AUTH&amp;quot; -H &amp;quot;$CTYPE&amp;quot; -d @bgpaas.json http://10.0.100.140:8082/bgp-as-a-services
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final step is setting up a BGP peering on the CumulusVX side. CumulusVX configuration is very simple and self-explanatory. The BGP neighbor IP is the IP of virtual network&amp;rsquo;s default gateway located on local vRouter.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;!
interface lo
 ip address 1.1.1.1/32
!
router bgp 321
 neighbor 10.0.100.161 remote-as 64512
 !
 address-family ipv4 unicast
  network 1.1.1.1/32
 exit-address-family
!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;verification-1&#34;&gt;Verification&lt;/h2&gt;

&lt;p&gt;Here&amp;rsquo;s where we come across another limitation of EVPN. The loopback prefix &lt;code&gt;1.1.1.1/32&lt;/code&gt; does not get injected into EVPN address family, however it does show up automatically in the VPNv4 address family which can be verified from the MX80:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;admin@MX80&amp;gt; show route table bgp.l3vpn.0 hidden 1.1.1.1/32 extensive    

bgp.l3vpn.0: 6 destinations, 6 routes (3 active, 0 holddown, 3 hidden)
10.0.100.140:2:1.1.1.1/32 (1 entry, 0 announced)
         BGP    Preference: 170/-101
                Route Distinguisher: 10.0.100.140:2
                Next hop type: Unusable, Next hop index: 0
                Next-hop reference count: 6
                State: &amp;lt;Hidden Ext ProtectionPath ProtectionCand&amp;gt;
                Local AS: 65411 Peer AS: 64512
                Age: 37:44 
                Validation State: unverified 
                Task: BGP_64512.10.0.100.140
                AS path: 64512 321 I
                Communities: target:200:200 target:64512:8000003 encapsulation:unknown(0x2) encapsulation:mpls-in-udp(0xd) unknown type 8004 value fc00:7a1201 unknown type 8071 value fc00:1389
                Import Accepted
                VPN Label: 31
                Localpref: 100
                Router ID: 10.0.100.140
                Secondary Tables: EVPN-L3-IRB.inet.0
                Indirect next hops: 1
                        Protocol next hop: 10.0.100.140
                        Label operation: Push 31
                        Label TTL action: prop-ttl
                        Load balance label: Label 31: None; 
                        Indirect next hop: 0x0 - INH Session ID: 0x0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s hidden since I haven&amp;rsquo;t configured MPLSoUDP &lt;a href=&#34;https://www.juniper.net/documentation/en_US/junos/topics/example/example-next-hop-based-dynamic-mpls-udp-tunnel-configuring.html&#34; target=&#34;_blank&#34;&gt;dynamic tunnels&lt;/a&gt; on MX80. However this proves that the prefix does get injected into customer VPNs and become available on all devices with the matching import route-target communities.&lt;/p&gt;

&lt;h1 id=&#34;outro&#34;&gt;Outro&lt;/h1&gt;

&lt;p&gt;This post concludes Series 2 of my OpenStack SDN saga. I&amp;rsquo;ve covered quite an extensive range of topics in my two-part series, however, OpenStack networking landscape is so big, it&amp;rsquo;s simply impossible to cover everything I find interesting. I started writing about OpenStack SDN when I first learned I got a job with Nokia. Back then I knew little about VMware NSX and even less about OpenStack. That&amp;rsquo;s why I started researching topics that I found interesting and branching out into adjacent areas as I went along. Almost 2 years later, looking back I can say I&amp;rsquo;ve learned a lot about the internals of SDN in general and hopefully so have my readers. Now I&amp;rsquo;m leaving Nokia to rediscover my networking roots at Arista. I&amp;rsquo;ll dive into DC networking from a different perspective now and it may be awhile before I accumulate a critical mass of interesting material to start spilling it out in my blog again. I still may come back to OpenStack some day but for now I&amp;rsquo;m gonna take a little break, maybe do some house keeping (e.g. move my blog from Jekyll to &lt;a href=&#34;https://gohugo.io/&#34; target=&#34;_blank&#34;&gt;Hugo&lt;/a&gt;, add TLS support) and enjoy my time being a farther.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN - OpenDaylight With BGP VPN</title>
      <link>https://networkop.co.uk/blog/2017/12/15/os-odl-netvirt/</link>
      <pubDate>Fri, 15 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/12/15/os-odl-netvirt/</guid>
      <description>

&lt;p&gt;For the last 5 years OpenStack has been the training ground for a lot of emerging DC SDN solutions. OpenStack integration use case was one of the most compelling and easiest to implement thanks to the limited and suboptimal implementation of the native networking stack. Today, in 2017, features like &lt;a href=&#34;https://networkop.co.uk/blog/2016/05/06/neutron-l2pop/&#34;&gt;L2 population&lt;/a&gt;, local &lt;a href=&#34;https://networkop.co.uk/blog/2016/05/06/neutron-l2pop/&#34;&gt;ARP responder&lt;/a&gt;, &lt;a href=&#34;https://networkop.co.uk/blog/2016/05/21/neutron-l2gw/&#34;&gt;L2 gateway integration&lt;/a&gt;, &lt;a href=&#34;https://networkop.co.uk/blog/2016/10/13/os-dvr/&#34;&gt;distributed routing&lt;/a&gt; and &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/15/os-sfc-skydive/&#34;&gt;service function chaining&lt;/a&gt; have all become available in vanilla OpenStack and don&amp;rsquo;t require a proprietary SDN controller anymore. Admittedly, some of the features are still not (and may never be) implemented in the most optimal way (e.g. DVR). This is where new opensource SDN controllers, the likes of &lt;a href=&#34;https://networkop.co.uk/blog/2016/12/10/ovn-part2/&#34;&gt;OVN&lt;/a&gt; and &lt;a href=&#34;https://docs.openstack.org/developer/dragonflow/distributed_dragonflow.html&#34; target=&#34;_blank&#34;&gt;Dragonflow&lt;/a&gt;, step in to provide scalable, elegant and efficient implementation of these advanced networking features. However one major feature still remains outside of the scope of a lot of these new opensource SDN projects, and that is data centre gateway (DC-GW) integration. Let me start by explain why you would need this feature in the first place.&lt;/p&gt;

&lt;h1 id=&#34;optimal-forwarding-of-north-south-traffic&#34;&gt;Optimal forwarding of North-South traffic&lt;/h1&gt;

&lt;p&gt;OpenStack Neutron and VMware NSX, both being pure software solutions, rely on a special type of node to forward traffic between VMs and hosts outside of the data centre. This node acts as a L2/L3 gateway for all North-South traffic and is often implemented as either a VM or a network namespace. This kind of solution gives software developers greater independence from the underlying networking infrastructure which makes it easier for them to innovate and introduce new features.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sdn-ns.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;However, from the traffic forwarding point of view, having a gateway/network node is not a good solution at all. There is no technological reason for a packet to have to go through this node when after all it ends up on a DC-GW anyway. In fact, this solution introduces additional complexity which needs to be properly managed (e.g. designed, configured and troubleshooted) and a potential bottleneck for high-throughput traffic flows.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s clear that the most optimal way to forward traffic is directly from a compute node to a DC-GW. The only question is how can this optimal forwarding be achieved? SDN controller needs to be able to exchange reachability information with DC-GW using a common protocol understood by most of the existing routing stacks. One such protocol, becoming very common in DC environments, is BGP, which has two address families we can potentially use:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;VPNv4/6 will allow routes to be exchanged and the dataplance to use MPLSoGRE encapsulation. This should be considered a &amp;ldquo;legacy&amp;rdquo; approach since for a very long time DC-GWs did not have the VXLAN ecap/decap capabilities.&lt;/li&gt;
&lt;li&gt;EVPN with VXLAN-based overlays. EVPN makes it possible to exchange both L2 and L3 information under the same AF, which means we have the flexibility of doing not only a L3 WAN integration, but also a L2 data centre interconnect with just a single protocol.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In OpenStack specifically, BGPVPN project was created to provide a pluggable driver framework for 3rd party BGP implementations. Apart from a reference BaGPipe driver (BaGPipe is an ExaBGP fork with lightweight implementation of BGP VPNs), which relies on a default &lt;code&gt;openvswitch&lt;/code&gt; ML2 mechanism driver, only Nuage, OpenDaylight and OpenContrail have contributed their drivers to this project. In this post I will focus on OpenDaylight and show how to install containerised OpenStack with OpenDaylight and integrate it with Cisco CSR using EVPN.&lt;/p&gt;

&lt;h1 id=&#34;opendaylight-integration-with-openstack&#34;&gt;OpenDaylight integration with OpenStack&lt;/h1&gt;

&lt;p&gt;Historically, OpenDaylight has had multiple projects implementing custom OpenStack networking drivers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VTN&lt;/strong&gt; (Virtual Tenant Networking) - spearheaded by NEC was the first project to provide OpenStack networking implementation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GBP&lt;/strong&gt; (Group Based Policy) - a project led by Cisco, one of the first (if not THE first) commercial implementation of Intent-based networking&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NetVirt&lt;/strong&gt; - currently a default Neutron plugin from ODL, developed jointly by Brocade (RIP), RedHat, Ericsson, Intel and many others.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NetVirt provides several common Neutron services including L2 and L3 forwarding, ACL and NAT, as well as advanced services like L2 gateway, QoS and SFC. To do that it assumes full control over an OVS switch inside each compute node and implements the above services inside a single &lt;code&gt;br-int&lt;/code&gt; OVS bridge. L2/L3 forwarding tables are built based on tenant IP/MAC addresses that have been allocated by Neutron and the current network topology. For high-level overview of NetVirt&amp;rsquo;s forwarding pipeline you can refer to &lt;a href=&#34;https://docs.google.com/presentation/d/15h4ZjPxblI5Pz9VWIYnzfyRcQrXYxA1uUoqJsgA53KM/edit#slide=id.g1c73ae9953_2_0&#34; target=&#34;_blank&#34;&gt;this document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It helps to think of an ODL-managed OpenStack as a big chassis switch. NetVirt plays the role of a supervisor by managing control plane and compiling RIB based on the information received from Neutron. Each compute node running an OVS is a linecard with VMs connected to its ports. Unlike the distributed architecture of &lt;a href=&#34;https://networkop.co.uk/blog/2016/12/10/ovn-part2/&#34;&gt;OVN&lt;/a&gt; and Dragonflow, compute nodes do not contain any control plane elements and each OVS gets its FIB programmed directly by the supervisor. DC underlay is a backplane, interconnecting all linecards and a supervisor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/odl-netvirt-chassis.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;opendaylight-bgp-vpn-service-architecture&#34;&gt;OpenDaylight BGP VPN service architecture&lt;/h1&gt;

&lt;p&gt;In order to provide BGP VPN functionality, NetVirt employs the use of three service components:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FIB service&lt;/strong&gt; - maintains L2/L3 forwarding tables and reacts to topology changes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BGP manager&lt;/strong&gt; - provides a translation of information sent to and received from an external BGP stack (Quagga BGP)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VPN Manager&lt;/strong&gt; - ties together the above two components, creates VRFs and keeps track of RD/RT values&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to exchange BGP updates with external DC-GW, NetVirt requires a BGP stack with EVPN and VPNV4/6 capabilities. Ideally, internal ODL BGP stack could have been used for that, however it didn&amp;rsquo;t meet all the performance requirements (injecting/withdrawing thousand of prefixes at the same time). Instead, an external &lt;a href=&#34;https://github.com/6WIND/quagga/tree/qthrift_mpbgp_evpn&#34; target=&#34;_blank&#34;&gt;Quagga fork&lt;/a&gt; with EVPN add-ons is connected to BGP manager via a high-speed Apache Thrift interface. This interface defines the &lt;a href=&#34;https://github.com/6WIND/quagga/blob/qthrift_mpbgp_evpn/qthriftd/vpnservice.thrift&#34; target=&#34;_blank&#34;&gt;format&lt;/a&gt; of data to be exchanged between Quagga (a.k.a QBGP) and NetVirt&amp;rsquo;s BGP Manager in order to do two things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Configure BGP settings like ASN and BGP neighbors&lt;/li&gt;
&lt;li&gt;Read/Write RIB entries inside QBGP&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;BGP session is established between QBGP and external DC-GW, however next-hop values installed by NetVirt and advertised by QBGP have IPs of the respective compute nodes, so that traffic is sent directly via the most optimal path.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/odl-netvirt.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;demo&#34;&gt;Demo&lt;/h1&gt;

&lt;p&gt;Enough of the theory, let&amp;rsquo;s have a look at how to configure a L3VPN between QBGP (advertising ODL&amp;rsquo;s distributed router subnets) and IOS-XE DC-GW using EVPN route type 5 or, more specifically, &lt;a href=&#34;https://tools.ietf.org/html/draft-ietf-bess-evpn-prefix-advertisement-09#section-4.4.1&#34; target=&#34;_blank&#34;&gt;Interface-less IP-VRF-to-IP-VRF model&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/odl-evpn-topo.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;My lab environment is still based on a pair of nested VMs running containerised Kolla OpenStack I&amp;rsquo;ve described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/08/os-lab-docker/&#34;&gt;earlier post&lt;/a&gt;. A few months ago OpenDaylight role has been added to kolla-ansible so now it is possible to install OpenDaylight-intergrated OpenStack automatically. However, there is no option to install QBGP so I had to augment the default &lt;a href=&#34;https://github.com/openstack/kolla&#34; target=&#34;_blank&#34;&gt;Kolla&lt;/a&gt; and &lt;a href=&#34;https://github.com/openstack/kolla-ansible&#34; target=&#34;_blank&#34;&gt;Kolla-ansible&lt;/a&gt; repositories to include the QBGP &lt;a href=&#34;https://github.com/networkop/kolla-odl-bgpvpn/blob/master/roles/kolla_build/templates/quagga-Dockerfile.j2&#34; target=&#34;_blank&#34;&gt;Dockerfile template&lt;/a&gt; and QBGP &lt;a href=&#34;https://github.com/networkop/kolla-odl-bgpvpn/blob/master/roles/kolla_deploy/tasks/create.yml#L90-L120&#34; target=&#34;_blank&#34;&gt;ansible role&lt;/a&gt;. So the first step is to download my latest automated installer and make sure &lt;code&gt;enable_opendaylight&lt;/code&gt; global variable is set to &lt;code&gt;yes&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/networkop/kolla-odl-bgpvpn.git &amp;amp;&amp;amp; cd kolla-odl-bgpvpn
mkdir group_vars
echo &amp;quot;enable_opendaylight: \&amp;quot;yes\&amp;quot;&amp;quot; &amp;gt;&amp;gt; group_vars/all.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At the time of writing I was relying on a couple of latest bug fixes inside OpenDaylight, so I had to modify the default ODL role to install the latest master-branch ODL build. Make sure the link below is pointing to the latest &lt;code&gt;zip&lt;/code&gt; file in &lt;code&gt;0.8.0-SNAPSHOT&lt;/code&gt; directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt;&amp;gt; group_vars/all.yaml
odl_latest_enabled: true
odl_latest_url: https://nexus.opendaylight.org/content/repositories/opendaylight.snapshot/org/opendaylight/integration/netvirt/karaf/0.8.0-SNAPSHOT/karaf-0.8.0-20171106.102232-1767.zip
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next few steps are similar to what I&amp;rsquo;ve described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/08/os-lab-docker/&#34;&gt;Kolla lab post&lt;/a&gt;, will create a pair of VMs, build all Kolla containers, push them to a local Docker repo and finally deploy OpenStack using Kolla-ansible playbooks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./1-create.sh do
./2-bootstrap.sh do
./3-build.sh do 
./4-deploy.sh do
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final &lt;code&gt;4-deploy.sh&lt;/code&gt; script will also create a simple &lt;code&gt;init.sh&lt;/code&gt; script inside the controller VM that can be used to setup a test topology with a single VM connected to a &lt;code&gt;10.0.0.0/24&lt;/code&gt; subnet:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh kolla-controller
source /etc/kolla/admin-openrc.sh
./init.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Of course, another option to build a lab is to follow the official &lt;a href=&#34;https://docs.openstack.org/kolla-ansible/latest/user/quickstart.html&#34; target=&#34;_blank&#34;&gt;Kolla documentation&lt;/a&gt; to create your own custom test environment.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;Assuming the test topology was setup with no issues and a test VM can ping its default gateway &lt;code&gt;10.0.0.1&lt;/code&gt;, we can start configuring BGP VPNs. Unfortunately, we won&amp;rsquo;t be able to use OpenStack BGPVPN API/CLI, since ODL requires an extra parameter (L3 VNI for symmetric IRB) which is not available in OpenStack BGPVPN API, but we still can configure everything directly through ODL&amp;rsquo;s API. My interface of choice is always REST, since it&amp;rsquo;s easier to build it into a fully programmatic plugin, so even though all of the below steps can be accomplished through karaf console CLI, I&amp;rsquo;ll be using cURL to send and retrieve data from ODL&amp;rsquo;s REST API.&lt;/p&gt;

&lt;h3 id=&#34;1-source-admin-credentials-and-setup-odl-s-rest-variables&#34;&gt;1. Source admin credentials and setup ODL&amp;rsquo;s REST variables&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;source /etc/kolla/admin-openrc.sh
export ODL_URL=&#39;http://192.168.133.100:8181/restconf&#39;
export CT_JSON=&amp;quot;Content-Type: application/json&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-configure-local-bgp-settings-and-bgp-peering-with-dc-gw&#34;&gt;2. Configure local BGP settings and BGP peering with DC-GW&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./bgp-full.json
{
    &amp;quot;bgp&amp;quot;: {
        &amp;quot;as-id&amp;quot;: {
            &amp;quot;announce-fbit&amp;quot;: false,
            &amp;quot;local-as&amp;quot;: 100,
            &amp;quot;router-id&amp;quot;: &amp;quot;192.168.133.100&amp;quot;,
            &amp;quot;stalepath-time&amp;quot;: 0
        },
        &amp;quot;logging&amp;quot;: {
            &amp;quot;file&amp;quot;: &amp;quot;/var/log/bgp_debug.log&amp;quot;,
            &amp;quot;level&amp;quot;: &amp;quot;errors&amp;quot;
        },
        &amp;quot;neighbors&amp;quot;: [
            {
                &amp;quot;address&amp;quot;: &amp;quot;192.168.133.50&amp;quot;,
                &amp;quot;remote-as&amp;quot;: 100,
                &amp;quot;address-families&amp;quot;: [
                   {
                     &amp;quot;ebgp:afi&amp;quot;: &amp;quot;3&amp;quot;,
                     &amp;quot;ebgp:peer-ip&amp;quot;: &amp;quot;192.168.133.50&amp;quot;,
                     &amp;quot;ebgp:safi&amp;quot;: &amp;quot;6&amp;quot;
                   }
                ]
            }
        ]
    }
}
EOF

curl -X PUT -u admin:admin -k -v -H &amp;quot;$CT_JSON&amp;quot;  \
     $ODL_URL/config/ebgp:bgp -d @bgp-full.json
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-define-l3vpn-instance-and-associate-it-with-openstack-admin-tenant&#34;&gt;3. Define L3VPN instance and associate it with OpenStack &lt;code&gt;admin&lt;/code&gt; tenant&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TENANT_UUID=$(openstack project show admin -f value -c id | \
            sed &#39;s/\(........\)\(....\)\(....\)\(....\)\(.*\)/\1-\2-\3-\4-\5/&#39;)

cat &amp;lt;&amp;lt; EOF &amp;gt; ./l3vpn-full.json
{
   &amp;quot;input&amp;quot;: {
      &amp;quot;l3vpn&amp;quot;:[
         {
            &amp;quot;id&amp;quot;:&amp;quot;f503fcb0-3fd9-4dee-8c3a-5034cf707fd9&amp;quot;,
            &amp;quot;name&amp;quot;:&amp;quot;L3EVPN&amp;quot;,
            &amp;quot;route-distinguisher&amp;quot;: [&amp;quot;100:100&amp;quot;],
            &amp;quot;export-RT&amp;quot;: [&amp;quot;100:100&amp;quot;],
            &amp;quot;import-RT&amp;quot;: [&amp;quot;100:100&amp;quot;],
            &amp;quot;l3vni&amp;quot;: &amp;quot;5000&amp;quot;,
            &amp;quot;tenant-id&amp;quot;:&amp;quot;${TENANT_UUID}&amp;quot;
         }
      ]
   }
}
EOF

curl -X POST -u admin:admin -k -v -H &amp;quot;$CT_JSON&amp;quot;  \
      $ODL_URL/operations/neutronvpn:createL3VPN -d @l3vpn-full.json
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-inject-prefixes-into-l3vpn-by-associating-the-previously-created-l3vpn-with-a-demo-router&#34;&gt;4. Inject prefixes into L3VPN by associating the previously created L3VPN with a &lt;code&gt;demo-router&lt;/code&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ROUTER_UUID=$(openstack router show demo-router -f value -c id)

cat &amp;lt;&amp;lt; EOF &amp;gt; ./l3vpn-assoc.json
{
  &amp;quot;input&amp;quot;:{
     &amp;quot;vpn-id&amp;quot;:&amp;quot;f503fcb0-3fd9-4dee-8c3a-5034cf707fd9&amp;quot;,
     &amp;quot;router-id&amp;quot;:[ &amp;quot;${ROUTER_UUID}&amp;quot; ]
   }
}
EOF

curl -X POST -u admin:admin -k -v -H &amp;quot;$CT_JSON&amp;quot;  \
     $ODL_URL/operations/neutronvpn:associateRouter -d @l3vpn-assoc.json
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-configure-dc-gw-vtep-ip&#34;&gt;5. Configure DC-GW VTEP IP&lt;/h3&gt;

&lt;p&gt;ODL cannot automatically extract VTEP IP from updates received from DC-GW, so we need to explicitly configure it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./tep.json
{
  &amp;quot;input&amp;quot;: {
    &amp;quot;destination-ip&amp;quot;: &amp;quot;1.1.1.1&amp;quot;,
    &amp;quot;tunnel-type&amp;quot;: &amp;quot;odl-interface:tunnel-type-vxlan&amp;quot;
  }
}
EOF
curl -X POST -u admin:admin -k -v -H &amp;quot;$CT_JSON&amp;quot;  \
     $ODL_URL/operations/itm-rpc:add-external-tunnel-endpoint -d @tep.json
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;6-dc-gw-configuration&#34;&gt;6. DC-GW configuration&lt;/h3&gt;

&lt;p&gt;That is all what needs to be configured on ODL. Although I would consider this to be outside of the scope of the current post, for the sake of completeness I&amp;rsquo;m including the relevant configuration from the DC-GW:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;!
vrf definition ODL
 rd 100:100
 route-target export 100:100
 route-target import 100:100
 !        
 address-family ipv4
  route-target export 100:100 stitching
  route-target import 100:100 stitching
 exit-address-family
!
bridge-domain 5000 
 member vni 5000
!
interface Loopback0
 ip address 1.1.1.1 255.255.255.255
!
interface GigabitEthernet1
 ip address 192.168.133.50 255.255.255.0
!
interface nve1
 no ip address
 source-interface Loopback0
 host-reachability protocol bgp
 member vni 5000 vrf ODL
!
interface BDI5000
 vrf forwarding ODL
 ip address 8.8.8.8 255.255.255.0
 encapsulation dot1Q 500
!
router bgp 100
 bgp log-neighbor-changes
 no bgp default ipv4-unicast
 neighbor 192.168.133.100 remote-as 100
 !
 address-family l2vpn evpn
  import vpnv4 unicast
  neighbor 192.168.133.100 activate
 exit-address-family
 !
 address-family ipv4 vrf ODL
  advertise l2vpn evpn
  redistribute connected
 exit-address-family
!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For detailed explanation of how EVPN RT5 is configured on Cisco CSR refer to the &lt;a href=&#34;https://www.cisco.com/c/en/us/td/docs/ios-xml/ios/cether/configuration/xe-16/ce-xe-16-book/evpn-vxlan-l3.html&#34; target=&#34;_blank&#34;&gt;following guide&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;verification&#34;&gt;Verification&lt;/h2&gt;

&lt;p&gt;There are several things that can be checked to verify that the DC-GW integration is working. One of the first steps would be to check if BGP session with CSR is up.
This can be done from the CSR side, however it&amp;rsquo;s also possible to check this from the QBGP side. First we need to get into the QBGP&amp;rsquo;s interactive shell from the controller node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[centos@controller-1 ~]$ sudo docker exec -it quagga /opt/quagga/bin/vtysh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From here, we can check that the BGP session has been established:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;controller-1# sh bgp neighbors 192.168.133.50     
BGP neighbor is 192.168.133.50, remote AS 100, local AS 100, internal link
  BGP version 4, remote router ID 1.1.1.1
  BGP state = Established, up for 00:03:05
&amp;lt;snip&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can also check the contents of EVPN RIB compiled by QBGP&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;controller-1# sh bgp evpn rd 100:100
BGP table version is 0, local router ID is 192.168.133.100
Status codes: s suppressed, d damped, h history, * valid, &amp;gt; best, i - internal
Origin codes: i - IGP, e - EGP, ? - incomplete

   Network          Next Hop            Metric LocPrf Weight Path
Route Distinguisher: as2 100:100
*&amp;gt; [0][fa:16:3e:37:42:d8/48][10.0.0.2/32]
                    192.168.133.100         0          32768 i
*&amp;gt; [0][fa:16:3e:dc:77:65/48][10.0.0.3/32]
                    192.168.133.101         0          32768 i
*&amp;gt;i8.8.8.0/24       1.1.1.1         0     100       0 ?
*&amp;gt; 10.0.0.0/24      192.168.133.100         0          32768 i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we can verify that the prefix &lt;code&gt;8.8.8.0/24&lt;/code&gt; advertised from DC-GW is being passed by QBGP and accepted by NetVirt&amp;rsquo;s FIB Manager:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -u admin:admin -k -v  $ODL_URL/config/odl-fib:fibEntries/\
  vrfTables/100%3A100/vrfEntry/8.8.8.0%2F24 | python -m json.tool
{
    &amp;quot;vrfEntry&amp;quot;: [
        {
            &amp;quot;destPrefix&amp;quot;: &amp;quot;8.8.8.0/24&amp;quot;,
            &amp;quot;encap-type&amp;quot;: &amp;quot;vxlan&amp;quot;,
            &amp;quot;gateway_mac_address&amp;quot;: &amp;quot;00:1e:49:69:24:bf&amp;quot;,
            &amp;quot;l3vni&amp;quot;: 5000,
            &amp;quot;origin&amp;quot;: &amp;quot;b&amp;quot;,
            &amp;quot;route-paths&amp;quot;: [
                {
                    &amp;quot;nexthop-address&amp;quot;: &amp;quot;1.1.1.1&amp;quot;
                }
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last output confirms that the prefix is being received and accepted by ODL. To do a similar check on CSR side we can run the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CSR1k#show bgp l2vpn evpn 
&amp;lt;snip&amp;gt;
     Network          Next Hop            Metric LocPrf Weight Path
Route Distinguisher: 100:100 (default for vrf ODL)
 *&amp;gt;i  [2][100:100][0][48][FA163E3742D8][32][10.0.0.2]/24
                      192.168.133.100          0    100      0 i
 *&amp;gt;i  [2][100:100][0][48][FA163EDC7765][32][10.0.0.3]/24
                      192.168.133.101          0    100      0 i
 *&amp;gt;   [5][100:100][0][24][8.8.8.0]/17
                      0.0.0.0                  0         32768 ?
 *&amp;gt;i  [5][100:100][0][24][10.0.0.0]/17
                      192.168.133.100          0    100      0 i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This confirms that the control plane information has been successfully exchanged between NetVirt and Cisco CSR.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;At the time of writing, there was an &lt;a href=&#34;https://git.opendaylight.org/gerrit/#/c/63324/&#34; target=&#34;_blank&#34;&gt;open bug&lt;/a&gt; in ODL master branch that prevented the forwarding entries from being installed in OVS datapath. Once the bug is fixed I will update this post with the dataplance verification, a.k.a ping&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;OpenDaylight is a pretty advanced OpenStack SDN platform. Its functionality includes clustering, site-to-site federation (without EVPN) and L2/L3 EVPN DC-GW integration for both IPv4 and IPv6. It is yet another example of how an open-source platform can match even the most advanced proprietary SDN solutions from incumbent vendors. This is all thanks to the companies involved in OpenDaylight development. I also want to say special thanks to Vyshakh Krishnan, Kiran N Upadhyaya and Dayavanti Gopal Kamath from Ericsson for helping me clear up some of the questions I posted on netvirt-dev mailing list.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN - NFV Management and Orchestration</title>
      <link>https://networkop.co.uk/blog/2017/11/23/os-nfv-mano/</link>
      <pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/11/23/os-nfv-mano/</guid>
      <description>

&lt;p&gt;In the ongoing hysteria surrounding all things SDN, one important thing gets often overlooked. You don&amp;rsquo;t build SDN for its own sake. SDN is just a little cog in a big machine called &amp;ldquo;cloud&amp;rdquo;. To take it even further, I would argue that the best SDN solution is the one that you don&amp;rsquo;t know even exists. Despite what the big vendors tell you, operators are not supposed to interact with SDN interface, be it GUI or CLI. If you dig up some of the earliest presentation about Cisco ACI, when the people talking about it were the actual people who designed the product, you&amp;rsquo;ll notice one common motif being repeated over and over again. That is that ACI was never designed for direct human interaction, but rather was supposed to be configured by a higher level orchestrating system. In data center environments such orchestrating system may glue together services of virtualization layer and SDN layer to provide a seamless &amp;ldquo;cloud&amp;rdquo; experience to the end users. The focus of this post will be one incarnation of such orchestration system, specific to SP/Telco world, commonly known as NFV MANO.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;nfv-mano-for-telco-sdn&#34;&gt;NFV MANO for Telco SDN&lt;/h1&gt;

&lt;p&gt;At the early dawn of SDN/NFV era a lot of people got very excited by &lt;strong&gt;&amp;ldquo;the promise&amp;rdquo;&lt;/strong&gt; and started applying the disaggregation and virtualization paradigms to all areas of networking. For Telcos that meant virtualizing network functions that built the service core of their networks - EPC, IMS, RAN. Traditionally those network functions were a collection of vertically-integrated baremetal appliances that took a long time to commission and had to be overprovisioned to cope with the peak-hour demand. Virtualizing them would have made it possible to achieve quicker time-to-market, elasticity to cope with a changing network demand and hardware/software disaggregation.&lt;/p&gt;

&lt;p&gt;As expected however, such fundamental change has to come at price. Not only do Telcos get a new virtualization platform to manage but they also need to worry about lifecycle management and end-to-end orchestration (MANO) of VNFs. Since any such change presents an opportunity for new streams of revenue, it didn&amp;rsquo;t take long for vendors to jump on the bandwagon and start working on a new architecture designed to address those issues.&lt;/p&gt;

&lt;p&gt;The first problem was the easiest to solve since VMware and OpenStack already existed at that stage and could be used to host VNFs with very little modifications. The management and orchestration problem, however, was only partially solved by existing orchestration solutions. There were a lot of gaps between the current operational model and the new VNF world and although these problems could have been solved by Telcos engaging themselves with the open-source community, this proved to be too big of a change for them and they&amp;rsquo;ve turned to the only thing they could trust - the standards bodies.&lt;/p&gt;

&lt;h1 id=&#34;etsi-mano&#34;&gt;ETSI MANO&lt;/h1&gt;

&lt;p&gt;ETSI NFV MANO working group has set out to define a reference architecture for management and orchestration of virtualized resources in Telco data centers. The goal of NFV MANO initiative was to do a research into what&amp;rsquo;s required to manage and orchestrate VNFs, what&amp;rsquo;s currently available and identify potential gaps for other standards bodies to fill. Initial ETSI NFV Release 1 (2014) defined a base framework through relatively weak requirements and recommendations and was followed by Release 2 (2016) that made them more concrete by locking down the interfaces and data model specifications. For a very long time Release 1 was the only available NFV MANO standard, which led to a lot of inconsistencies in each vendors&amp;rsquo; implementations of it. This was very frustrating for Telcos since it required a lot of integration effort to build a multi-vendor MANO stack. Another potential issue with ETSI MANO standard is its limited scope - a lot of critical components like OSS and EMS are left outside of it which created a lot of confusion for Telcos and resulted in other standardisation efforts addressing those gaps.&lt;/p&gt;

&lt;p&gt;On the below diagram I have shown an adbridged version of the original ETSI MANO &lt;a href=&#34;https://www.ietf.org/proceedings/88/slides/slides-88-opsawg-6.pdf&#34; target=&#34;_blank&#34;&gt;reference architecture diagram&lt;/a&gt; adapted to the use case I&amp;rsquo;ll be demonstrating in this post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/etsi-mano.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This architecture consists of the following building blocks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NFVI&lt;/strong&gt; (NFV Infrastructure) - OpenStacks compute or VMware&amp;rsquo;s ESXI nodes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VIM&lt;/strong&gt; (Virtual Infrastructure Manager) - OpenStack&amp;rsquo;s controller/API or VMware&amp;rsquo;s vCenter nodes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VNFM&lt;/strong&gt; (VNF Manager) - an element responsible for lifecycle management (create,delete,scale) and monitoring of VNFs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NFVO&lt;/strong&gt; (NFV Orchestrator) - an element responsible for lifecyle management of Network Services (described below)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these elements are working together towards a single goal - managing and orchestrating a Network Service (NS), which itself is comprised of multiple VNFs, Virtual Links (VLs), VNF Forwarding Graphs (VNFFGs) and Physical Network Functions (PNFs). In this post I create a NS for a simple virtual IDS use case, described in my previous &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/15/os-sfc-skydive/&#34;&gt;SFC post&lt;/a&gt;. The goal is to steer all ICMP traffic coming from VM1 through a vIDS VNF which will forward the traffic to its original destination.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/vids-created.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Before I get to the implementation, let me give a quick overview of how a Network Service is build from its constituent parts, in the context of our vIDS use case.&lt;/p&gt;

&lt;h1 id=&#34;relationship-between-ns-vnf-and-vnffg&#34;&gt;Relationship between NS, VNF and VNFFG&lt;/h1&gt;

&lt;p&gt;According to ETSI MANO, a &lt;strong&gt;Network Service&lt;/strong&gt; (NS) is a subset of end-to-end service implemented by VNFs and instantiated on the NFVI. As I&amp;rsquo;ve mentioned before, some examples of a NS would be vEPC, vIMS or vCPE. NS can be described in either a YANG or a Tosca template called NS Descriptor (NSD). The main goal of a NSD is to tie together VNFs, VLs, VNFFGs and PNFs by defining relationship between various templates describing those objects (VNFDs, VLDs, VNFFGDs). Once NSD is onboarded (uploaded), it can be instantiated by NFVO, which communicates with VIM and VNFM to create the constituent components and stitch them together as described in a template. NSD normally does not contain VNFD or VNFFGD templates, but imports them through their names, which means that in order to instantiate a NSD, the corresponding VNFDs and VNFFGDs should already be onboarded.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/vids-nsd.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VNF Descriptor&lt;/strong&gt; is a template describing the compute and network parameters of a single VNF. Each VNF consists of one or more VNF components (VNFCs), represented in Tosca as Virtual Deployment Units (VDUs). A VDU is the smallest part of a VNF and can be implemented as either a container or, as it is in our case, a VM. Apart from the usual set of parameters like CPU, RAM and disk, VNFD also describes all the virtual networks required for internal communication between VNFCs, called internal VLs. VNFM can ask VIM to create those networks when the VNF is being instantiated. VNFD also contains a reference to external networks, which are supposed to be created by NFVO. Those networks are used to connect different VNFs together or to connect VNFs to PNFs and other elements outside of NFVI platform. If external VLs are defined in a VNFD, VNFM will need to source them externally, either as input parameters to VNFM or from NFVO. In fact, VNF instantiation by VNFM, as described in Tacker &lt;a href=&#34;https://docs.openstack.org/tacker/latest/user/vnfm_usage_guide.html&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;, is only used for testing purposes and since a VNF only makes sense as a part of a Network Service, the intended way is to use a NSD to instantiate all VNFs in production environment.&lt;/p&gt;

&lt;p&gt;The final component that we&amp;rsquo;re going to use is VNF Forwarding Graph. &lt;strong&gt;VNFFG Descriptor&lt;/strong&gt; is an optional component that describes how different VNFs are supposed to be chained together to form a Network Service. In the absence of VNFFG, VNFs will fall back to the default destination-based forwarding, when the IPs of VNFs forming a NS are either automatically discovered (e.g. through DNS) or provisioned statically. Tacker&amp;rsquo;s implementation of VNFFG is not fully integrated with NSD yet and VNFFGD has to be instantiated separately and, as will be shown below, linked to an already running instance of a Network Service through its ID.&lt;/p&gt;

&lt;h1 id=&#34;using-tacker-to-orchestrate-a-network-service&#34;&gt;Using Tacker to orchestrate a Network Service&lt;/h1&gt;

&lt;p&gt;Tacker is an OpenStack project implementing a generic VNFM and NFVO. At the input it consumes Tosca-based templates, converts them to Heat templates which are then used to spin up VMs on OpenStack. This diagram from Brocade, the biggest Tacker contributor (at least until its acquisition), is the best overview of internal Tacker architecture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/brocade-tacker.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For this demo environment I&amp;rsquo;ll keep using my OpenStack Kolla lab environment described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/08/os-lab-docker/&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;step-1-vim-registration&#34;&gt;Step 1 - VIM registration&lt;/h2&gt;

&lt;p&gt;Before we can start using Tacker, it needs to know how to reach the OpenStack environment, so the first step in the workflow is OpenStack or VIM registration. We need to provide the address of the keystone endpoint along with the admin credentials to give Tacker enough rights to create and delete VMs and SFC objects:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./vim.yaml
auth_url: &#39;http://192.168.133.254:35357/v3&#39;
username: &#39;admin&#39;
password: &#39;admin&#39;
project_name: &#39;admin&#39;
project_domain_name: &#39;Default&#39;
user_domain_name: &#39;Default&#39;
EOF

tacker vim-register --is-default --config-file vim.yaml --description MYVIM KOLLA-OPENSTACK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The successful result can be checked with &lt;code&gt;tacker vim-list&lt;/code&gt; which should report that registered VIM is now reachable.&lt;/p&gt;

&lt;h2 id=&#34;step-2-onboarding-a-vnfd&#34;&gt;Step 2 - Onboarding a VNFD&lt;/h2&gt;

&lt;p&gt;VNFD defines a set of VMs (VNFCs), network ports (CPs) and networks (VLs) and their relationship. In our case we have a single cirros-based VM with a pair of ingress/egress ports. In this template we also define a special node type &lt;code&gt;tosca.nodes.nfv.vIDS&lt;/code&gt; which will be used by NSD to pass the required parameters for ingress and egress VLs. These parameters are going to be used by VNFD to attach network ports (CPs) to virtual networks (VLs) as defined in the &lt;code&gt;substitution_mappings&lt;/code&gt; section.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./vnfd.yaml
tosca_definitions_version: tosca_simple_profile_for_nfv_1_0_0
description = Cirros vIDS example

node_types:
  tosca.nodes.nfv.vIDS:
    requirements:
      - INGRESS_VL:
          type: tosca.nodes.nfv.VL
          required: true
      - EGRESS_VL:
          type: tosca.nodes.nfv.VL
          required: true

topology_template:
  substitution_mappings:
    node_type: tosca.nodes.nfv.vIDS
    requirements:
      INGRESS_VL: [CP1, virtualLink]
      EGRESS_VL:  [CP2, virtualLink]

  node_templates:
    VDU1:
      type: tosca.nodes.nfv.VDU.Tacker
      properties:
        availability_zone: nova
        flavor: m1.nano
        image: cirros
        mgmt_driver: noop
        user_data_format: RAW
        user_data: |
          #!/bin/sh
          sudo cirros-dhcpc up eth1
          sudo ip rule add iif eth0 table default
          sudo ip route add default via 10.0.0.1 dev eth1 table default
          sudo sysctl -w net.ipv4.ip_forward=1

    CP1:
      type: tosca.nodes.nfv.CP.Tacker
      properties:
        anti_spoofing_protection: false
      requirements:
        - virtualBinding:
            node: VDU1

    CP2:
      type: tosca.nodes.nfv.CP.Tacker
      properties:
        anti_spoofing_protection: false
      requirements:
        - virtualBinding:
            node: VDU1
EOF

tacker vnfd-create --vnfd-file vnfd.yaml vIDS-TEMPLATE
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-4-onboarding-a-nsd&#34;&gt;Step 4 - Onboarding a NSD&lt;/h2&gt;

&lt;p&gt;In our use case the NSD template is going to really small. All what we need to define is a single VNF of the &lt;code&gt;tosca.nodes.nfv.vIDS&lt;/code&gt; type that was defined previously in the VNFD. We also define a VL node which points to the pre-existing &lt;code&gt;demo-net&lt;/code&gt; virtual network and pass this VL to both INGRESS_VL and EGRESS_VL parameters of the VNFD.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./nsd.yaml
tosca_definitions_version: tosca_simple_profile_for_nfv_1_0_0
imports:
  - vIDS-TEMPLATE

topology_template:
  node_templates:
    vIDS:
      type: tosca.nodes.nfv.vIDS
      requirements:
        - INGRESS_VL: VL1
        - EGRESS_VL: VL1
    VL1:
      type: tosca.nodes.nfv.VL
      properties:
          network_name: demo-net
          vendor: tacker
EOF

tacker nsd-create --nsd-file nsd.yaml NSD-vIDS-TEMPLATE
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-5-instantiating-a-nsd&#34;&gt;Step 5 - Instantiating a NSD&lt;/h2&gt;

&lt;p&gt;As I&amp;rsquo;ve mentioned before, VNFFG is not integrated with NSD yet, so we&amp;rsquo;ll add it later. For now, we have provided enough information to instantiate our NSD.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tacker ns-create --nsd-name NSD-vIDS-TEMPLATE NS-vIDS-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This last command creates a cirros-based VM with two interfaces and connects them to &lt;code&gt;demo-net&lt;/code&gt; virtual network. All ICMP traffic from VM1 still goes directly to its default gateway so the last thing we need to do is create a VNFFG.&lt;/p&gt;

&lt;h2 id=&#34;step-6-onboarding-and-instantiating-a-vnffg&#34;&gt;Step 6 - Onboarding and Instantiating a VNFFG&lt;/h2&gt;

&lt;p&gt;VNFFG consists of two two types of nodes. The first type defines a Forwarding Path (FP) as a set of virtual ports (CPs) and a flow classifier to build an equivalent service function chain inside the VIM. The second type groups multiple forwarding paths to build a complex service chain graphs, however only one FP is supported by Tacker at the time of writing.&lt;/p&gt;

&lt;p&gt;The following template demonstrates another important feature - template parametrization. Instead of defining all parameters statically in a template, they can be provided as inputs during instantiation, which allows to keep templates generic. In this case I&amp;rsquo;ve replaced the network port id parameter with &lt;code&gt;PORT_ID&lt;/code&gt; variable which will be provided during VNFFGD instantiation.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./vnffg.yaml
tosca_definitions_version: tosca_simple_profile_for_nfv_1_0_0

description = vIDS VNFFG tosca

topology_template:
  inputs:
    PORT_ID:
      type: string
description = Port ID of the target VM

  node_templates:

    Forwarding_Path-1:
      type: tosca.nodes.nfv.FP.Tacker
description = creates path (CP1-&amp;gt;CP2)
      properties:
        id: 51
        policy:
          type: ACL
          criteria:
            - network_src_port_id: { get_input: PORT_ID }
            - ip_proto: 1
        path:
          - forwarder: vIDS-TEMPLATE
            capability: CP1
          - forwarder: vIDS-TEMPLATE
            capability: CP2


  groups:
    VNFFG1:
      type: tosca.groups.nfv.VNFFG
description = Set of Forwarding Paths
      properties:
        vendor: tacker
        version: 1.0
        number_of_endpoints: 1
        dependent_virtual_link: [VL1]
        connection_point: [CP1]
        constituent_vnfs: [vIDS-TEMPLATE]
      members: [Forwarding_Path-1]
EOF

tacker vnffgd-create --vnffgd-file vnffgd.yaml VNFFG-TEMPLATE
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that the VNFFGD has been updated to support multiple flow classifiers which means you many need to update the above template as per the &lt;a href=&#34;https://github.com/openstack/tacker/blob/master/samples/tosca-templates/vnffgd/tosca-vnffgd-multiple-classifiers-sample.yaml&#34; target=&#34;_blank&#34;&gt;sample VNFFGD template&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In order to instantiate a VNFFGD we need to provide two runtime parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;OpenStack port ID of VM1 for forwarding path flow classifier&lt;/li&gt;
&lt;li&gt;ID of the VNF created by the Network Service&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these parameters can be obtained using the CLI commands as shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CLIENT_IP=$(openstack server list | grep VM1 | grep -Eo &#39;[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+&#39;)
PORT_ID=$(openstack port list | grep $CLIENT_IP | awk &#39;{print $2}&#39;)
echo &amp;quot;PORT_ID: $PORT_ID&amp;quot; &amp;gt; params-vnffg.yaml
vIDS_ID=$(tacker ns-show NS-vIDS-1 -f value -c vnf_ids | sed &amp;quot;s/&#39;/\&amp;quot;/g&amp;quot; | jq &#39;.vIDS&#39; | sed &amp;quot;s/\&amp;quot;//g&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command creates a VNFFG and an equivalent SFC to steer all ICMP traffic from VM1 through vIDS VNF. The result can be verified using Skydive following the procedure described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/15/os-sfc-skydive/&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tacker vnffg-create --vnffgd-name VNFFG-TEMPLATE \
                    --vnf-mapping vIDS-TEMPLATE:$vIDS_ID \
                    --param-file params-vnffg.yaml VNFFG-1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;other-tacker-features&#34;&gt;Other Tacker features&lt;/h1&gt;

&lt;p&gt;This post only scratches the surface of what&amp;rsquo;s available in Tacker with a lot of other salient features left out of scope, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VNF monitoring - through monitoring driver its possible to do VNF monitoring from VNFM using various methods ranging from a single ICMP/HTTP ping to Alarm-based monitoring using OpenStack&amp;rsquo;s &lt;a href=&#34;https://wiki.openstack.org/wiki/Telemetry&#34; target=&#34;_blank&#34;&gt;Telemetry framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Enhanced Placement Awareness - VNFD Tosca template extensions that allow the definition of required performance features like NUMA topology mapping, SR-IOV and CPU pinning.&lt;/li&gt;
&lt;li&gt;Mistral workflows - ability to drive Tacker workflows through Mistral&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Tacker is one of &lt;a href=&#34;https://thenewstack.io/opensource-nfv-part-4-opensource-mano/&#34; target=&#34;_blank&#34;&gt;many&lt;/a&gt; NFV orchestration platforms in a very competitive environment. Other &lt;a href=&#34;https://www.mirantis.com/blog/which-nfv-orchestration-platform-best-review-osm-open-o-cord-cloudify/&#34; target=&#34;_blank&#34;&gt;open-source initiatives&lt;/a&gt; have been created in response to the shortcomings of the original ETSI Release 1 reference architecture. The fact the some of the biggest Telcos have finally realised that the only way to achieve the goal of NFV orchestration is to get involved with open-source and do it themselves, may be a good sign for the industry and maybe not so good for the ETSI NFV MANO working group. Whether ONAP with its broader scope becomes a new de-facto standard for NFV orchestration, still remains to be seen, until then ETSI MANO remains the only viable standard for NFV lifecycle management and orchestration.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN - Skydiving Into Service Function Chaining</title>
      <link>https://networkop.co.uk/blog/2017/09/15/os-sfc-skydive/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/09/15/os-sfc-skydive/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;abbr:Service Function Chaining&#34; target=&#34;_blank&#34;&gt;SFC&lt;/a&gt; is another SDN feature that for a long time only used to be available in proprietary SDN solutions and that has recently become available in vanilla OpenStack. It serves as another proof that proprietary SDN solutions are losing the competitive edge, especially for Telco SDN/NFV use cases. Hopefully, by the end of this series of posts I&amp;rsquo;ll manage do demonstrate how to build a complete open-source solution that has feature parity (in terms of major networking features) with all the major proprietary data centre SDN platforms. But for now, let&amp;rsquo;s just focus on SFC.&lt;/p&gt;

&lt;h1 id=&#34;sfc-high-level-overview&#34;&gt;SFC High-level overview&lt;/h1&gt;

&lt;p&gt;In most general terms, SFC refers to packet forwarding technique that uses more than just destination IP address to decide how to forward packets. In more specific terms, SFC refers to &amp;ldquo;steering&amp;rdquo; of traffic through a specific set of endpoints (a.k.a Service Functions), overriding the default destination-based forwarding. For those coming from a traditional networking background, think of SFC as a set of policy-based routing instances orchestrated from a central element (SDN controller). Typical use cases for SFC would be things like firewalling, IDS/IPS, proxying, NAT&amp;rsquo;ing, monitoring.&lt;/p&gt;

&lt;p&gt;SFC is usually modelled as a directed (acyclic) graph, where the first and the last elements are the source and destination respectively and each vertex inside the graph represents a SF to be chained. IETF RFC7665 defines the reference architecture for SFC implementations and establishes some of the basic terminology. A simplified SFC architecture consists of the following main components:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Classifier - a network element that matches and redirects traffic flows to a chain&lt;/li&gt;
&lt;li&gt;Service Function - an element responsible for packet processing&lt;/li&gt;
&lt;li&gt;Service Function Forwarder - a network element that forwards traffic to and from a directly connected SF&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sfc-overview.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;One important property of a SF is elasticity. More instances of the same type can be added to a pool of SF and SFF will load-balance the traffic between them. This is the reason why, as we&amp;rsquo;ll see in the next section, SFF treats connections to a SF as a group of ports rather than just a single port.&lt;/p&gt;

&lt;h1 id=&#34;insertion-modes-and-implementation-models&#34;&gt;Insertion modes and implementation models&lt;/h1&gt;

&lt;p&gt;In legacy, pre-SDN environments SFs had no idea if they were a part of a service chain and network devices (routers and switches) had to &amp;ldquo;insert&amp;rdquo; the interesting traffic into the service function using one of the following two modes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;L2 mode&lt;/strong&gt; is when SF is physically inserted between the source and destination inside a single broadcast domain, so traffic flows through a SF without any intervention from a switch. Example of this mode could be a firewall in transparent mode, physically connected between a switch and a default gateway router. All packets entering a SF have their original source and destination MAC addresses, which requires SF to be in promiscuous mode.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;L3 mode&lt;/strong&gt; is when a router overrides its default destination-based forwarding and redirects the interesting traffic to a SF. In legacy networks this could have been achieved with PBR or WCCP. In this case SF needs to be L2-attached to a router and all redirected packets have their destination MAC updated to that of a SF&amp;rsquo;s ingress interface.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Modern SDN networks make it really easy to modify forwarding behaviour of network elements, both physical and virtual. There is no need for policy-based routing or bump-in-the-wire designs anymore. When flow needs to be redirected to a SF on a virtual switch, all what&amp;rsquo;s required is a matching OpenFlow entry with a high enough priority. However redirecting traffic to a SF is just one part of the problem. Another part is how to make SFs smarter, to provide greater visibility of end-to-end service function path.&lt;/p&gt;

&lt;p&gt;So far SFs have only been able to extract metadata from the packet itself. This limited the flexibility of SF logic and became computationally expensive in case many SFs need to access some L7 header information. Ideal way would be to have an additional header which can be used to read and write arbitrary information and pass it along the service function chain. RFC7665 defines requirements for &amp;ldquo;SFC Encapsulation&amp;rdquo; header which can be used to uniquely identify an instance of a chain as well as share metadata between all its elements. Neutron API refers to SFC encapsulation as &lt;em&gt;correlation&lt;/em&gt; since its primary function is to identify a particular service function path. There are two implementations of SFC encapsulation in use today:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MPLS&lt;/strong&gt; - used by current OVS agent driver (as of Pike). This method does not provide any means to share metadata and serves only for SFP identification. It is intended as an interim solution until NSH becomes available upstream in OVS.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NSH&lt;/strong&gt; - complete implementation of SFC encapsulation defined in RFC7665. This method is currently implemented in Opendaylight where NSH is used as a shim between VXLAN-GPE and the encapsulated packet&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It should be noted that the new approach with SFC encapsulation still allows for legacy, non-SFC-aware SFs to be chained. In this case SFC encapsulation is stripped off the packet by an &amp;ldquo;SFC proxy&amp;rdquo; before the packet is sent to the ingress port of a service function. All logical elements forming an SFC forwarding pipeline, including SFC proxy, Classifier and Forwarder, are implemented inside the same OVS bridges (br-int and br-tun) used by vanilla OVS-agent driver.&lt;/p&gt;

&lt;h1 id=&#34;configuring-neutron-sfc&#34;&gt;Configuring Neutron SFC&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;ll pick up where we left off in the &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/08/os-lab-docker/&#34;&gt;previous post&lt;/a&gt;. All Neutron and ML2 configuration files have already been updated thanks to the &lt;code&gt;enable_sfc=&amp;quot;yes&amp;quot;&lt;/code&gt; setting in the global Kolla-Ansible configuration file. If not, you can change it in &lt;code&gt;/etc/kolla/globals.yaml&lt;/code&gt; and re-run kolla-ansible deployment script.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s generate OpenStack credentials using a post-deployment script. We later can use a default bootstrap script to downloads the cirros image and set up some basic networking and security rules.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kolla-ansible post-deploy
source /etc/kolla/admin-openrc.sh
/usr/share/kolla-ansible/init-runonce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The goal for this post is to create a simple uni-directional SFC to steer the ICMP requests from VM1 to its default gateway through another VM that will be playing the role of a firewall.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sfc-example.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The network was already created by the bootstrap script so all what we have to do is create a test VM. I&amp;rsquo;m creating a port in a separate step simply so that I can refer to it by name instead of UUID.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack port create --network demo-net P0
openstack server create --image cirros --flavor m1.tiny --port P0 VM1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;rsquo;ll go over all the necessary steps to setup SFC, but will only provide a brief explanation. Refer to the official OpenStack &lt;a href=&#34;https://docs.openstack.org/newton/networking-guide/config-sfc.html&#34; target=&#34;_blank&#34;&gt;Networking Guide&lt;/a&gt; for a complete SFC configuration guide.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s create a FW VM with two ports - P1 and P2.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack port create --network demo-net P1
openstack port create --network demo-net P2
openstack server create --image cirros --flavor m1.tiny --port P1 --port P2 FW
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we need create an ingress/egress port pair and assign it to a port pair group. The default setting for &lt;strong&gt;correlation&lt;/strong&gt; in a port pair (not shown) is &lt;code&gt;none&lt;/code&gt;. That means that SFC encapsulation header (MPLS) will get stripped before the packet is sent to P1.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack sfc port pair create --ingress P1 --egress P2 PPAIR
openstack sfc port pair group create --port-pair PPAIR PPGROUP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Port pair group also allows to specify the L2-L4 headers which to use for load-balancing in OpenFlow groups, overriding the default behaviour described in the next section.&lt;/p&gt;

&lt;p&gt;Another required element is a flow classifier. We will be redirecting ICMP traffic coming from VM1&amp;rsquo;s port P0&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack sfc flow classifier create --protocol icmp --logical-source-port P0 FLOW-ICMP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we can tie together flow classifier with a previously created port pair group. The default setting for &lt;strong&gt;correlation&lt;/strong&gt; (not shown again) in this case is &lt;code&gt;mpls&lt;/code&gt;. That means that each chain will have its own unique MPLS label to be used as an SFC encapsulation.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack sfc port chain create --port-pair-group PPGROUP --flow-classifier FLOW-ICMP PCHAIN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s all the configuration needed to setup SFC. However if you login VM1&amp;rsquo;s console and try pinging default gateway, it will fail. Next, I&amp;rsquo;m going to give a quick demo of how to use a real-time network analyzer tool called Skydive to troubleshoot this issue.&lt;/p&gt;

&lt;h1 id=&#34;using-skydive-to-troubleshoot-sfc&#34;&gt;Using Skydive to troubleshoot SFC&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://skydive-project.github.io/skydive/&#34; target=&#34;_blank&#34;&gt;Skydive&lt;/a&gt; is a new open-source distributed network probing and traffic analyzing tool. It consists of a set of agents running on compute nodes, collecting topology and flow information and forwarding it to a central element for analysis.&lt;/p&gt;

&lt;p&gt;The idea of using Skydive to analyze and track SFC is not new. In fact, for anyone interested in this topic I highly recommend the &lt;a href=&#34;http://blog.cafarelli.fr/2017/02/tracking-service-function-chaining-with-skydive/&#34; target=&#34;_blank&#34;&gt;following blogpost&lt;/a&gt;. In my case I&amp;rsquo;ll show how to use Skydive from a more practical perspective - troubleshooting multiple SFC issues.&lt;/p&gt;

&lt;p&gt;Skydive CLI client is available inside the &lt;code&gt;skydive_analyzer&lt;/code&gt; container. We need to start an interactive bash session inside this container and set some environment variables:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker exec -it skydive_analyzer bash
export SKYDIVE_ANALYZERS=192.168.133.100:8085
export SKYDIVE_USERNAME=admin
export SKYDIVE_PASSWORD=admin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first thing we can do to troubleshoot is see if ICMP traffic is entering the &lt;code&gt;ingress&lt;/code&gt; port of the FW VM. Based on the output of &lt;code&gt;openstack port list&lt;/code&gt; command I know that P1 has got an IP of &lt;code&gt;10.0.0.8&lt;/code&gt;. Let&amp;rsquo;s if we can identify a tap port corresponding to P1:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;skydive client topology query --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.8&#39;, &#39;Type&#39;, &#39;tun&#39;).Values(&#39;Neutron&#39;)&amp;quot;
{
  &amp;quot;IPs&amp;quot;: &amp;quot;10.0.0.8&amp;quot;,
  &amp;quot;NetworkID&amp;quot;: &amp;quot;8eabb451-b026-417c-b54b-8e79ee6e71c3&amp;quot;,
  &amp;quot;NetworkName&amp;quot;: &amp;quot;demo-net&amp;quot;,
  &amp;quot;PortID&amp;quot;: &amp;quot;e6334df9-a5c4-4e86-a5f3-671760c2bbbe&amp;quot;,
  &amp;quot;TenantID&amp;quot;: &amp;quot;bd5829e0cb5b40b68ab4f8e7dc68b14d&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output above proves that skydive agent has successfully read the configuration of the port and we can start a capture on that object to see any packets arriving on P1.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;skydive client capture create --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.8&#39;, &#39;Type&#39;, &#39;tun&#39;)&amp;quot;
skydive client topology query --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.8&#39;, &#39;Type&#39;, &#39;tun&#39;).Flows().Has(&#39;Application&#39;,&#39;ICMPv4&#39;).Values(&#39;Metric.ABPackets&#39;)&amp;quot;
[
  7
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you &lt;code&gt;watch&lt;/code&gt; the last command for several seconds you should see that the number in brackets is increasing. That means that packets are hitting the ingress port of the FW VM. Now let&amp;rsquo;s repeat the same test on &lt;code&gt;egress&lt;/code&gt; port P2.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;skydive client capture create --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.4&#39;, &#39;Type&#39;, &#39;tun&#39;)&amp;quot;
skydive client topology query --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.4&#39;, &#39;Type&#39;, &#39;tun&#39;).Flows()&amp;quot;
[]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output above tells us that there are no packets coming out of the FW VM. This is expected since we haven&amp;rsquo;t done any changes to the blank cirros image to make it forward the packets between the two interfaces. If we examine the IP configuration of the FW VM, we would see that it doesn&amp;rsquo;t have an IP address configured on the second interface. We would also need to create a source-based routing policy to force all traffic from VM1 (&lt;code&gt;10.0.0.6&lt;/code&gt;) to egress via interface &lt;code&gt;eth2&lt;/code&gt; and make sure IP forwarding is turned on. The following commands would need to be executed on FW VM:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo cirros-dhcpc up eth1
sudo ip rule add from 10.0.0.6 table default
sudo ip route add default via 10.0.0.1 dev eth1 table default
sudo sysctl -w net.ipv4.ip_forward=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Having done that, we should see some packets coming out of &lt;code&gt;egress&lt;/code&gt; port P2.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;skydive client topology query --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.4&#39;, &#39;Type&#39;, &#39;tun&#39;).Flows().Has(&#39;Application&#39;,&#39;ICMPv4&#39;).Values(&#39;Metric.ABPackets&#39;)&amp;quot;
[
  7
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However form the VM1&amp;rsquo;s perspective the ping is still failing. Next step would be to see if the packets are hitting the integration bridge that port P2 is attached to:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;skydive client capture create --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.4&#39;, &#39;Type&#39;, &#39;veth&#39;)&amp;quot;
skydive client topology query --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.4&#39;, &#39;Type&#39;, &#39;veth&#39;).Flows()&amp;quot;
[]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;No packets means they are getting dropped somewhere between the P2 and the integration bridge. This can only be done by security groups. In fact, source MAC/IP anti-spoofing is enabled by default which would only allow packets matching the source MAC/IP addresses assigned to P2 and would drop any packets coming from VM1&amp;rsquo;s IP address. The easiest fix would be to disable security groups for P2 completely:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack port set --no-security-group --disable-port-security P2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After this step the counters should start incrementing and the ping from VM1 to its default gateway is resumed.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;skydive client topology query --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.4&#39;, &#39;Type&#39;, &#39;veth&#39;).Flows().Has(&#39;Application&#39;,&#39;ICMPv4&#39;).Values(&#39;Metric.ABPackets&#39;)&amp;quot;
[
  79
]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;sfc-implementation-in-ovs-forwarding-pipeline&#34;&gt;SFC implementation in OVS forwarding pipeline&lt;/h1&gt;

&lt;p&gt;The only element being affected in our case (both VM1 and FW are on the same compute node) is the integration bridge. Refer to my &lt;a href=&#34;http://networkop.co.uk/blog/2016/04/22/neutron-native/&#34; target=&#34;_blank&#34;&gt;older post&lt;/a&gt; about vanilla OpenStack networking for a refresher of the vanilla OVS-agent architecture.&lt;/p&gt;

&lt;p&gt;Normally, I would start by collecting all port and flow details from the integration bridge with the following commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ovs-ofctl dump-ports-desc br-int  | grep addr
ovs-ofctl dump-flows br-int | cut -d &#39;,&#39; -f3-
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, for the sake of brevity, I will omit the actual outputs and only show graphical representation of forwarding tables and packet flows. The tables below have two columns - first showing what is being matched and second showing the resulting action. Let&amp;rsquo;s start with the OpenFlow rules in an integration bridge before SFC is configured:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sfc-before-tables.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, the table structure is quite simple, since integration bridge mostly relies on data-plane MAC learning. A couple of MAC and ARP anti-spoofing tables will check the validity of a packet and send it to table 60 where &lt;code&gt;NORMAL&lt;/code&gt; action will trigger the &amp;ldquo;flood-and-learn&amp;rdquo; behaviour. Therefore, an ICMP packet coming from VM1 will take the following path:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sfc-before-packet.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After we&amp;rsquo;ve configured SFC, the forwarding pipeline is changed and now looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sfc-after-tables.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First, we can see that table 0 acts as a classifier, by redirecting the &amp;ldquo;interesting&amp;rdquo; packets towards &lt;code&gt;group 1&lt;/code&gt;. This groups is an &lt;a href=&#34;https://floodlight.atlassian.net/wiki/spaces/floodlightcontroller/pages/7995427/How+to+Work+with+Fast-Failover+OpenFlow+Groups&#34; target=&#34;_blank&#34;&gt;OpenFlow Group&lt;/a&gt; of type &lt;code&gt;select&lt;/code&gt;, which load-balances traffic between multiple destinations. By default OVS will use a combination of L2-L4 header as described &lt;a href=&#34;http://docs.openvswitch.org/en/latest/faq/openflow/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; to calculate a hash which determines the output bucket, similar to how per-flow load-balancing works in traditional routers and switches. This behaviour can be overridden with a specific set of headers in &lt;code&gt;lb_fields&lt;/code&gt; setting of a port pair group.&lt;/p&gt;

&lt;p&gt;In our case we&amp;rsquo;ve only got a single SF, so the packet gets its destination MAC updated to that of SF&amp;rsquo;s ingress port and is forwarded to a new table 5. Table 5 is where all packets destined for a SF are aggregated with a single MPLS label which uniquely identifies the service function path. The packet is then forwarded to table 10, which I&amp;rsquo;ve called &lt;code&gt;SFC Ingress&lt;/code&gt;. This is where the packets are distributed to SF&amp;rsquo;s ingress ports based on the assigned MPLS label.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sfc-after-packet.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After being processed by a SF, the packet leaves the &lt;code&gt;egress&lt;/code&gt; port and re-enters the integration bridge. This time table 0 knows that the packet has already been processed by a SF and, since the anti-spoofing rules have been disabled, simply floods the packet out of all ports in the same VLAN. The packet gets flooded to the tunnel bridge where it gets replicated and delivered to the &lt;code&gt;qrouter&lt;/code&gt; sitting on the controller node as per the &lt;a href=&#34;http://networkop.co.uk/blog/2016/04/22/neutron-native/&#34; target=&#34;_blank&#34;&gt;default behaviour&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;upcoming-enhancements&#34;&gt;Upcoming enhancements&lt;/h1&gt;

&lt;p&gt;SFC is a pretty vast topic and is still under active development. Some of the upcoming enhancement to the current implementation of SFC will include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NSH&lt;/strong&gt; header for SFC correlation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TAP&lt;/strong&gt; functionality which can replace the separate Tap-as-a-service OpenStack project&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service graphs&lt;/strong&gt; allowing multiple chains to be interconnected to create more complex service chain scenarios&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;coming-up&#34;&gt;Coming Up&lt;/h1&gt;

&lt;p&gt;SFC is one of the major features in Telco SDN and, like many things, it&amp;rsquo;s not meant to be configured manually. In fact, Telco SDN have their own framework for management and orchestration of VNFs (a.k.a. VMs) and VNF forwarding graphs (a.k.a. SFCs) called ETSI MANO. As it is expected from a Telco standard, it abounds with acronyms and confuses the hell out of anyone who&amp;rsquo;s name is not on the list of authors or contributors. That&amp;rsquo;s why in the next post I will try to provide a brief overview of what Telco SDN is and use Tacker, a software implementation of NFVO and VNFM, to automatically build a firewall VNF and provision a SFC, similar to what has been done in this post manually.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN - Building a Containerized OpenStack Lab</title>
      <link>https://networkop.co.uk/blog/2017/09/08/os-lab-docker/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/09/08/os-lab-docker/</guid>
      <description>

&lt;p&gt;For quite a long time installation and deployment have been deemed as major barriers for OpenStack adoption. The classic &amp;ldquo;install everything manually&amp;rdquo; approach could only work in small production or lab environments and the ever increasing number of project under the &lt;a href=&#34;https://governance.openstack.org/tc/reference/projects/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;Big Tent&amp;rdquo;&lt;/a&gt; made service-by-service installation infeasible. This led to the rise of automated installers that over time evolved from a simple collection of scripts to container management systems.&lt;/p&gt;

&lt;h1 id=&#34;evolution-of-automated-openstack-installers&#34;&gt;Evolution of automated OpenStack installers&lt;/h1&gt;

&lt;p&gt;The first generation of automated installers were simple utilities that tied together a collection of Puppet/Chef/Ansible scripts. Some of these tools could do baremetal server provisioning through Cobbler or Ironic (Fuel, Compass) and some relied on server operating system to be pre-installed (Devstack, Packstack). In either case the packages were pulled from the Internet or local repository every time the installer ran.&lt;/p&gt;

&lt;p&gt;The biggest problem with the above approach is the time it takes to re-deploy, upgrade or scale the existing environment. Even for relatively small environments it could be hours before all packages are downloaded, installed and configured. One of the ways to tackle this is to pre-build an operating system with all the necessary packages and only use Puppet/Chef/Ansible to change configuration files and turn services on and off. Redhat&amp;rsquo;s TripleO is one example of this approach. It uses a &amp;ldquo;golden image&amp;rdquo; with pre-installed OpenStack packages, which is dd-written bit-by-bit onto the baremetal server&amp;rsquo;s disk. The undercloud then decides which services to turn on based on the overcloud server&amp;rsquo;s role.&lt;/p&gt;

&lt;p&gt;Another big problem with most of the existing deployment methods was that, despite their microservices architecture, all OpenStack services were deployed as static packages on top of a shared operating system. This made the ongoing operations, troubleshooting and ugprades really difficult. The obvious thing to do would be to have all OpenStack services (e.g. Neutron, Keyston, Nova) deployed as containers and managed by a container management system. The first company to implement that, as far as I know, was Canonical. The deployment process is quite complicated, however the end result is a highly flexible OpenStack cloud deployed using LXC containers, managed and orchestrated by Juju controller.&lt;/p&gt;

&lt;p&gt;Today (September 2017) deploying OpenStack services as containers is becoming mainstream and in this post I&amp;rsquo;ll show how to use Kolla to build container images and Kolla-Ansible to deploy them on a pair of &amp;ldquo;baremetal&amp;rdquo; VMs.&lt;/p&gt;

&lt;h1 id=&#34;lab-overview&#34;&gt;Lab overview&lt;/h1&gt;

&lt;p&gt;My lab consists of a single controller and a single compute VM. The goal was to make them as small as possible so they could run on a laptop with limited resources. Both VMs are connected to three VM bridged networks - provisioning, management and external VM access.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/kolla-lab.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve written some bash and Ansible scripts to automate the deployment of VMs on top of any Fedora derivative (e.g. Centos7). These scripts should be run directly from the hypervisor:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/networkop/kolla-odl-bgpvpn.git &amp;amp;&amp;amp; cd kolla-odl-bgpvpn
./1-create.sh do
./2-bootstrap.sh do
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first bash script downloads the VM OS (Centos7), creates two blank VMs and sets up a local Docker registry. The second script installs all the dependencies, including Docker and Ansible.&lt;/p&gt;

&lt;h1 id=&#34;building-openstack-docker-containers-with-kolla&#34;&gt;Building OpenStack docker containers with Kolla&lt;/h1&gt;

&lt;p&gt;The first step in Kolla deployment workflow is deciding where to get the Docker images. Kolla maintains a &lt;a href=&#34;https://hub.docker.com/u/kolla/&#34; target=&#34;_blank&#34;&gt;Docker Hub registry&lt;/a&gt; with container images built for every major OpenStack release. The easiest way to get them would be to pull the images from Docker hub either directly or via a &lt;a href=&#34;https://docs.docker.com/registry/recipes/mirror/&#34; target=&#34;_blank&#34;&gt;pull-through caching registry&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In my case I needed to build the latest version of OpenStack packages, not just the latest major release. I also wanted to build a few additional, non-OpenStack images (Opendaylight and Quagga). Because of that I had to build all Docker images locally and push them into a local docker registry. The procedure to build container images is very well documented in the official &lt;a href=&#34;https://docs.openstack.org/kolla/latest/image-building.html&#34; target=&#34;_blank&#34;&gt;Kolla image building guide&lt;/a&gt;. I&amp;rsquo;ve modified it slightly to include the Quagga Dockerfile and automated it so that the whole process can be run with a single command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./3-build.sh do
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This step can take quite a long time (anything from 1 to 4 hours depending on the network and disk I/O speed), however, once it&amp;rsquo;s been done these container images can be used to deploy as many OpenStack instances as necessary.&lt;/p&gt;

&lt;h1 id=&#34;deploying-openstack-with-kolla-ansible&#34;&gt;Deploying OpenStack with Kolla-Ansible&lt;/h1&gt;

&lt;p&gt;The next step in OpenStack deployment workflow is to deploy Docker images on target hosts. &lt;a href=&#34;https://docs.openstack.org/kolla-ansible/latest/quickstart.html&#34; target=&#34;_blank&#34;&gt;Kolla-Ansible&lt;/a&gt; is a highly customizable OpenStack deployment tool that is also extemely easy to use, at least for people familiar with Ansible. There are two main sources of information for Kolla-Ansible:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Global configuration file (/etc/kolla/globals.yaml), which contains some of the most common customization options&lt;/li&gt;
&lt;li&gt;Ansible inventory file (/usr/share/kolla-ansible/ansible/inventory/*), which maps OpenStack packages to target deployment hosts&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To get started with Kolla-Ansible all what it takes is a few modifications to the global configuration file to make sure that network settings match the underlying OS interface configuration and an update to the inventory file to point it to the correct deployment hosts. In my case I&amp;rsquo;m making additional changes to enable SFC, Skydive and Tacker and adding files for Quagga container, all of which can be done with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./4-deploy.sh do
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The best thing about this method of deployment is that it takes (in my case) under 5 minutes to get the full OpenStack cloud from scratch. That means if I break something or want to redeploy with some major changes (add/remove Opendaylight), all what I have to do is destroy the existing deployment (approx. 1 minute), modify global configuration file and re-deploy OpenStack. This makes Kolla-Ansible an ideal choice for my lab environment.&lt;/p&gt;

&lt;h1 id=&#34;overview-of-containerized-openstack&#34;&gt;Overview of containerized OpenStack&lt;/h1&gt;

&lt;p&gt;Once the deployment has been completed, we should be able to see a number of running Docker containers - one for each OpenStack process.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@compute-1# docker ps
CONTAINER ID        IMAGE                                                                 COMMAND             CREATED             STATUS              PORTS               NAMES
0bb8a8eeb1a9        172.26.0.1:5000/kolla/centos-source-skydive-agent:5.0.0               &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               skydive_agent
63b5b643dfae        172.26.0.1:5000/kolla/centos-source-neutron-openvswitch-agent:5.0.0   &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               neutron_openvswitch_agent
f6f74c5982cb        172.26.0.1:5000/kolla/centos-source-openvswitch-vswitchd:5.0.0        &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               openvswitch_vswitchd
3078421a3892        172.26.0.1:5000/kolla/centos-source-openvswitch-db-server:5.0.0       &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               openvswitch_db
9146c16d561b        172.26.0.1:5000/kolla/centos-source-nova-compute:5.0.0                &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               nova_compute
8079f840627f        172.26.0.1:5000/kolla/centos-source-nova-libvirt:5.0.0                &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               nova_libvirt
220d617d31a5        172.26.0.1:5000/kolla/centos-source-nova-ssh:5.0.0                    &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               nova_ssh
743ce602d485        172.26.0.1:5000/kolla/centos-source-cron:5.0.0                        &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               cron
8b71f08d2781        172.26.0.1:5000/kolla/centos-source-kolla-toolbox:5.0.0               &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               kolla_toolbox
f76d0a7fcf2a        172.26.0.1:5000/kolla/centos-source-fluentd:5.0.0                     &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               fluentd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All the standard docker tools are available to interact with those containers. For example, this is how we can see what processes are running inside a container:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@compute-1# docker exec nova_compute ps -www aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
nova         1  0.0  0.0    188     4 pts/3    Ss+  Sep04   0:00 /usr/local/bin/dumb-init /bin/bash /usr/local/bin/kolla_start
nova         7  0.7  1.3 2292560 134896 ?      Ssl  Sep04  35:33 /var/lib/kolla/venv/bin/python /var/lib/kolla/venv/bin/nova-compute
root        86  0.0  0.3 179816 32900 ?        S    Sep05   0:00 /var/lib/kolla/venv/bin/python /var/lib/kolla/venv/bin/privsep-helper --config-file /etc/nova/nova.conf --privsep_context vif_plug_ovs.privsep.vif_plug --privsep_sock_path /tmp/tmpFvP0GS/privsep.sock
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some of you may have noticed that none of the containers expose any ports. So how do they communicate? The answer is very simple - all containers run in a &lt;strong&gt;host&lt;/strong&gt; networking mode, effectively disabling any network isolation and giving all contaners access to TCP/IP stacks of their Docker hosts. This is a simple way to avoid having to deal with Docker networking complexities, while at the same time preserving the immutability and portability of Docker containers.&lt;/p&gt;

&lt;p&gt;All containers are configured to restart in case of a failure, however there&amp;rsquo;s no &lt;a href=&#34;abbr:Container Management System&#34; target=&#34;_blank&#34;&gt;CMS&lt;/a&gt; to provide full lifecycle management and advanced scheduling. If upgrade of scale-in/out is needed, Kolla-Ansible will have to be re-run with updated configuration options. There is sibling project called &lt;a href=&#34;https://github.com/openstack/kolla-kubernetes&#34; target=&#34;_blank&#34;&gt;Kolla-Kubernetes&lt;/a&gt; (still under developement), that&amp;rsquo;s designed to address some of the mentioned shortcomings.&lt;/p&gt;

&lt;h1 id=&#34;coming-up&#34;&gt;Coming up&lt;/h1&gt;

&lt;p&gt;Now that the lab is up we can start exploring the new OpenStack SDN features. In the next post I&amp;rsquo;ll have a close look at Neutron&amp;rsquo;s &lt;a href=&#34;abbr: Service Function Chainng&#34; target=&#34;_blank&#34;&gt;SFC&lt;/a&gt; feature, how to configure it and how it&amp;rsquo;s been implemented in OVS forwarding pipeline.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux SSH Session Management for Network Engineers</title>
      <link>https://networkop.co.uk/blog/2017/05/12/linux-ssh/</link>
      <pubDate>Fri, 12 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/05/12/linux-ssh/</guid>
      <description>

&lt;p&gt;A few weeks ago I bought myself a new Dell XPS-13 and decided for the n-th time to go all-in Linux, that is to have Linux as the main and only laptop OS. Since most of my Linux experience is with Fedora-family distros, I quickly installed Fedora-25 and embarked on a long and painful journey of getting out of my Windows comfort zone and re-establishing it in Linux. One of the most important aspects for me, as a network engineer, is to have a streamlined process of accessing network devices. In Windows I was using MTPutty and it helped define my expectations of an ideal SSH session manager:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I want a multi-tab terminal with the ability to switch between tabs quickly - default (GNOME) terminal does that out-of-the box with no extra modifications&lt;/li&gt;
&lt;li&gt;I want to login the device without having to enter a password - Not available by default but is possible with some dirty &lt;code&gt;expect&lt;/code&gt; hacks.&lt;/li&gt;
&lt;li&gt;I want my SSH sessions to be organised in a hierarchical manner with groups representing various administrative domains - customer A, local VMs, lab.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although GNOME terminal looked like a very good option, it didn&amp;rsquo;t meet all of my requirements. I briefly looked and PAC Manager and GNOME Connection Manager but quickly dismissed them due to their ugliness and clunkiness. Ideally I wanted to keep using GNOME terminal as the main terminal emulator, without having to configure and rely on other 3rd party apps. I also didn&amp;rsquo;t want to wrap my SSH session in &lt;code&gt;expect&lt;/code&gt; as I didn&amp;rsquo;t want my password to be pasted in my screen every time I &lt;em&gt;cat&lt;/em&gt; a file containing the trigger keyword &lt;em&gt;Password:&lt;/em&gt;. I&amp;rsquo;ve finally managed to make everything work inside the native GNOME terminal and this post is a documentation of my approach.&lt;/p&gt;

&lt;h1 id=&#34;1-install-ssh-copy-net&#34;&gt;1. Install ssh-copy-net&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;ve written a little &lt;a href=&#34;https://github.com/networkop/ssh-copy-net&#34; target=&#34;_blank&#34;&gt;tool&lt;/a&gt; that uses &lt;a href=&#34;https://github.com/ktbyers/netmiko&#34; target=&#34;_blank&#34;&gt;Netmiko&lt;/a&gt; to install (and remove) public SSH keys onto network devices. Assuming &lt;code&gt;python-pip&lt;/code&gt; is already installed here&amp;rsquo;s what&amp;rsquo;s required to download and install &lt;code&gt;ssh-copy-net&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pip install git+https://github.com/networkop/ssh-copy-net.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Its functionality mimics the one of &lt;code&gt;ssh-copy-id&lt;/code&gt;, so the next step is always to upload the public key to the device:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh-copy-net 10.6.142.1 juniper
Username: admin
Password:
All Done!
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;2-define-ssh-config-for-network-devices&#34;&gt;2. Define SSH config for network devices&lt;/h1&gt;

&lt;p&gt;OpenSSH client &lt;a href=&#34;https://linux.die.net/man/5/ssh_config&#34; target=&#34;_blank&#34;&gt;config file&lt;/a&gt; provides a nice way of managing user&amp;rsquo;s SSH sessions. Configuration file allows you to define per-host SSH settings including username, port forwarding options, key checking flags etc. In my case all what I had to do was define IP addresses of my network devices:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Host srx
  HostName 10.6.142.1

Host arista
  HostName 10.6.142.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now I am able to login the device by simply typing its name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh arista
Last login: Sun May  7 10:57:30 2017 from 10.1.2.3
arista-1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;3-define-zsh-aliases&#34;&gt;3. Define zsh aliases&lt;/h1&gt;

&lt;p&gt;The final step is session organisations. For that I&amp;rsquo;ve decided to use zsh aliases and have device groups encoded in the alias name, separated by dashes. For example, if my SRX device was in the &lt;strong&gt;lab&lt;/strong&gt; and Arista was in &lt;strong&gt;Site-51&lt;/strong&gt; of &lt;strong&gt;Customer-A&lt;/strong&gt; this is how I would write my aliases:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;alias lab-srx=&#39;ssh srx&#39;
alias customer-a-site-51-arista=&#39;ssh arista&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;4-multi-pane-sessions-with-tmux&#34;&gt;4. Multi-pane sessions with tmux&lt;/h1&gt;

&lt;p&gt;As a network engineer, I often find myself troubleshooting issues spanning multiple devices, which is why I need multiple tabs inside a single terminal window. Simply pressing Ctrl+T in GNOME terminal opens a new tab and I can switch between tabs using Alt+[1-9]. However what would be really nice is to have a couple of tabs opened side by side so that I can see the logs and compare output on a number of devices at the same time. This is where tmux comes in. It can do much more than this, but I simply use it to have multiple panes inside the same terminal tab:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/tmux.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an example of my tmux configuration file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Automatically set window title
set-window-option -g automatic-rename on
set-option -g set-titles on

# Use Alt-arrow keys without prefix key to switch panes
bind -n M-Left select-pane -L
bind -n M-Right select-pane -R
bind -n M-Up select-pane -U
bind -n M-Down select-pane -D

# Pane splitting keys
bind-key v split-window -h
bind-key s split-window -v

# New key-binding to reset hung SSH sessions
bind-key k respawn-pane -k

# Easy fix for arrow keys inside ssh
set -g default-terminal &amp;quot;xterm&amp;quot;

# Enable mouse mode (tmux 2.1 and above)
set -g mouse on

# Reload tmux config
bind r source-file ~/.tmux.conf

# No delay for escape key press
set -sg escape-time 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;demo&#34;&gt;Demo&lt;/h1&gt;

&lt;p&gt;Now having all the above defined and with the help of zsh command autocompletion, I can login the device with just a few keypresses (shown in square brackets below):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lab  [TAB]
$ lab-  [TAB]
lab-srx
$ lab-  [s][TAB]
$ lab-srx  [ENTER]
--- JUNOS 12.3X48-D30.7 built 2016-04-28 22:37:34 UTC
{primary:node0}
null@srx&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press Ctrl+B v to split the terminal window vertically:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ customer [TAB]
$ customer- [TAB]
customer-a-site-51-arista
$ customer- [a][TAB]
$ customer-a-arista [ENTER]
Last login: Thu May 11 15:28:03 2017 from 10.1.2.3
arista-1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An so on and so forth&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/tmux.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using YANG Models in Ansible to Configure and Verify State of IOS-XE and JUNOS Devices</title>
      <link>https://networkop.co.uk/blog/2017/04/04/ansible-yang/</link>
      <pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/04/04/ansible-yang/</guid>
      <description>

&lt;p&gt;The idea of using Ansible for &lt;a href=&#34;http://networkop.co.uk/blog/2015/08/26/automating-network-build-p1/&#34; target=&#34;_blank&#34;&gt;configuration changes&lt;/a&gt; and &lt;a href=&#34;https://github.com/networktocode/ntc-ansible&#34; target=&#34;_blank&#34;&gt;state verification&lt;/a&gt; is not new. However the approach I&amp;rsquo;m going to demonstrate in this post, using YANG and NETCONF, will have a few notable differences:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I will not use any templates and absolutely no XML/JSON for device config generation&lt;/li&gt;
&lt;li&gt;All changes will be pushed through a single, vendor and model-independent Ansible module&lt;/li&gt;
&lt;li&gt;State verification will be done with no pattern-matching or screen-scraping&lt;/li&gt;
&lt;li&gt;All configuration and operational state will be based on a couple of YAML files&lt;/li&gt;
&lt;li&gt;To demonstrate the model-agnostic behaviour I will use a mixture of vendor&amp;rsquo;s native, IETF and OpenConfig YANG models&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I hope this promise is exciting enough so without further ado, let&amp;rsquo;s get cracking.&lt;/p&gt;

&lt;h1 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h1&gt;

&lt;p&gt;The test environment will consist of a single instance of CSR1000v running IOS-XE version 16.4.1 and a single instance of vMX running JUNOS version 17.1R1.8. The VMs containing the two devices are deployed within a single hypervisor and connected with one interface to the management network and back-to-back with the second  pair of interfaces for BGP peering.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ansible-yang.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each device contains some basic initial configuration to allow it be reachable from the Ansible server.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;interface GigabitEthernet1
ip address 192.168.145.51 255.255.255.0
!
netconf-yang
netconf-yang cisco-odm polling enable
netconf-yang cisco-odm actions parse Interfaces
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vMX configuration is quite similar. Static MAC address is &lt;a href=&#34;http://noshut.ru/2015/09/how-to-run-juniper-vmx-in-unetlab/&#34; target=&#34;_blank&#34;&gt;required&lt;/a&gt; in order for &lt;code&gt;ge&lt;/code&gt; interfaces to work.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set system login user admin class super password admin123
set system services netconf
set interface fxp0 unit 0 family inet address 192.168.145.53/24
set interface ge-0/0/0 mac 00:0c:29:fc:1a:b7
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;ansible-playbook-configuration&#34;&gt;Ansible playbook configuration&lt;/h1&gt;

&lt;p&gt;My &lt;a href=&#34;https://github.com/networkop/yang/tree/master/ansible-101&#34; target=&#34;_blank&#34;&gt;Ansible-101&lt;/a&gt; repository contains two plays - one for configuration and one for state verification. The local inventory file contains details about the two devices along with the login credentials. All the work will be performed by a custom Ansible module stored in the &lt;code&gt;./library&lt;/code&gt; directory. This module is a wrapper for a &lt;code&gt;ydk_yaml&lt;/code&gt; module described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/03/13/yaml-yang/&#34;&gt;previous post&lt;/a&gt;. I had to heavily modify the original &lt;code&gt;ydk_yaml&lt;/code&gt; module to work around some Ansible limitations, like the lack of support for &lt;strong&gt;set&lt;/strong&gt; data structures.&lt;br /&gt;
This custom Ansible module also relies on a number of &lt;a href=&#34;https://networkop.co.uk/blog/2017/02/22/odl-ydk/&#34;&gt;YDK&lt;/a&gt; Python bindings to be pre-installed. Refer to my &lt;a href=&#34;https://github.com/networkop/yang/tree/master/yaml-101&#34; target=&#34;_blank&#34;&gt;YAML&lt;/a&gt;, &lt;a href=&#34;https://github.com/networkop/yang/tree/master/oper-101&#34; target=&#34;_blank&#34;&gt;Operational&lt;/a&gt; and &lt;a href=&#34;https://github.com/networkop/yang/tree/master/junos-101&#34; target=&#34;_blank&#34;&gt;JUNOS&lt;/a&gt; repositories for the instructions on how to install those modules.&lt;br /&gt;
The desired configuration and expected operational state are documented inside a couple of device-specific host variable files. For each device there is a configuration file &lt;code&gt;config.yaml&lt;/code&gt;, describing the desired configuration state. For IOS-XE there is an additional file &lt;code&gt;verify.yaml&lt;/code&gt;, describing the expected operational state using the IETF interface YANG model (I couldn&amp;rsquo;t find how to get the IETF or OpenConfig state models to work on Juniper).&lt;br /&gt;
All of these files follow the same structure:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Root container can be either &lt;code&gt;config&lt;/code&gt; or &lt;code&gt;verify&lt;/code&gt; and defines how the enclosed data is supposed to be used&lt;/li&gt;
&lt;li&gt;First nested container has to match the top-most container of a YANG model. For example it could be &lt;strong&gt;bgp-state&lt;/strong&gt; for &lt;a href=&#34;https://github.com/YangModels/yang/blob/master/vendor/cisco/xe/1641/cisco-bgp-state.yang&#34; target=&#34;_blank&#34;&gt;cisco-bgp-state.yang&lt;/a&gt; or &lt;strong&gt;openconfig-bgp&lt;/strong&gt; for &lt;a href=&#34;https://github.com/openconfig/public/blob/master/release/models/bgp/openconfig-bgp.yang&#34; target=&#34;_blank&#34;&gt;openconfig-bgp.yang&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;The remaining nested data has to follow the structure of the original YANG model as described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/03/13/yaml-yang/&#34;&gt;previous post&lt;/a&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here&amp;rsquo;s how IOS-XE will be configured, using IETF interfaca YANG models (to unshut the interface) and Cisco&amp;rsquo;s native YANG model for interface IP and BGP settings:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;---
config:
  interfaces:
    interface:
      - name: GigabitEthernet3
        enabled: true
  native:
    interface:
      gigabitethernet:
        - name: &#39;3&#39;
          description: P2P link
          ip:
            address:
              primary:
                address: 12.12.12.1
                mask: 255.255.255.0
      loopback:
        - name: 0
          description: ROUTER ID
          ip:
            address:
              primary:
                address: 1.1.1.1
                mask: 255.255.255.255
    router:
      bgp:
        - id: 65111
          bgp:
            router_id: 1.1.1.1
          neighbor:
            - id: 12.12.12.2
              remote_as: 65222
          redistribute:
            connected:
              empty: empty
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For JUNOS configuration, instead of the default humongous native model, I&amp;rsquo;ll use a set of much more light-weight OpenConfig YANG models to configure interfaces, BGP and redistribution policies:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;---
config:
  openconfig-interfaces:
    interface:
      - name: ge-0/0/0
        subinterfaces:
          subinterface:
            - index: 0
              ipv4:
                addresses:
                  address:
                    - ip: 12.12.12.2/24
                      config:
                        ip: 12.12.2.2
                        prefix_length: 24
      - name: lo0
        subinterfaces:
          subinterface:
            - index: 0
              ipv4:
                addresses:
                  address:
                    - ip: 2.2.2.2/32
                      config:
                        ip: 2.2.2.2
                        prefix_length: 32
  openconfig-policy:
    policy_definitions:
      policy_definition:
        - name: CONNECTED-&amp;gt;BGP
          statements:
            statement:
              - name: Loopback0
                conditions:
                  match_interface:
                    config:
                      interface: lo0
                      subinterface: 0
                actions:
                  config:
                    accept_route: empty
  openconfig-bgp:
    global_:
      config:
        as_: 65222
    neighbors:
      neighbor:
        - neighbor_address: 12.12.12.1
          config:
            peer_group: YANG
            peer_as: 65111
    peer_groups:
      peer_group:
        - peer_group_name: YANG
          config:
            peer_as: 65111
          apply_policy:
            config:
              export_policy:
                - CONNECTED-&amp;gt;BGP
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;configuration&#34;&gt;Configuration&lt;/h1&gt;

&lt;p&gt;Both devices now can be configured with just a single command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook config.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Behind the scenes, Ansible calls my custom &lt;code&gt;ydk_module&lt;/code&gt; and passes to it the full configuration state and device credentials. This module then constructs an empty YDK binding based on the name of a YANG model and &lt;a href=&#34;https://networkop.co.uk/blog/2017/03/13/yaml-yang/&#34;&gt;populates it recursively&lt;/a&gt; with the data from the &lt;code&gt;config&lt;/code&gt; container. Finally, it pushes the data to the device with the help of YDK NETCONF service provider.&lt;/p&gt;

&lt;h1 id=&#34;verification&#34;&gt;Verification&lt;/h1&gt;

&lt;p&gt;There&amp;rsquo;s one side to YANG which I have carefully avoided until now and it&amp;rsquo;s operational state models. These YANG models are built similarly to configuration models, but with a different goal - to extract the running state from a device. The reason why I&amp;rsquo;ve avoided them is that, unlike the configuration models, the current support for state models is limited and somewhat brittle.&lt;br /&gt;
For example, JUNOS natively only supports state models as RPCs, where each RPC represents a certain &lt;code&gt;show&lt;/code&gt; command which, I assume, when passed to the devices gets evaluated, its output parsed and result returned back to the client. With IOX-XE things are a little better with a few of the operational models available in the current 16.4 release. You can check out my &lt;a href=&#34;https://github.com/networkop/yang/tree/master/oper-101&#34; target=&#34;_blank&#34;&gt;Github repo&lt;/a&gt; for some examples of how to check the interface and BGP neighbor state between the two IOS-XE devices. However, most of the models are still missing (I&amp;rsquo;m not counting the MIB-mapped YANG models) in the current release. The next few releases, though, are promised to come with an improved state model support, including some OpenConfig models, which is going to be super cool.&lt;br /&gt;
So in this post, since I couldn&amp;rsquo;t get JUNOS OpenConfig models report any state and my IOS-XE BGP state model wouldn&amp;rsquo;t return any output unless the BGP peering was with another Cisco device or in the &lt;strong&gt;Idle&lt;/strong&gt; state, I&amp;rsquo;m going to have to resort to simply checking the state of physical interfaces. This is how a sample operational state file would look like (question marks are YAML&amp;rsquo;s special notation for sets which is how I decided to encode Enum data type):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
verify:
  interfaces-state:
    interface:
      - name: GigabitEthernet3
        oper_status:
          ? up
      - name: Loopback0
        oper_status:
          ? up
      - name: GigabitEthernet2
        oper_status:
          ? down
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once again, all expected state can be verified with a single command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook verify.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the state defined in that YAML file matches the data returned by the IOS-XE device, the playbook completes successfully. You can check that it works by shutting down one of the &lt;code&gt;GigabitEthernet3&lt;/code&gt; or &lt;code&gt;Loopback0&lt;/code&gt; interfaces and observing how Ansible module returns an error.&lt;/p&gt;

&lt;h1 id=&#34;outro&#34;&gt;Outro&lt;/h1&gt;

&lt;p&gt;Now that I&amp;rsquo;ve come to the end of my YANG series of posts I feel like I need to provide some concise and critical summary of everything I&amp;rsquo;ve been through. However, if there&amp;rsquo;s one thing I&amp;rsquo;ve learned in the last couple of months about YANG, it&amp;rsquo;s that things are changing very rapidly. Both Cisco and Juniper are working hard introducing new models and improving support for the existing ones. So one thing to keep in mind, if you&amp;rsquo;re reading this post a few months after it was published (April 2017), is that some or most of the above limitations may not exist and it&amp;rsquo;s always worth checking what the latest software release has to offer.&lt;/p&gt;

&lt;p&gt;Finally, I wanted to say that I&amp;rsquo;m a strong believer that YANG models are the way forward for network device configuration and state verification, despite the timid scepticism of the networking industry. I think that there are two things that may improve the industry&amp;rsquo;s perception of YANG and help increase its adoption:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Support from networking vendors - we&amp;rsquo;ve already seen Cisco changing by introducing YANG support on IOS-XE instead of producing another dubious One-PK clone. So big thanks to them and I hope that other vendors will follow suit.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tools - this part, IMHO, is the most crucial. In order for people to start using YANG models we have to have the right tools that would be versatile enough to allow network engineers to be limited only by their imagination and at the same time be as robust as the CLI. So I wanted to give a big shout out to all the people contributing to open-source projects like &lt;strong&gt;pyang&lt;/strong&gt;, &lt;strong&gt;YDK&lt;/strong&gt; and many others that I have missed or don&amp;rsquo;t know about. You&amp;rsquo;re doing a great job guys, don&amp;rsquo;s stop.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>YANG &amp; Ansible</title>
      <link>https://networkop.co.uk/tags/ansible-yang/</link>
      <pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/tags/ansible-yang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Configuring Cisco IOS XE With YANG-based YAML Files</title>
      <link>https://networkop.co.uk/blog/2017/03/13/yaml-yang/</link>
      <pubDate>Mon, 13 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/03/13/yaml-yang/</guid>
      <description>

&lt;p&gt;XML, just like many more structured data formats, was not designed to be human-friendly. That&amp;rsquo;s why many network engineers lose interest in YANG as soon as the conversation gets to the XML part. JSON is a much more human-readable alternative, however very few devices support RESTCONF, and the ones that do may have &lt;a href=&#34;https://github.com/CiscoDevNet/openconfig-getting-started/issues/4&#34; target=&#34;_blank&#34;&gt;buggy implementations&lt;/a&gt;. At the same time, a lot of network engineers have happily embraced Ansible, which extensively uses YAML. That&amp;rsquo;s why I&amp;rsquo;ve decided to write a Python module that would program network devices using YANG and NETCONF according to configuration data described in a YAML format.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&#34;blog/2017/02/22/odl-ydk/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; I have introduced a new open-source tool called YDK, designed to create API bindings for YANG models and interact with network devices using NETCONF or RESTCONF protocols. I have also mentioned that I would still prefer to use &lt;a href=&#34;https://github.com/robshakir/pyangbind&#34; target=&#34;_blank&#34;&gt;pyangbind&lt;/a&gt; along with other open-source tools to achieve the same functionality. Now, two weeks later, I must admin I have been converted. Initially, I was planning to write a simple REST API client to interact with RESTCONF interface of IOS XE, create an API binding with &lt;strong&gt;pyangbind&lt;/strong&gt;, use it to produce the JSON output, convert it to XML and send it to the device, similar to what I&amp;rsquo;ve described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;netconf&lt;/a&gt; and &lt;a href=&#34;blog/2017/02/15/restconf-yang/&#34; target=&#34;_blank&#34;&gt;restconf&lt;/a&gt; posts. However, I&amp;rsquo;ve realised that YDK can already do all what I need with just a few function calls. All what I&amp;rsquo;ve got left to do is create a wrapper module to consume the YAML data and use it to automatically populate YDK bindings.&lt;/p&gt;

&lt;p&gt;This post will be mostly about the internal structure of this wrapper module I call &lt;code&gt;ydk_yaml.py&lt;/code&gt;, which will serve as a base library for a YANG Ansible module, which I will describe in my next post. This post will be very programming-oriented, I&amp;rsquo;ll start with a quick overview of some of the programming concepts being used by the module and then move on to the details of module implementation. Those who are not interested in technical details can jump straight to the &lt;strong&gt;examples&lt;/strong&gt; sections at the end of this post for a quick demonstration of how it works.&lt;/p&gt;

&lt;h1 id=&#34;recursion&#34;&gt;Recursion&lt;/h1&gt;

&lt;p&gt;One of the main tasks of &lt;code&gt;ydk_yaml.py&lt;/code&gt; module is to be able parse a YAML data structure. This data structure, when loaded into Python, is stored as a collection of Python objects like dictionaries, lists and primitive data types like strings, integers and booleans. One key property of YAML data structures is that they can be represented as trees and parsing trees is a very well-known programming problem.&lt;/p&gt;

&lt;p&gt;After having completed &lt;a href=&#34;https://www.coursera.org/learn/programming-languages&#34; target=&#34;_blank&#34;&gt;this programming course&lt;/a&gt; I fell in love with functional programming and recursions. Every problem I see, I try to solve with a recursive function. Recursions are very interesting in a way that they are very difficult to understand but relatively easy to write. Any recursive function will consist of a number of &lt;code&gt;if/then/else&lt;/code&gt; conditional statements. The first one (or few) &lt;code&gt;if&lt;/code&gt; statements are called the base of a recursion - this is where recursion stops and the value is returned to the outer function. The remaining few &lt;code&gt;if&lt;/code&gt; statements will implement the recursion by calling the same function with a &lt;strong&gt;reduced input&lt;/strong&gt;. You can find a much better explanation of recursive functions &lt;a href=&#34;http://composingprograms.com/pages/17-recursive-functions.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. For now, let&amp;rsquo;s consider the problem of parsing the following tree-like data structure:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{ &#39;parent&#39;: {
    &#39;child_1&#39;: {
      &#39;leaf_1&#39;: &#39;value_1&#39;
    },
    &#39;child_1&#39;: &#39;value_2&#39;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Recursive function to parse this data structure written in a pseudo-language will look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def recursion(input_key, input_value):
  if input_value is String:
    return process(input_value)
  elif input_value is Dictonary:
    for key, value in input_value.keys_and_values():
      return recursion(key, value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The beauty of recursive functions is that they are capable parsing data structures of arbitrary complexity. That means if we had 1000 randomly nested child elements in the parent data structure, they all could have been parsed by the same 6-line function.&lt;/p&gt;

&lt;h1 id=&#34;introspection&#34;&gt;Introspection&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://zetcode.com/lang/python/introspection/&#34; target=&#34;_blank&#34;&gt;Introspection&lt;/a&gt; refers to the ability of Python to examine objects at runtime. It can be useful when dealing with object of arbitrary structure, e.g. a YAML document. Introspection is used whenever there is a need for a function to behave differently based on the runtime data. In the above pseudo-language example, the two conditional statements are the examples of introspection. Whenever we need to determine the type of an object in Python we can either use a built-in function &lt;code&gt;type(obj)&lt;/code&gt; which returns the type of an object or &lt;code&gt;isinstance(obj, type)&lt;/code&gt; which checks if the &lt;strong&gt;object&lt;/strong&gt; is an &lt;em&gt;instance&lt;/em&gt; or a &lt;em&gt;descendant&lt;/em&gt; of a particular &lt;strong&gt;type&lt;/strong&gt;. This is how we can re-write the above two conditional statements using real Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if isinstance(input_value, str):
  print(&#39;input value is a string&#39;)
elif isinstance(input_value, dict):
  print(&#39;intput value is a dictionary&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;metaprogramming&#34;&gt;Metaprogramming&lt;/h1&gt;

&lt;p&gt;Another programming concept used in my Python module is &lt;a href=&#34;http://chase-seibert.github.io/blog/2013/04/12/getattr-setattr.html&#34; target=&#34;_blank&#34;&gt;metaprogramming&lt;/a&gt;. Metaprogramming, in general, refers to an ability of programs to write themselves. This is what compilers normally do when they read the program written in a higher-level language and translate it to a lower-level language, like assembler. What I&amp;rsquo;ve used in my module is the simplest version of metaprogramming - dynamic getting and setting of object attributes. For example, this is how we would configure BGP using YDK Python binding, as described in my &lt;a href=&#34;blog/2017/02/22/odl-ydk/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bgp.id = 100
n = bgp.Neighbor()
n.id = &#39;2.2.2.2&#39;
n.remote_as = 65100
bgp.neighbor.append(n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The same code could be re-written using the &lt;code&gt;getattr&lt;/code&gt; and &lt;code&gt;setattr&lt;/code&gt; method calls:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;setattr(bgp, &#39;id&#39;, 100)
n = getattr(bgp, &#39;Neighbor&#39;)()
setattr(n, &#39;id&#39;, &#39;2.2.2.2&#39;)
setattr(n, &#39;remote_as&#39;, 65100)
getattr(bgp, &#39;neighbor&#39;).append(n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is also very useful when working with arbitrary data structures and objects. In my case the goal was to write a module that would be completely independent of the structure of a particular YANG model, which means that I can &lt;strong&gt;not know&lt;/strong&gt; the structure of the Python binding generated by YDK. However, I can &amp;ldquo;guess&amp;rdquo; the name of the attributes if I assume that my YAML document is structured &lt;strong&gt;exactly&lt;/strong&gt; like the YANG model. This simple assumption allows me to implement YAML mapping for &lt;strong&gt;all&lt;/strong&gt; possible YANG models with just a single function.&lt;/p&gt;

&lt;h1 id=&#34;yang-mapping-to-yaml&#34;&gt;YANG mapping to YAML&lt;/h1&gt;

&lt;p&gt;As I&amp;rsquo;ve mentioned in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;previous post&lt;/a&gt;, YANG is simply a way to define the structure of an XML document. At the same time, it is known that YANG-based XML can be mapped to JSON as described in &lt;a href=&#34;https://tools.ietf.org/html/draft-ietf-netmod-yang-json-10&#34; target=&#34;_blank&#34;&gt;this RFC&lt;/a&gt;. Since YAML is a superset of JSON, it&amp;rsquo;s easy to come up with a similar XML-to-YAML mapping convention. The following table contains the mapping between some of the most common YAML and YANG data structures and types:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;YANG data&lt;/th&gt;
&lt;th&gt;YAML representation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;container&lt;/td&gt;
&lt;td&gt;dictionary&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;container name&lt;/td&gt;
&lt;td&gt;dictionary key&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;leaf name&lt;/td&gt;
&lt;td&gt;dictionary key&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;leaf&lt;/td&gt;
&lt;td&gt;dictionary value&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;string, bool, integer&lt;/td&gt;
&lt;td&gt;string, bool, integer&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;empty&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Using this table, it&amp;rsquo;s easy to map the YANG data model to a YAML document. Let me demonstrate it on IOS XE&amp;rsquo;s native OSPF data model. First, I&amp;rsquo;ve generated a tree representation of an OSPF data model using &lt;strong&gt;pyang&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pyang -f tree --tree-path &amp;quot;/native/router/ospf&amp;quot; ~/ydk-gen/gen-api/.cache/models/cisco_ios_xe@0.1.0/ned.yang -o ospf.tree
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, I&amp;rsquo;ve trimmed it down to only contain the options that I would like to set and created a YAML document based on the model&amp;rsquo;s tree structure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/yang-yaml.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With the right knowledge of &lt;a href=&#34;blog/2017/02/15/restconf-yang/&#34; target=&#34;_blank&#34;&gt;YANG model&amp;rsquo;s structure&lt;/a&gt;, it&amp;rsquo;s fairly easy to generate similar YAML configuration files for other configuration objects, like &lt;a href=&#34;https://github.com/networkop/yang/blob/master/yaml-101/interface.yaml&#34; target=&#34;_blank&#34;&gt;interface&lt;/a&gt; and &lt;a href=&#34;https://github.com/networkop/yang/blob/master/yaml-101/bgp.yaml&#34; target=&#34;_blank&#34;&gt;BGP&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;yang-instantiating-function&#34;&gt;YANG instantiating function&lt;/h1&gt;

&lt;p&gt;At the heart of the &lt;code&gt;ydk_yaml&lt;/code&gt; module is a single recursive function that traverses the input YAML data structure and uses it to instantiate the YDK-generated Python binding. Here is a simple, abridged version of the function that demonstrates the main logic.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def instantiate(binding, model_key, model_value):
    if any(isinstance(model_value, x) for x in [str, bool, int]):
        setattr(binding, model_key, model_value)
    elif isinstance(model_value, list):
        for el in model_value:
            getattr(binding, model_key).append(instantiate(binding, model_key, el))
    elif isinstance(model_value, dict):
        container_instance = getattr(binding, model_key)()
        for k, v in model_value.iteritems():
            instantiate(container_instance, k, v)
        setattr(binding, model_key, container_instance)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Most of it should already make sense based on what I&amp;rsquo;ve covered above. The first conditional statement is the base of the recursion and performs the action of setting the value of a YANG Leaf element. The second conditional statement takes care of a YANG List by traversing all its elements, instantiating them recursively, and appends the result to a YDK binding. The last &lt;code&gt;elif&lt;/code&gt; statement creates a class instance for a YANG container, recursively populates its values and saves the final result inside a YDK binding.&lt;/p&gt;

&lt;p&gt;The full version of this function covers a few extra corner cases and can be found &lt;a href=&#34;https://github.com/networkop/yang/blob/master/yaml-101/ydk_yaml.py&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;the-ydk-module-wrapper&#34;&gt;The YDK module wrapper&lt;/h1&gt;

&lt;p&gt;The final step is to write a wrapper class that would consume the YDK model binding along with the YAML data, and both instantiate and push the configuration down to the network device.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class YdkModel:

    def __init__(self, model, data):
        self.model = model
        self.data = data
        from ydk.models.cisco_ios_xe.ned import Native
        self.binding = Native()
        for k,v in self.data.iteritems():
            instantiate(self.binding, k, v)

    def action(self, crud_action, device):
        from ydk.services import CRUDService
        from ydk.providers import NetconfServiceProvider
        provider = NetconfServiceProvider(address=device[&#39;hostname&#39;],
                                          port=device[&#39;port&#39;],
                                          username=device[&#39;username&#39;],
                                          password=device[&#39;password&#39;],
                                          protocol=&#39;ssh&#39;)
        crud = CRUDService()
        crud_instance = getattr(crud, crud_action)
        crud_instance(provider, self.binding)
        provider.close()
        return
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The structure of this class is pretty simple. The constructor instantiates a YDK native data model and calls the recursive instantiation function to populate the binding. The &lt;strong&gt;action&lt;/strong&gt; method implements standard CRUD actions using the YDK&amp;rsquo;s NETCONF provider. The full version of this Python module can be found &lt;a href=&#34;https://github.com/networkop/yang/blob/master/yaml-101/ydk_yaml.py&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;configuration-examples&#34;&gt;Configuration examples&lt;/h1&gt;

&lt;p&gt;In my &lt;a href=&#34;https://github.com/networkop/yang/tree/master/yaml-101&#34; target=&#34;_blank&#34;&gt;Github repo&lt;/a&gt;, I&amp;rsquo;ve included a few examples of how to configure Interface, OSPF and BGP settings of IOS XE device. A helper Python script &lt;code&gt;1_send_yaml.py&lt;/code&gt; accepts the YANG model name and the name of the YAML configuration file as the input. It then instantiates the &lt;code&gt;YdkModel&lt;/code&gt; class and calls the &lt;code&gt;create&lt;/code&gt; action to push the configuration to the device. Let&amp;rsquo;s assume that we have the following YAML configuration data saved in a &lt;code&gt;bgp.yaml&lt;/code&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+++
router:
  bgp:
    - id: 100
      bgp:
        router_id: 1.1.1.1
        fast_external_fallover: null
        update_delay: 15
      neighbor:
        - id: 2.2.2.2
          remote_as: 200
        - id: 3.3.3.3
          remote_as: 300
      redistribute:
        connected: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To push this BGP configuration to the device all what I need to do is run the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./1_send_yaml.py bgp bgp.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The resulting configuration on IOS XE device would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;router bgp 100
 bgp router-id 1.1.1.1
 bgp log-neighbor-changes
 bgp update-delay 15
 redistribute connected
 neighbor 2.2.2.2 remote-as 200
 neighbor 3.3.3.3 remote-as 300
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To see more example, follow &lt;a href=&#34;https://github.com/networkop/yang/tree/master/yaml-101&#34; target=&#34;_blank&#34;&gt;this link&lt;/a&gt; to my Github repo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Configuring Cisco IOS XE With YDK and OpenDaylight</title>
      <link>https://networkop.co.uk/blog/2017/02/22/odl-ydk/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/02/22/odl-ydk/</guid>
      <description>

&lt;p&gt;In the previous posts about &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;NETCONF&lt;/a&gt; and &lt;a href=&#34;https://networkop.co.uk/blog/2017/02/15/restconf-yang/&#34;&gt;RESTCONF&lt;/a&gt; I&amp;rsquo;ve demonstrated how to interact with Cisco IOS XE device directly from the Linux shell of my development VM. This approach works fine in some cases, e.g. whenever I setup a new DC fabric, I would make calls directly to the devices I&amp;rsquo;m configuring. However, it becomes impractical in the Ops world where change is constant and involves a large number of devices. This is where centralised service orchestrators come to the fore. The prime examples of such platforms are Network Services Orchestrator from Tail-f/Cisco and open-source project OpenDaylight. In this post we&amp;rsquo;ll concentrate on ODL and how to make it work with Cisco IOS XE. Additionally, I&amp;rsquo;ll show how to use an open-source tool &lt;a href=&#34;https://developer.cisco.com/site/ydk/&#34; target=&#34;_blank&#34;&gt;YDK&lt;/a&gt; to generate Python bindings for native YANG models and how it compares with &lt;strong&gt;pyangbind&lt;/strong&gt;.&lt;/p&gt;

&lt;h1 id=&#34;opendaylight-primer&#34;&gt;OpenDaylight primer&lt;/h1&gt;

&lt;p&gt;OpenDaylight is a swiss army knife of SDN controllers. At the moment it is comprised of dozens of projects implementing all possible sorts of SDN functionality starting from Openflow controller all the way up to L3VPN orchestrator. ODL speaks most of the modern Southbound protocols like Openflow, SNMP, NETCONF and BGP. The brain of the controller is in the Service Abstraction Layer, a framework to model all network-related characteristics and properties. All logic inside SAL is modelled in YANG which is why I called it the godfather of YANG models. Towards the end users ODL exposes Java function calls for applications running on the same host and REST API for application running remotely.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/odl-sal.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;OpenDaylight has several commercial offerings from companies involved in its development. Most notable ones are from Brocade and Cisco. Here I will allow myself a bit of a rant, feel free to skip it to go straight to the technical stuff.&lt;/p&gt;

&lt;p&gt;One thing I find interesting is that Cisco are being so secretive about their Open SDN Controller, perhaps due to the earlier market pressure to come up with a single SDN story, but still have a very large number of contributors to this open-source project. It could be the case of having an egg in each basket, but the number of Cisco&amp;rsquo;s employees involved in ODL development is substantial. I wonder if, now that the use cases for ACI and ODL have finally formed and ACI still not showing the uptake originally expected, Cisco will change their strategy and start promoting ODL more aggressively, or at least stop hiding it deep in the bowels of &lt;a href=&#34;cisco.com&#34; target=&#34;_blank&#34;&gt;cisco.com&lt;/a&gt;. Or, perhaps, it will always stay in the shade of Tail-f&amp;rsquo;s NSC and Insieme&amp;rsquo;s ACI and will be used only for customer with unique requirements, e.g. to have both OpenStack and network devices managed through the same controller.&lt;/p&gt;

&lt;h1 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;ll use the same environment we&amp;rsquo;ve setup in the &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;previous posts&lt;/a&gt;, consisting of a CSR1K and a Linux VM connected to the same network inside my hypervisor. IOS XE device needs to have &lt;code&gt;netconf-yang&lt;/code&gt; configured in order to enable the northbound NETCONF interface.&lt;/p&gt;

&lt;p&gt;On the same Linux VM, I&amp;rsquo;ve downloaded and launched the latest version of ODL (Boron-SR2), and enabled NETCONF and RESTCONF plugins.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;unzip distribution-karaf-0.5.2-Boron-SR2.zip
mv distribution-karaf-0.5.2-Boron-SR2 odl-0.5.2
cd odl-0.5.2/
./bin/karaf
opendaylight-user@root&amp;gt;feature:install odl-netconf-connector-all
opendaylight-user@root&amp;gt;feature:install odl-restconf-all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ll use NETCONF to connect to Cisco IOS XE device and RESTCONF to interact with ODL from a Linux shell.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/odl-ydk.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It might be useful to turn on logging in karaf console to catch any errors we might encounter later:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;opendaylight-user@root&amp;gt;log:tail
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;connecting-ios-xe-to-odl&#34;&gt;Connecting IOS XE to ODL&lt;/h1&gt;

&lt;p&gt;According to ODL &lt;a href=&#34;http://docs.opendaylight.org/en/stable-boron/user-guide/netconf-user-guide.html&#34; target=&#34;_blank&#34;&gt;NETCONF&lt;/a&gt; user guide, in order to connect a new device to the controller, we need to create an XML document which will include the IP, port and user credentials of the IOS XE device. Here&amp;rsquo;s the excerpt from the &lt;a href=&#34;https://github.com/networkop/yang/blob/master/odl-101/new_device.xml.1&#34; target=&#34;_blank&#34;&gt;full XML document&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;module xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:config&amp;quot;&amp;gt;
  &amp;lt;type xmlns:prefix=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;prefix:sal-netconf-connector&amp;lt;/type&amp;gt;
  &amp;lt;name&amp;gt;CSR1K&amp;lt;/name&amp;gt;
  &amp;lt;address xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;192.168.145.51&amp;lt;/address&amp;gt;
  &amp;lt;port xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;830&amp;lt;/port&amp;gt;
  &amp;lt;username xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;admin&amp;lt;/username&amp;gt;
  &amp;lt;password xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;admin&amp;lt;/password&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming this XML is saved in a file called &lt;a href=&#34;https://github.com/networkop/yang/blob/master/odl-101/new_device.xml.1&#34; target=&#34;_blank&#34;&gt;new_device.xml.1&lt;/a&gt;, we can use &lt;code&gt;curl&lt;/code&gt; to send it to ODL&amp;rsquo;s netconf-connector plugin:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k -u admin:admin -H &amp;quot;Content-Type: application/xml&amp;quot; -X POST \
 http://localhost:8181/restconf/config/network-topology:network-topology\
 /topology/topology-netconf/node/controller-config/yang-ext:mount/config:modules\
  -d @new_device.xml.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the controller gets this information it will try to connect to the device via NETCONF and do the following three things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Discover device capabilities advertised in the Hello message&lt;/li&gt;
&lt;li&gt;Download all YANG models advertised by the device into the &lt;code&gt;./cache/schema&lt;/code&gt; directory&lt;/li&gt;
&lt;li&gt;Go through all of the imports in each model and verify that they can be satisfied&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After ODL downloads all of the 260 available models (can take up to 20 minutes) we will see the following errors in the karaf console:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Netconf device does not provide all yang models reported in hello message capabilities
Unable to build schema context, unsatisfied imports
Initialization in sal failed, disconnecting from device
No more sources for schema context
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Due to inconsistencies between the advertised and the available models, ODL fails to build the full device YANG schema context, which ultimately results in inability to connect the device to the controller. However, we won&amp;rsquo;t need all of the 260 models advertised by the device. In fact, most of the configuration can be done through a single Cisco native YANG model, &lt;code&gt;ned&lt;/code&gt;. With ODL it is possible to override the default capabilities advertised in the Hello message and &amp;ldquo;pin&amp;rdquo; only the ones that are going to be used. Assuming that ODL has downloaded most of the models at the previous step, we can simply tell it use the selected few with the following additions to the &lt;a href=&#34;https://github.com/networkop/yang/blob/master/odl-101/new_device.xml.2&#34; target=&#34;_blank&#34;&gt;XML document&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;yang-module-capabilities xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;
    &amp;lt;override&amp;gt;true&amp;lt;/override&amp;gt;
    &amp;lt;capability xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;
      urn:ietf:params:xml:ns:yang:ietf-inet-types?module=ietf-inet-types&amp;amp;amp;revision=2013-07-15
    &amp;lt;/capability&amp;gt;
    &amp;lt;capability xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;
      http://cisco.com/ns/yang/ned/ios?module=ned&amp;amp;amp;revision=2016-10-24
    &amp;lt;/capability&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming the updated XML is saved in &lt;a href=&#34;https://github.com/networkop/yang/blob/master/odl-101/new_device.xml.2&#34; target=&#34;_blank&#34;&gt;new_device.xml.2&lt;/a&gt; file, the following command will update the current configuration of &lt;strong&gt;CSR1K&lt;/strong&gt; device:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k -u admin:admin -H &amp;quot;Content-Type: application/xml&amp;quot; -X PUT \
http://localhost:8181/restconf/config/network-topology:network-topology\
/topology/topology-netconf/node/controller-config\
/yang-ext:mount/config:modules/module\
/odl-sal-netconf-connector-cfg:sal-netconf-connector\
/CSR1K -d @new_device.xml.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then verify that the device has been successfully mounted to the controller:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k -u admin:admin http://localhost:8181/restconf/operational\
/network-topology:network-topology/ | python -m json.tool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output should look similar to the following with the connection-status set to &lt;code&gt;connected&lt;/code&gt; and no detected &lt;code&gt;unavailable-capabilities&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;netconf-node-topology:connection-status&amp;quot;: &amp;quot;connected&amp;quot;,
&amp;quot;netconf-node-topology:host&amp;quot;: &amp;quot;192.168.145.51&amp;quot;,
&amp;quot;netconf-node-topology:port&amp;quot;: 830,
&amp;quot;netconf-node-topology:unavailable-capabilities&amp;quot;: {},
&amp;quot;node-id&amp;quot;: &amp;quot;CSR1K&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point we should be able to interact with IOS XE&amp;rsquo;s native YANG model through ODL&amp;rsquo;s RESTCONF interface using the following URL&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; http://localhost:8181/restconf/config/network-topology:network-topology\
 /topology/topology-netconf/node/CSR1K/yang-ext:mount/ned:native
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only thing that&amp;rsquo;s missing is the actual configuration data. To generate it, I&amp;rsquo;ll use a new open-source tool called YDK.&lt;/p&gt;

&lt;h1 id=&#34;ydk-primer&#34;&gt;YDK primer&lt;/h1&gt;

&lt;p&gt;Yang Development Kit is a suite of tools to work with NETCONF/RESTCONF interfaces of a network device. The way I see it, YDK accomplishes two things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Generates API bindings for programming languages (Python and C++) from YANG models&lt;/li&gt;
&lt;li&gt;Creates an abstraction layer to interact with southbound protocols (NETCONF or RESTCONF) in a uniform way&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There&amp;rsquo;s a lot of overlap between the tools that we&amp;rsquo;ve used &lt;a href=&#34;https://networkop.co.uk/blog/2017/02/15/restconf-yang/&#34;&gt;before&lt;/a&gt; and YDK. Effectively YDK combines in itself the functions of a NETCONF client, a REST client, pyangbind and pyang(the latter is used internally for model verification). Since one of the main functions of YDK is API generation I thought it&amp;rsquo;d be interesting to know how it compares to Rob Shakir&amp;rsquo;s &lt;strong&gt;pyangbind&lt;/strong&gt; plugin. The following information is what I&amp;rsquo;ve managed to find on the Internet and from the comment of Santiago Alvarez below:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Pyangbind&lt;/th&gt;
&lt;th&gt;YDK&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;PL support&lt;/td&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td&gt;Python, C++ with Ruby and Go in the pipeline&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Serialization&lt;/td&gt;
&lt;td&gt;JSON, XML&lt;/td&gt;
&lt;td&gt;only XML &lt;a href=&#34;https://github.com/CiscoDevNet/ydk-gen/blob/master/sdk/python/core/ydk/providers/codec_provider.py#L53&#34; target=&#34;_blank&#34;&gt;at this stage&lt;/a&gt; with JSON coming up in a few weeks&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Southbound interfaces&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;NETCONF, RESTCONF with ODL coming up in a few weeks&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Support&lt;/td&gt;
&lt;td&gt;Cisco&amp;rsquo;s devnet team&lt;/td&gt;
&lt;td&gt;Rob Shakir&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So it looks like YDK is a very promising alternative to &lt;strong&gt;pyangbind&lt;/strong&gt;, however I, personally, would still prefer to use &lt;strong&gt;pyangbind&lt;/strong&gt; due to familiarity, simplicity and the fact that I don&amp;rsquo;t need the above extra features offered by YDK right now. However, given that YDK has been able to achieve so much in just under one year of its existence, I don&amp;rsquo;t discount the possibility that I may switch to YDK as it becomes more mature and feature-rich.&lt;/p&gt;

&lt;h1 id=&#34;python-binding-generation-with-ydk-gen&#34;&gt;Python binding generation with YDK-GEN&lt;/h1&gt;

&lt;p&gt;One of the first things we need to do is install YDK-GEN, the tools responsible for API bindings generation, and it&amp;rsquo;s core Python packages on the local machine. The following few commands are my version of the official &lt;a href=&#34;https://github.com/CiscoDevNet/ydk-gen&#34; target=&#34;_blank&#34;&gt;installation procedure&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/CiscoDevNet/ydk-gen.git ~/ydk-gen
pip install -r ~/ydk-gen/requirements.txt
export YDKGEN_HOME=~/ydk-gen/
~/ydk-gen/generate.py --python --core
pip install ~/ydk-gen/gen-api/python/ydk/dist/ydk*.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;YDK-GEN generates Python bindings based on the so-called &lt;strong&gt;bundle profile&lt;/strong&gt;. This is a simple JSON document which lists all YANG models to include in the output package. In our case we&amp;rsquo;d need to include a &lt;code&gt;ned&lt;/code&gt; model along with all its imports. The sample below shows only the model specification. Refer to my &lt;a href=&#34;https://github.com/networkop/yang/blob/master/odl-101/cisco-ios-xe_0_1_0.json&#34; target=&#34;_blank&#34;&gt;Github repo&lt;/a&gt; for a complete bundle profile for Cisco IOS XE native YANG model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;models&amp;quot;:{&amp;quot;git&amp;quot;:[{&amp;quot;url&amp;quot;:&amp;quot;https://github.com/YangModels/yang.git&amp;quot;,
  &amp;quot;commits&amp;quot;:[{&amp;quot;commitid&amp;quot;:&amp;quot;6f4a025431103f8cbbf3405ce01bdc61d0811b1d&amp;quot;,
    &amp;quot;file&amp;quot;:[&amp;quot;vendor/cisco/xe/1641/ned.yang&amp;quot;,
      &amp;quot;vendor/cisco/xe/1641/tailf-common.yang&amp;quot;,
      &amp;quot;vendor/cisco/xe/1641/tailf-meta-extensions.yang&amp;quot;,
      &amp;quot;vendor/cisco/xe/1641/tailf-cli-extensions.yang&amp;quot;,
      &amp;quot;standard/ietf/RFC/ietf-inet-types.yang&amp;quot;,
      &amp;quot;standard/ietf/RFC/ietf-yang-types.yang&amp;quot;]
      }]}]}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming that the IOS XE bundle profile is saved in a file called &lt;a href=&#34;https://github.com/networkop/yang/blob/master/odl-101/cisco-ios-xe_0_1_0.json&#34; target=&#34;_blank&#34;&gt;cisco-ios-xe_0_1_0.json&lt;/a&gt;, we can use YDK to generate and install the Python binding package:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~/ydk-gen/generate.py --python --bundle cisco-ios-xe_0_1_0.json -v
pip install ~/ydk-gen/gen-api/python/cisco_ios_xe-bundle/dist/ydk*.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;configuring-bgp-with-ydk&#34;&gt;Configuring BGP with YDK&lt;/h1&gt;

&lt;p&gt;Now we can start configuring BGP using our newly generated Python package. First, we need to create an instance of BGP configuration data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ydk.models.cisco_ios_xe.ned import Native
bgp = Native().router.Bgp()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The configuration will follow the pattern defined in the original model, which is why it&amp;rsquo;s important to understand &lt;a href=&#34;https://networkop.co.uk/blog/2017/02/15/restconf-yang/&#34;&gt;the internal structure&lt;/a&gt; of a YANG model. YANG leafs are represented as simple instance attributes. All YANG containers need to be explicitly instantiated, just like the &lt;code&gt;Native&lt;/code&gt; and &lt;code&gt;Bgp&lt;/code&gt; classes in the example above. Presence containers (&lt;code&gt;router&lt;/code&gt; in the above example) will be instantiated at the same time as its parent container, inside the &lt;code&gt;__init__&lt;/code&gt; function of the &lt;code&gt;Native&lt;/code&gt; class. Don&amp;rsquo;t worry if this doesn&amp;rsquo;t make sense, use &lt;strong&gt;iPython&lt;/strong&gt; or any IDE with autocompletion and after a few tries, you&amp;rsquo;ll get the hang of it.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how we can set the local BGP AS number and add a new BGP peer to the neighbor list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bgp.id = 100
new_neighbor = bgp.Neighbor()
new_neighbor.id = &#39;2.2.2.2&#39;
new_neighbor.remote_as = 65100
bgp.neighbor.append(new_neighbor)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point of time all data is stored inside the instance of a &lt;code&gt;Bgp&lt;/code&gt; class. In order to get an XML representation of it, we need to use YDK&amp;rsquo;s XML provider and encoding service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ydk.providers import CodecServiceProvider
from ydk.services import CodecService
provider = CodecServiceProvider(type=&amp;quot;xml&amp;quot;)
codec = CodecService()
xml_string = codec.encode(provider, bgp)
print xml_string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All what we&amp;rsquo;ve got left now is to send the data to ODL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests
url = (&amp;quot;http://localhost:8181/restconf&amp;quot;
       &amp;quot;/config/network-topology:network-topology&amp;quot;
       &amp;quot;/topology/topology-netconf/node&amp;quot;
       &amp;quot;/CSR1K/yang-ext:mount/ned:native&amp;quot;
       &amp;quot;/router&amp;quot;)
headers = {&#39;Content-Type&#39;: &#39;application/xml&#39;}
result = requests.post(url, auth=(&#39;admin&#39;, &#39;admin&#39;), headers=headers, data=xml_string)
print result.status_code
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The controller should have returned the status code &lt;code&gt;204 No Content&lt;/code&gt;, meaning that configuration has been changed successfully.&lt;/p&gt;

&lt;h1 id=&#34;verification&#34;&gt;Verification&lt;/h1&gt;

&lt;p&gt;Back at the IOS XE CLI we can see the new BGP configuration that has been pushed down from the controller.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TEST#sh run | i router
router bgp 100
 bgp log-neighbor-changes
 neighbor 2.2.2.2 remote-as 65100
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;more-examples&#34;&gt;More examples&lt;/h1&gt;

&lt;p&gt;You can find a shorter version of the above procedure in my &lt;a href=&#34;https://github.com/networkop/yang/tree/master/odl-101&#34; target=&#34;_blank&#34;&gt;ODL 101 repo&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to YANG Programming and RESTCONF on Cisco IOS XE</title>
      <link>https://networkop.co.uk/blog/2017/02/15/restconf-yang/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/02/15/restconf-yang/</guid>
      <description>

&lt;p&gt;In the &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;previous post&lt;/a&gt; I have demonstrated how to make changes to interface configuration of Cisco IOS XE device using the standard &lt;strong&gt;IETF&lt;/strong&gt; model. In this post I&amp;rsquo;ll show how to use Cisco&amp;rsquo;s &lt;strong&gt;native&lt;/strong&gt; YANG model to modify static IP routes. To make things even more interesting I&amp;rsquo;ll use RESTCONF, an HTTP-based sibling of NETCONF.&lt;/p&gt;

&lt;h1 id=&#34;restconf-primer&#34;&gt;RESTCONF primer&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://www.rfc-editor.org/rfc/rfc8040.txt&#34; target=&#34;_blank&#34;&gt;RESTCONF&lt;/a&gt; is a very close functional equivalent of NETCONF. Instead of SSH, RESTCONF relies on HTTP to interact with configuration data and operational state of the network device and encodes all exchanged data in either XML or JSON. RESTCONF borrows the idea of Create-Read-Update-Delete operations on resources from &lt;a href=&#34;https://networkop.co.uk/blog/2016/01/01/rest-for-neteng/&#34;&gt;REST&lt;/a&gt; and maps them to YANG models and datastores. There is a direct relationship between NETCONF operations and RESTCONF HTTP verbs:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;HTTP VERB&lt;/th&gt;
&lt;th&gt;NETCONF OPERATION&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;POST&lt;/td&gt;
&lt;td&gt;create&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;PUT&lt;/td&gt;
&lt;td&gt;replace&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;PATCH&lt;/td&gt;
&lt;td&gt;merge&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DELETE&lt;/td&gt;
&lt;td&gt;delete&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;GET&lt;/td&gt;
&lt;td&gt;get/get-config&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Both RESTfullness and the ability to encode data as JSON make RESTCONF a very attractive choice for application developers. In this post, for the sake of simplicity, we&amp;rsquo;ll use Python CLI and &lt;code&gt;curl&lt;/code&gt; to interact with RESTCONF API. In the upcoming posts I&amp;rsquo;ll show how to implement the same functionality inside a simple Python library.&lt;/p&gt;

&lt;h1 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;ll pick up from where we left our environment in the &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;previous post&lt;/a&gt; right after we&amp;rsquo;ve configured a network interface. The following IOS CLI command enables RESTCONF&amp;rsquo;s root URL at &lt;code&gt;http://192.168.145.51/restconf/api/&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CSR1k(config)#restconf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can start exploring the structure of RESTCONF interface starting at the root URL by specifying resource names separated by &amp;ldquo;/&amp;rdquo;. For example, the following command will return all configuration from Cisco&amp;rsquo;s native datastore.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k admin:admin http://192.168.145.51/restconfi/api/config/native?deep
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to get JSON instead of the default XML output the client should specify JSON media type &lt;code&gt;application/vnd.yang.datastore+json&lt;/code&gt; and pass it in the &lt;code&gt;Accept&lt;/code&gt; header.&lt;/p&gt;

&lt;h1 id=&#34;writing-a-yang-model&#34;&gt;Writing a YANG model&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;Normally&lt;/a&gt;, you would expect to download the YANG model from the device itself. However IOS XE&amp;rsquo;s NETCONF and RESTCONF support is so new that not all of the models are available. Specifically, Cisco&amp;rsquo;s native YANG model for static routing cannot be found in either &lt;a href=&#34;https://github.com/YangModels&#34; target=&#34;_blank&#34;&gt;Yang Github Repo&lt;/a&gt; or the device itself (via &lt;code&gt;get_schema&lt;/code&gt; RPC), which makes it a very good candidate for this post.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Update 13-02-2017&lt;/strong&gt;: As it turned out, the model was right under my nose the whole time. It&amp;rsquo;s called &lt;code&gt;ned&lt;/code&gt; and encapsulates the whole of Cisco&amp;rsquo;s native datastore. So think of everything that&amp;rsquo;s to follow as a simple learning exercise, however the point I raise in the closing paragraph still stands.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The first thing we need to do is get an understanding of the structure and naming convention of the YANG model. The simplest way to do that would be to make a change on the CLI and observe the result via RESTCONF.&lt;/p&gt;

&lt;h2 id=&#34;retrieving-running-configuration-data&#34;&gt;Retrieving running configuration data&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start by adding the following static route to the IOS XE device:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip route 2.2.2.2 255.255.255.255 GigabitEthernet2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can view the configured static route via RESTCONF:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k -u admin:admin -H &amp;quot;Accept: application/vnd.yang.data+json&amp;quot; \
 http://192.168.145.51/restconf/api/config/native/ip/route?deep
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The returned output should look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{ &amp;quot;ned:route&amp;quot;: {
    &amp;quot;ip-route-interface-forwarding-list&amp;quot;: [
      { &amp;quot;prefix&amp;quot;: &amp;quot;2.2.2.2&amp;quot;,
        &amp;quot;mask&amp;quot;: &amp;quot;255.255.255.255&amp;quot;,
        &amp;quot;fwd-list&amp;quot;: [ { &amp;quot;fwd&amp;quot;: &amp;quot;GigabitEthernet2&amp;quot; } ]
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This JSON object gives us a good understanding of how the YANG model should look like. The root element &lt;code&gt;route&lt;/code&gt; contains a list of IP prefixes, called &lt;code&gt;ip-route-interface-forwarding-list&lt;/code&gt;. Each element of this list contains values for IP network and mask as well as the list of next-hops called &lt;code&gt;fwd-list&lt;/code&gt;. Let&amp;rsquo;s see how we can map this to YANG model concepts.&lt;/p&gt;

&lt;h2 id=&#34;building-a-simple-yang-model&#34;&gt;Building a simple YANG model&lt;/h2&gt;

&lt;p&gt;YANG &lt;a href=&#34;https://tools.ietf.org/html/rfc6020&#34; target=&#34;_blank&#34;&gt;RFC&lt;/a&gt; defines a number of data structures to model an XML tree. Let&amp;rsquo;s first concentrate on the three most fundamental data structures that constitute the biggest part of any YANG model:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Container&lt;/strong&gt; is a node of a tree with a unique name which encloses a set of child elements. In JSON it is mapped to a name/object pair &lt;code&gt;&#39;name&#39;: {...}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leaf&lt;/strong&gt; is a node which contains a value and does not contain any child elements. In JSON leaf is mapped to a single key/value pair &lt;code&gt;&#39;name&#39;: &#39;value&#39;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;List&lt;/strong&gt; can be thought of as a table that contains a set rows (list entries). Each list entry can contain Leafs, Containers and other elements and can be uniquely identified by at least one Leaf element called a &lt;code&gt;key&lt;/code&gt;. In JSON lists are encoded as name/arrays pairs containing JSON objects &lt;code&gt;&#39;name&#39;: [{...}, {...}]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now let&amp;rsquo;s see how we can describe the received data in terms of the above data structures:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The value of the topmost &lt;code&gt;route&lt;/code&gt; element is a JSON object, therefore it can only be mapped to a YANG container.&lt;/li&gt;
&lt;li&gt;The value of &lt;code&gt;ip-route-interface-forwarding-list&lt;/code&gt; is an array of JSON objects, therefore it must be a list.&lt;/li&gt;
&lt;li&gt;The only entry of this list contains &lt;code&gt;prefix&lt;/code&gt; and &lt;code&gt;mask&lt;/code&gt; key/value pairs. Since they don&amp;rsquo;t contain any child elements and their values are strings they can only be mapped to YANG leafs.&lt;/li&gt;
&lt;li&gt;The third element, &lt;code&gt;fwd-list&lt;/code&gt;, is another YANG list and so far contains a single next-hop value inside a YANG leaf called &lt;code&gt;fwd&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Finally, since &lt;code&gt;fwd&lt;/code&gt; is the only leaf in the &lt;code&gt;fwd-list&lt;/code&gt; list, it must be that lists&amp;rsquo; key. The &lt;code&gt;ip-route-interface-forwarding-list&lt;/code&gt; list will have both &lt;code&gt;prefix&lt;/code&gt; and &lt;code&gt;mask&lt;/code&gt; as its key values since their combination represents a unique IP destination.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With all that in mind, this is how a skeleton of our YANG model will look like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;module cisco-route-static {
  namespace &amp;quot;http://cisco.com/ns/yang/ned/ios&amp;quot;;
  prefix ned;
  container route {
    list ip-route-interface-forwarding-list {
      key &amp;quot;prefix mask&amp;quot;;
      leaf prefix { type string; }
      leaf mask { type string; }
      list fwd-list {
        key &amp;quot;fwd&amp;quot;;
        leaf fwd { type string; }
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;YANG&amp;rsquo;s syntax is pretty light-weight and looks very similar to JSON. The topmost &lt;code&gt;module&lt;/code&gt; defines the model&amp;rsquo;s name and encloses all other elements. The first two statements are used to define XML namespace and prefix that I&amp;rsquo;ve described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;refactoring-a-yang-model&#34;&gt;Refactoring a YANG model&lt;/h2&gt;

&lt;p&gt;At this stage the model can already be instantiated by &lt;strong&gt;pyang&lt;/strong&gt; and &lt;strong&gt;pyangbind&lt;/strong&gt;, however there&amp;rsquo;s a couple of very important changes and additions that I wanted to make to demonstrate some of the other features of YANG.&lt;/p&gt;

&lt;p&gt;The first of them is common IETF data types. So far in our model we&amp;rsquo;ve assumed that prefix and mask can take &lt;strong&gt;any&lt;/strong&gt; value in string format. But what if we wanted to check that the values we use are, in fact, the correctly-formatted IPv4 addresses and netmasks before sending them to the device? That is where IETF common data types come to the rescue. All what we need to do is add an import statement to define which model to use and we can start referencing them in our type definitions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;...
import ietf-yang-types { prefix &amp;quot;yang&amp;quot;; }
import ietf-inet-types { prefix &amp;quot;inet&amp;quot;; }
...
leaf prefix { type inet:ipv4-address; }
leaf mask { type yang:dotted-quad; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This solves the problem for the prefix part of a static route but how about its next-hop? Next-hops can be defined as either strings (representing an interface name) or IPv4 addresses. To make sure we can use either of these two types in the &lt;code&gt;fwd&lt;/code&gt; leaf node we can define its type as a &lt;code&gt;union&lt;/code&gt;. This built-in type is literally a union, a logical OR, of all its member elements. This is how we can change the &lt;code&gt;fwd&lt;/code&gt; leaf definition:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;...
typedef ip-next-hop {
  type union {
    type inet:ipv4-address;
    type string;
  }
}
...
leaf fwd { type ip-next-hop; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So far we&amp;rsquo;ve been concentrating on the simplest form of a static route, which doesn&amp;rsquo;t include any of the optional arguments. Let&amp;rsquo;s add the leaf nodes for name, AD, tag, track and permanent options of the static route:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;...
leaf metric { type uint8; }
leaf name { type string; }
leaf tag { type uint8; }
leaf track { type uint8; }
leaf permanent { type empty; }
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since &lt;strong&gt;track&lt;/strong&gt; and &lt;strong&gt;permanent&lt;/strong&gt; options are mutually exclusive they should not appear in the configuration at the same time. To model that we can use the &lt;code&gt;choice&lt;/code&gt; YANG statement. Let&amp;rsquo;s remove the &lt;strong&gt;track&lt;/strong&gt; and &lt;strong&gt;permanent&lt;/strong&gt; leafs from the model and replace them with this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;choice track-or-perm {
  leaf track { type uint8; }
  leaf permanent { type empty; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally, we need to add an options for VRF. When VRF is defined the whole &lt;code&gt;ip-route-interface-forwarding-list&lt;/code&gt; gets encapsulated inside a list called &lt;code&gt;vrf&lt;/code&gt;. This list has just one more leaf element &lt;code&gt;name&lt;/code&gt; which plays the role of this lists&amp;rsquo; key. In order to model this we can use another oft-used YANG concept called &lt;code&gt;grouping&lt;/code&gt;. I like to think of it as a Python function, a reusable part of code that can be referenced multiple times by its name. Here are the final changes to our model to include the VRF support:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;grouping ip-route-list {
  list ip-route-interface-forwarding-list {
      ...
  }
}
grouping vrf-grouping {
  list vrf {
    key &amp;quot;name&amp;quot;;
    leaf name { type string; }
    uses ip-route-list;
  }
}
container route {
  uses vrf-grouping;
  uses ip-route-list;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each element in a YANG model is optional by default, which means that the &lt;code&gt;route&lt;/code&gt; container can include any number of VRF and non-VRF routes. The full YANG model can be found &lt;a href=&#34;https://github.com/networkop/yang/blob/master/yang-101/cisco-route-static.yang&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;modifying-static-route-configuration&#34;&gt;Modifying static route configuration&lt;/h1&gt;

&lt;p&gt;Now let me demonstrate how to use our newly built YANG model to change the next-hop of an existing static route. Using &lt;a href=&#34;https://github.com/mbj4668/pyang&#34; target=&#34;_blank&#34;&gt;pyang&lt;/a&gt; we need to generate a Python module based on the YANG model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pyang --plugindir $PYBINDPLUGIN -f pybind -o binding.py cisco-route-static.yang
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From a Python shell, download the current static IP route configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests
url = &amp;quot;http://{h}:{p}/restconf/api/config/native/ip/route?deep&amp;quot;.format(h=&#39;192.168.145.51&#39;, p=&#39;80&#39;)
headers = {&#39;accept&#39;: &#39;application/vnd.yang.data+json&#39;}
result = requests.get(url, auth=(&#39;admin&#39;, &#39;admin&#39;), headers=headers)
current_json = result.text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Import the downloaded JSON into a YANG model instance:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import binding
import pyangbind.lib.pybindJSON as pybindJSON
model = pybindJSON.loads_ietf(current_json, binding, &amp;quot;cisco_route_static&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete the old next-hop and replace it with &lt;strong&gt;12.12.12.2&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;route = model.route.ip_route_interface_forwarding_list[&amp;quot;2.2.2.2 255.255.255.255&amp;quot;]
route.fwd_list.delete(&amp;quot;GigabitEthernet2&amp;quot;)
route.fwd_list.add(&amp;quot;12.12.12.2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save the updated model in a JSON file with the help of a &lt;a href=&#34;https://github.com/networkop/yang/blob/master/yang-101/helpers.py&#34; target=&#34;_blank&#34;&gt;write_file&lt;/a&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;json_data = pybindJSON.dumps(model, mode=&#39;ietf&#39;)
write_file(&#39;new_conf.json&#39;, json_data)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;updating-running-configuration&#34;&gt;Updating running configuration&lt;/h1&gt;

&lt;p&gt;If we tried sending the &lt;code&gt;new_conf.json&lt;/code&gt; file now, the device would have responded with an error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;missing element: prefix in /ios:native/ios:ip/ios:route/ios:ip-route-interface-forwarding-list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In our JSON file the order of elements inside a JSON object can be different from what was defined in the YANG model. This is expected since one of the fundamental principles of JSON is that an object is an &lt;strong&gt;unordered&lt;/strong&gt; collection of name/value pairs. However it looks like behind the scenes IOS XE converts JSON to XML before processing and expects all elements to come in a strict, predefined order. Fortunately, this &lt;a href=&#34;https://github.com/CiscoDevNet/openconfig-getting-started/issues/4&#34; target=&#34;_blank&#34;&gt;bug&lt;/a&gt; is already known and we can hope that Cisco will implement the fix for IOS XE soon. In the meantime, we&amp;rsquo;re gonna have to resort to sending XML.&lt;/p&gt;

&lt;p&gt;Following the procedure described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;previous post&lt;/a&gt;, we can use &lt;strong&gt;json2xml&lt;/strong&gt; tool to convert our instance into an XML document. Here we hit another issue. Since &lt;strong&gt;json2xml&lt;/strong&gt; was designed to produce a NETCONF-compliant XML, it wraps the payload inside a &lt;strong&gt;data&lt;/strong&gt; or a &lt;strong&gt;config&lt;/strong&gt; element. Thankfully, &lt;strong&gt;json2xml&lt;/strong&gt; is a Python script and can be easily patched to produce a RESTCONF-compliant XML. The following is a diff between the original and the patched files&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;408c409
&amp;lt;     if args.target not in [&amp;quot;data&amp;quot;, &amp;quot;config&amp;quot;]:
+++
&amp;gt;     if args.target not in [&amp;quot;data&amp;quot;, &amp;quot;config&amp;quot;, &amp;quot;restconf&amp;quot;]:
437c438,442
&amp;lt;     ET.ElementTree(root_el).write(outfile, encoding=&amp;quot;utf-8&amp;quot;, xml_declaration=True)
+++
&amp;gt;     if args.target != &#39;restconf&#39;:
&amp;gt;         ET.ElementTree(root_el).write(outfile, encoding=&amp;quot;utf-8&amp;quot;, xml_declaration=True)
&amp;gt;     else:
&amp;gt;         ET.ElementTree(list(root_el)[0]).write(outfile, encoding=&amp;quot;utf-8&amp;quot;, xml_declaration=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead of patching the original file, I&amp;rsquo;ve applied the above changes to a local copy of the file. Once patched, the following commands should produce the needed XML.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pyang -f jtox -o static-route.jtox cisco-route-static.yang
./json2xml -t restconf -o new_conf.xml static-route.jtox new_conf.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final step would be to send the generated XML to the IOS XE device. Since we are replacing the old static IP route configuration we&amp;rsquo;re gonna have to use HTTP PUT to overwrite the old data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k -u admin:admin -H &amp;quot;Content-Type: application/vnd.yang.data+xml&amp;quot; \
 -X PUT http://192.168.145.51/restconf/api/config/native/ip/route/ -d @new_conf.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;verification&#34;&gt;Verification&lt;/h1&gt;

&lt;p&gt;Back at the IOS XE CLI we can see the new static IP route installed.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TEST#sh run | i ip route
ip route 2.2.2.2 255.255.255.255 12.12.12.2
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;more-examples&#34;&gt;More examples&lt;/h1&gt;

&lt;p&gt;As always there are more examples available in my &lt;a href=&#34;https://github.com/networkop/yang/tree/master/yang-101&#34; target=&#34;_blank&#34;&gt;YANG 101 repo&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The exercise we&amp;rsquo;ve done in this post, though useful from a learning perspective, can come in very handy when dealing with vendors who forget or simply don&amp;rsquo;t want to share their YANG models with their customers (I know of at least one vendor that would only publish tree representations of their YANG models). In the upcoming posts I&amp;rsquo;ll show how to create a simple Python library to program static routes via RESTCONF and finally how to build an Ansible module to do that.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting Started With NETCONF and YANG on Cisco IOS XE</title>
      <link>https://networkop.co.uk/blog/2017/01/25/netconf-intro/</link>
      <pubDate>Wed, 25 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/01/25/netconf-intro/</guid>
      <description>

&lt;p&gt;To kick things off I will show how to use &lt;a href=&#34;http://ncclient.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34;&gt;ncclient&lt;/a&gt; and &lt;a href=&#34;https://github.com/mbj4668/pyang&#34; target=&#34;_blank&#34;&gt;pyang&lt;/a&gt; to configure interfaces on Cisco IOS XE device. In order to make sure everyone is on the same page and to provide some reference points for the remaining parts of the post, I would first need to cover some basic theory about NETCONF, XML and YANG.&lt;/p&gt;

&lt;h1 id=&#34;netconf-primer&#34;&gt;NETCONF primer&lt;/h1&gt;

&lt;p&gt;NETCONF is a network management protocol that runs over a secure transport (SSH, TLS etc.). It defines a set of commands (&lt;a href=&#34;abbr:Remote Procedure Call&#34; target=&#34;_blank&#34;&gt;RPCs&lt;/a&gt;) to change the state of a network device, however it does not define the structure of the exchanged information. The only requirement is for the payload to be a well-formed XML document. Effectively NETCONF provides a way for a network device to expose its API and in that sense it is very similar to &lt;a href=&#34;https://networkop.co.uk/blog/2016/01/01/rest-for-neteng/&#34;&gt;REST&lt;/a&gt;. Here are some basic NETCONF operations that will be used later in this post:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;hello&lt;/strong&gt; - messages exchanged when the NETCONF session is being established, used to advertise the list of supported capabilities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;get-config&lt;/strong&gt; - used by clients to retrieve the configuration from a network device.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;edit-config&lt;/strong&gt; - used by clients to edit the configuration of a network device.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;close-session&lt;/strong&gt; - used by clients to gracefully close the NETCONF session.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these standard NETCONF operations are implemented in &lt;a href=&#34;http://ncclient.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34;&gt;ncclient&lt;/a&gt; Python library which is what we&amp;rsquo;re going to use to talk to CSR1k.&lt;/p&gt;

&lt;h1 id=&#34;xml-primer&#34;&gt;XML primer&lt;/h1&gt;

&lt;p&gt;There are several ways to exchange structured data over the network. HTML, YAML, JSON and XML are all examples of structured data formats. XML encodes data elements in tags and nests them inside one another to create complex tree-like data structures. Thankfully we are not going to spend much time dealing with XML in this post, however there are a few basic concepts that might be useful for the overall understanding:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Root&lt;/strong&gt; - Every XML document has one root element containing one or more child elements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Path&lt;/strong&gt; - is a way of addressing a particular element inside a tree.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Namespaces&lt;/strong&gt; - provide name isolation for potentially duplicate elements. As we&amp;rsquo;ll see later, the resulting XML document may be built from several YANG models and namespaces are required to make sure there are no naming conflicts between elements.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first two concepts are similar to paths in a Linux filesystem where all of the files are laid out in a tree-like structure with root partition at its top. Namespace is somewhat similar to a unique URL identifying a particular server on the network. Using namespaces you can address multiple unique &lt;code&gt;/etc/hosts&lt;/code&gt; files by prepending the host address to the path.&lt;/p&gt;

&lt;p&gt;As with other structured data formats, XML by itself does not define the structure of the document. We still need something to organise a set of XML tags, specify what is mandatory and what is optional and what are the value constraints for the elements. This is exactly what YANG is used for.&lt;/p&gt;

&lt;h1 id=&#34;yang-primer&#34;&gt;YANG primer&lt;/h1&gt;

&lt;p&gt;YANG was conceived as a human-readable way to model the structure of an XML document. Similar to a programming language it has some primitive data types (integers, boolean, strings), several basic data structures (containers, lists, leafs) and allows users to define their own data types. The goal is to be able to formally model any network device configuration.&lt;/p&gt;

&lt;p&gt;Anyone who has ever used Ansible to &lt;a href=&#34;http://networkop.co.uk/blog/2015/08/26/automating-network-build-p1/&#34; target=&#34;_blank&#34;&gt;generate text network configuration files&lt;/a&gt; is familiar with network modelling. Coming up with a naming conventions for variables, deciding how to split them into different files, creating data structures for variables representing different parts of configuration are all a part of network modelling. YANG is similar to that kind of modelling, only this time the models are already created for you. There are three main sources of YANG models today:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Equipment Vendors&lt;/strong&gt; create their own &amp;ldquo;native&amp;rdquo; models to interact with their devices.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Standards bodies&lt;/strong&gt; (e.g. IETF and IEEE) were supposed to be the driving force of model creation. However in reality they have managed to produce only a few models that cover basic functionality like interface configuration and routing. Half of these models are still in the &amp;ldquo;DRAFT&amp;rdquo; stage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenConfig&lt;/strong&gt; working group was formed by major telcos and SPs to fill the gap left by IETF. OpenConfig has produced the most number of models so far ranging from LLDP and VLAN to segment routing and BGP configurations. Unfortunately these models are only supported by high-end SP gear and we can only hope that they will find their way into the lower-end part of the market.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Be sure to check of these and many other YANG models on &lt;a href=&#34;https://github.com/YangModels/yang&#34; target=&#34;_blank&#34;&gt;YangModels&lt;/a&gt; Github repo.&lt;/p&gt;

&lt;h1 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h1&gt;

&lt;p&gt;My test environment consists of a single instance of Cisco CSR1k running IOS XE 16.04.01. For the sake of simplicity I&amp;rsquo;m not using any network emulator and simply run it as a stand-alone VM inside VMWare Workstation. CSR1k has the following configuration applied:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;username admin privilege 15 secret admin
!
interface GigabitEthernet1
  ip address 192.168.145.51 255.255.255.0
  no shutdown
!
netconf-yang
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last command is all what&amp;rsquo;s required to enable NETCONF/YANG support.&lt;/p&gt;

&lt;p&gt;On the same hypervisor I have my development CentOS7 VM, which is connected to the same network as the first interface of CSR1k. My VM is able to ping and ssh into the CSR1k. We will need the following additional packages installed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install openssl-devel python-devel python-pip gcc
pip install ncclient pyang pyangbind ipython
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;device-configuration-workflow&#34;&gt;Device configuration workflow&lt;/h1&gt;

&lt;p&gt;The following workflow will be performed in both interactive Python shell (e.g. iPython) and Linux bash shell. The best way to follow along is to have two sessions opened, one with each of the shells. This will save you from having to rerun import statements every time you re-open a python shell.&lt;/p&gt;

&lt;h2 id=&#34;1-discovering-device-capabilities&#34;&gt;1. Discovering device capabilities&lt;/h2&gt;

&lt;p&gt;The first thing you have to do with any NETCONF-capable device is discover its capabilities. We&amp;rsquo;ll use ncclient&amp;rsquo;s &lt;a href=&#34;http://ncclient.readthedocs.io/en/latest/manager.html&#34; target=&#34;_blank&#34;&gt;manager&lt;/a&gt; module to establish a session to CSR1k. Method &lt;code&gt;.connect()&lt;/code&gt; of the manager object takes device IP, port and login credentials as input and returns a reference to a NETCONF session established with the device.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ncclient import manager

m = manager.connect(host=&#39;192.168.145.51&#39;, port=830, username=&#39;admin&#39;,
                    password=&#39;admin&#39;, device_params={&#39;name&#39;: &#39;csr&#39;})

print m.server_capabilities
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the session is established, server capabilities advertised in the &lt;strong&gt;hello&lt;/strong&gt; message get saved in the &lt;code&gt;server_capabilities&lt;/code&gt; variable. Last command should print a long list of all capabilities and supported YANG models.&lt;/p&gt;

&lt;h2 id=&#34;2-obtaining-yang-models&#34;&gt;2. Obtaining YANG models&lt;/h2&gt;

&lt;p&gt;The task we have set for ourselves is to configure an interface. CSR1k supports both native (Cisco-specific) and IETF-standard ways of doing it. In this post I&amp;rsquo;ll show how to use the IETF models to do that. First we need to identify which model to use. Based on the discovered capabilities we can guess that &lt;strong&gt;ietf-ip&lt;/strong&gt; could be used to configure IP addresses, so let&amp;rsquo;s get this model first. One way to get a YANG model is to search for it on the Internet, and since its an IETF model, it most likely can be found in of the &lt;a href=&#34;https://tools.ietf.org/html/rfc7277&#34; target=&#34;_blank&#34;&gt;RFCs&lt;/a&gt;.
Another way to get it is to download it from the device itself. All devices supporting &lt;a href=&#34;https://tools.ietf.org/html/rfc6022&#34; target=&#34;_blank&#34;&gt;RFC6022&lt;/a&gt; must be able to send the requested model in response to the &lt;code&gt;get_schema&lt;/code&gt; call. Let&amp;rsquo;s see how we can download the &lt;strong&gt;ietf-ip&lt;/strong&gt; YANG model:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;schema = m.get_schema(&#39;ietf-ip&#39;)
print schema
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this stage the model is embedded in the XML response and we still need to extract it and save it in a file. To do that we&amp;rsquo;ll use python &lt;code&gt;lxml&lt;/code&gt; library to parse the received XML document, pick the first child from the root of the tree (&lt;strong&gt;data&lt;/strong&gt; element) and save it into a variable. A helper function &lt;a href=&#34;https://github.com/networkop/yang/blob/master/helpers.py&#34; target=&#34;_blank&#34;&gt;write_file&lt;/a&gt; simply saves the Python string contained in the &lt;code&gt;yang_text&lt;/code&gt; variable in a file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import xml.etree.ElementTree as ET
root = ET.fromstring(schema.xml)
yang_text = list(root)[0].text
write_file(&#39;ietf-ip.yang&#39;, yang_text)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Back at the Linux shell we can now start using pyang. The most basic function of pyang is to convert the YANG model into one of the &lt;a href=&#34;https://github.com/mbj4668/pyang#features&#34; target=&#34;_blank&#34;&gt;many supported formats&lt;/a&gt;. For example, tree format can be very helpful for high-level understanding of the structure of a YANG model. It produces a tree-like representation of a YANG model and annotates element types and constraints using syntax described in &lt;a href=&#34;https://tools.ietf.org/html/rfc7277#section-1.2&#34; target=&#34;_blank&#34;&gt;this RFC&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pyang -f tree ietf-ip.yang | head -
module: ietf-ip
  augment /if:interfaces/if:interface:
    +--rw ipv4!
    |  +--rw enabled?      boolean
    |  +--rw forwarding?   boolean
    |  +--rw mtu?          uint16
    |  +--rw address* [ip]
    |  |  +--rw ip               inet:ipv4-address-no-zone
    |  |  +--rw (subnet)
    |  |     +--:(prefix-length)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From the output above we can see the &lt;strong&gt;ietf-ip&lt;/strong&gt; augments or extends the &lt;strong&gt;interface&lt;/strong&gt; model. It adds new configurable (rw) containers with a list of IP prefixes to be assigned to an interface. Another thing we can see is that this model cannot be used on its own, since it doesn&amp;rsquo;t specify the name of the interface it augments. This model can only be used together with &lt;code&gt;ietf-interfaces&lt;/code&gt; YANG model which models the basic interface properties like MTU, state and description. In fact &lt;code&gt;ietf-ip&lt;/code&gt; relies on a number of YANG models which are specified as imports at the beginning of the model definition.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;module ietf-ip {
 namespace &amp;quot;urn:ietf:params:xml:ns:yang:ietf-ip&amp;quot;;
 prefix ip;
 import ietf-interfaces {
   prefix if;
 }
 import ietf-inet-types {
   prefix inet;
 }
 import ietf-yang-types {
   prefix yang;
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each import statement specifies the model and the prefix by which it will be referred later in the document. These prefixes create a clear separation between namespaces of different models.&lt;/p&gt;

&lt;p&gt;We would need to download all of these models and use them together with the &lt;strong&gt;ietf-ip&lt;/strong&gt; throughout the rest of this post. Use the procedure described above to download the &lt;strong&gt;ietf-interfaces&lt;/strong&gt;, &lt;strong&gt;ietf-inet-types&lt;/strong&gt; and &lt;strong&gt;ietf-yang-types&lt;/strong&gt; models.&lt;/p&gt;

&lt;h2 id=&#34;3-instantiating-yang-models&#34;&gt;3. Instantiating YANG models&lt;/h2&gt;

&lt;p&gt;Now we can use &lt;a href=&#34;https://github.com/robshakir/pyangbind&#34; target=&#34;_blank&#34;&gt;pyangbind&lt;/a&gt;, an extension to pyang, to build a Python module based on the downloaded YANG models and start building interface configuration. Make sure your &lt;code&gt;$PYBINDPLUGIN&lt;/code&gt; variable is set like its described &lt;a href=&#34;https://github.com/robshakir/pyangbind&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pyang --plugindir $PYBINDPLUGIN -f pybind -o ietf_ip_binding.py ietf-ip.yang ietf-interfaces.yang ietf-inet-types.yang ietf-inet-types.yang
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The resulting &lt;code&gt;ietf_ip_binding.py&lt;/code&gt; is now ready for use inside the Python shell. Note that we import &lt;code&gt;ietf_interfaces&lt;/code&gt; as this is the parent object for &lt;code&gt;ietf_ip&lt;/code&gt;. The details about how to work with generated Python binding can be found on pyangbind&amp;rsquo;s &lt;a href=&#34;https://github.com/robshakir/pyangbind&#34; target=&#34;_blank&#34;&gt;Github page&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ietf_ip_binding import ietf_interfaces
model = ietf_interfaces()
model.get()
{&#39;interfaces&#39;: {&#39;interface&#39;: {}}, &#39;interfaces-state&#39;: {&#39;interface&#39;: {}}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To setup an IP address, we first need to create a model of an interface we&amp;rsquo;re planning to manipulate. We can then use &lt;code&gt;.get()&lt;/code&gt; on the model&amp;rsquo;s instance to see the list of all configurable parameters and their defaults.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_interface = model.interfaces.interface.add(&#39;GigabitEthernet2&#39;)
new_interface.get()
{&#39;description&#39;: u&#39;&#39;,
 &#39;enabled&#39;: True,
 &#39;ipv4&#39;: {&#39;address&#39;: {},
  &#39;enabled&#39;: True,
  &#39;forwarding&#39;: False,
  &#39;mtu&#39;: 0,
  &#39;neighbor&#39;: {}},
 &#39;ipv6&#39;: {&#39;address&#39;: {},
  &#39;autoconf&#39;: {&#39;create-global-addresses&#39;: True,
   &#39;create-temporary-addresses&#39;: False,
   &#39;temporary-preferred-lifetime&#39;: 86400L,
   &#39;temporary-valid-lifetime&#39;: 604800L},
  &#39;dup-addr-detect-transmits&#39;: 1L,
  &#39;enabled&#39;: True,
  &#39;forwarding&#39;: False,
  &#39;mtu&#39;: 0L,
  &#39;neighbor&#39;: {}},
 &#39;link-up-down-trap-enable&#39;: u&#39;&#39;,
 &#39;name&#39;: u&#39;GigabitEthernet2&#39;,
 &#39;type&#39;: u&#39;&#39;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The simples thing we can do is modify the interface description.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_interface.description = &#39;NETCONF-CONFIGURED PORT&#39;
new_interface.get()[&#39;description&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;New objects are added by calling &lt;code&gt;.add()&lt;/code&gt; on the parent object and passing unique key as an argument.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ipv4_addr = new_interface.ipv4.address.add(&#39;12.12.12.2&#39;)
ipv4_addr.get()
{&#39;ip&#39;: u&#39;12.12.12.2&#39;, &#39;netmask&#39;: u&#39;&#39;, &#39;prefix-length&#39;: 0}
ipv4_addr.netmask = &#39;255.255.255.0&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At the time of writing pyangbind only supported serialisation into JSON format which means we have to do a couple of extra steps to get the required XML. For now let&amp;rsquo;s dump the contents of our interface model instance into a file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pyangbind.lib.pybindJSON as pybindJSON
json_data = pybindJSON.dumps(model, mode=&#39;ietf&#39;)
write_file(&#39;new_interface.json&#39;,json_data)
print json_data
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-applying-configuration-changes&#34;&gt;4. Applying configuration changes&lt;/h2&gt;

&lt;p&gt;Even though pyanbind does not support XML, it is possible to use &lt;a href=&#34;https://github.com/mbj4668/pyang/wiki/XmlJson&#34; target=&#34;_blank&#34;&gt;other pyang plugins&lt;/a&gt; to generate XML from JSON.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pyang -f jtox -o interface.jtox ietf-ip.yang ietf-interfaces.yang ietf-inet-types.yang ietf-yang-types.yang
json2xml -t config -o interface.xml interface.jtox interface.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The resulting &lt;code&gt;interface.xml&lt;/code&gt; file contains the XML document ready to be sent to the device. I&amp;rsquo;ll use &lt;a href=&#34;https://github.com/networkop/yang/blob/master/helpers.py&#34; target=&#34;_blank&#34;&gt;read_file&lt;/a&gt; helper function to read its contents and save it into a variable. We should still have a NETCONF session opened from one of the previous steps and we&amp;rsquo;ll use the &lt;a href=&#34;https://tools.ietf.org/html/rfc6241#section-7.2&#34; target=&#34;_blank&#34;&gt;edit-config&lt;/a&gt; RPC call to apply our changes to the running configuration of CSR1k.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;xml = read_file(&#39;interface.xml&#39;)
reply = m.edit_config(target=&#39;running&#39;, config=xml)
print(&amp;quot;Success? {}&amp;quot;.format(reply.ok))
m.close_session()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the change was applied successfully &lt;code&gt;reply.ok&lt;/code&gt; should return &lt;code&gt;True&lt;/code&gt; and we can close the session to the device.&lt;/p&gt;

&lt;h2 id=&#34;verifying-changes&#34;&gt;Verifying changes&lt;/h2&gt;

&lt;p&gt;Going back to the CSR1k&amp;rsquo;s CLI we should see our changes reflected in the running configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Router#sh run int gi 2
Building configuration...

Current configuration : 126 bytes
!
interface GigabitEthernet2
 description NETCONF-CONFIGURED PORT
 ip address 12.12.12.2 255.255.255.0
 negotiation auto
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;all-in-one-scripts&#34;&gt;All-in-one scripts&lt;/h2&gt;

&lt;p&gt;Checkout &lt;a href=&#34;https://github.com/networkop/yang&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; Github page for Python scripts that implement the above workflow in a more organised way.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In this post I have merely scratched the surface of YANG modelling and network device programming. In the following posts I am planning to take a closer look at the RESTCONF interface, internal structure of a YANG model, Ansible integration and other YANG-related topics until I run out of interest. So until that happens&amp;hellip; stay tuned.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN With OVN (Part 2) - Network Engineering Analysis</title>
      <link>https://networkop.co.uk/blog/2016/12/10/ovn-part2/</link>
      <pubDate>Sat, 10 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2016/12/10/ovn-part2/</guid>
      <description>

&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#openstack-virtual-network-topology&#34;&gt;OpenStack - virtual network topology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ovn-northbound-db-logical-network-topology&#34;&gt;OVN Northbound DB - logical network topology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ovn-southbound-db-logical-flows&#34;&gt;OVN Southbound DB - logical flows&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#l2-datapath&#34;&gt;L2 datapath&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#l3-datapath&#34;&gt;L3 datapath&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ovn-controller-openflow-flows&#34;&gt;OVN Controller - OpenFlow flows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#physical-network-geneve-overlay&#34;&gt;Physical network - GENEVE overlay&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;


&lt;h1 id=&#34;openstack-virtual-network-topology&#34;&gt;OpenStack - virtual network topology&lt;/h1&gt;

&lt;p&gt;In the &lt;a href=&#34;https://networkop.co.uk/blog/2016/11/27/ovn-part1/&#34;&gt;previous post&lt;/a&gt; we have installed OpenStack and created a simple virtual topology as shown below. In OpenStack&amp;rsquo;s data model this topology consists of the following elements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Network&lt;/strong&gt; defines a virtual L2 broadcast domain&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subnet&lt;/strong&gt; attached to the network, defines an IP subnet within the network&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Router&lt;/strong&gt; provides connectivity between all directly connected subnets&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Port&lt;/strong&gt; VM&amp;rsquo;s point of attachment to the subnet&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ovn-zoom1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So far nothing unusual, this is a simple Neutron data model, all that information is stored in Neutron&amp;rsquo;s database and can be queried with &lt;code&gt;neutron&lt;/code&gt; CLI commands.&lt;/p&gt;

&lt;h1 id=&#34;ovn-northbound-db-logical-network-topology&#34;&gt;OVN Northbound DB - logical network topology&lt;/h1&gt;

&lt;p&gt;Every call to implement an element for the above data model is forwarded to OVN ML2 driver as defined by the &lt;code&gt;mechanism driver&lt;/code&gt; setting of the ML2 plugin. This driver is responsible for the creation of an appropriate data model inside the OVN Northbound DB. The main elements of this data model are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Switch&lt;/strong&gt; equivalent of a Neutron&amp;rsquo;s Subnet, enables L2 forwarding for all attached ports&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed Router&lt;/strong&gt; provides distributed routing between directly connected subnets&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gateway Router&lt;/strong&gt; provides connectivity between external networks and distributed routers, implements NAT and Load Balancing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Port&lt;/strong&gt; of a logical switch, attaches VM to the switch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a visual representation of our network topology inside OVN&amp;rsquo;s Northbound DB, built based on the output of &lt;code&gt;ovn-nbctl show&lt;/code&gt; command:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ovn-zoom2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This topology is pretty similar to Neutron&amp;rsquo;s native data model with the exception of a gateway router. In OVN, a gateway router is a special non-distributed router which performs functions that are very hard or impossible to distribute amongst all nodes, like NAT and Load Balancing. This router only exists on a single compute node which is selected by the scheduler based on the &lt;code&gt;ovn_l3_scheduler&lt;/code&gt; setting of the ML2 plugin. It is attached to a distributed router via a point-to-point /30 subnet defined in the &lt;code&gt;ovn_l3_admin_net_cidr&lt;/code&gt; setting of the ML2 plugin.&lt;/p&gt;

&lt;p&gt;Apart from the logical network topology, Northbound database keeps track of all QoS, NAT and ACL settings and their parent objects. The detailed description of all tables and properties of this database can be found in the official &lt;a href=&#34;http://openvswitch.org/support/dist-docs/ovn-nb.5.html&#34; target=&#34;_blank&#34;&gt;Northbound DB documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;ovn-southbound-db-logical-flows&#34;&gt;OVN Southbound DB - logical flows&lt;/h1&gt;

&lt;p&gt;OVN &lt;a href=&#34;http://openvswitch.org/support/dist-docs/ovn-northd.8.html&#34; target=&#34;_blank&#34;&gt;northd&lt;/a&gt; process running on the controller node translates the above logical topology into a set of tables stored in Southbound DB. Each row in those tables is a logical flow and together they form a &lt;strong&gt;forwarding pipeline&lt;/strong&gt; by stringing together multiple actions to be performed on a packet. These actions range from packet drop through packet header modification to packet output. The stringing is implemented with a special &lt;code&gt;next&lt;/code&gt; action which moves the packet one step down the pipeline starting from table 0. Let&amp;rsquo;s have a look at the &lt;strong&gt;simplified&lt;/strong&gt; versions of L2 and L3 forwarding pipelines using examples from our virtual topology.&lt;/p&gt;

&lt;h2 id=&#34;l2-datapath&#34;&gt;L2 datapath&lt;/h2&gt;

&lt;p&gt;In the first example we&amp;rsquo;ll explore the L2 datapath between VM1 and VM3. Both VMs are attached to the ports of the same logical switch. The full datapath of a logical switch consists of two parts - ingress and egress datapath (the direction is from the perspective of a logical switch). The ultimate goal of an ingress datapath is to determine the output port or ports (in case of multicast) and pass the packet to the egress datapath. The egress datapath does a few security checks before sending the packet out to its destination. Two things are worth noting at this stage:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The two datapaths can be located either on the same or on two different hypervisor nodes. In the latter case, the packet is passed between the two nodes in an overlay tunnel.&lt;/li&gt;
&lt;li&gt;The egress datapath does not have a destination lookup step which means that all information about the output port MUST be supplied by the ingress datapath. This means that destination lookup does not have to be done twice and it also has some interesting implications on the choice of encapsulation protocol as we&amp;rsquo;ll see in the next section.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ovn-zoom3-l2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s have a closer look at each of the stages of the forwarding pipeline. I&amp;rsquo;ll include snippets of logical flows demonstrating the most interesting behaviour at each stage. Full logical datapath is quite long and can be viewed with &lt;code&gt;ovn-sbctl lflow-list [DATAPATH]&lt;/code&gt; command. Here is some useful information, collected from the Northbound database, that will be used in the examples below:&lt;/p&gt;

&lt;hr /&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;VM#&lt;/th&gt;
&lt;th&gt;IP&lt;/th&gt;
&lt;th&gt;MAC&lt;/th&gt;
&lt;th&gt;Port UUID&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;VM1&lt;/td&gt;
&lt;td&gt;10.0.0.2&lt;/td&gt;
&lt;td&gt;fa:16:3e:4f:2f:b8&lt;/td&gt;
&lt;td&gt;26c23a54-6a91-48fd-a019-3bd8a7e118de&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;VM3&lt;/td&gt;
&lt;td&gt;10.0.0.5&lt;/td&gt;
&lt;td&gt;fa:16:3e:2a:60:32&lt;/td&gt;
&lt;td&gt;5c62cfbe-0b2f-4c2a-98c3-7ee76c9d2879&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Port security&lt;/strong&gt; - makes sure that incoming packet has the correct source MAC and IP addresses.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;table=0 (ls_in_port_sec_l2), priority=50, match=(inport == &amp;quot;26c23a54-6a91-48fd-a019-3bd8a7e118de&amp;quot;
  &amp;amp;&amp;amp; eth.src == {fa:16:3e:4f:2f:b8}), action=(next;)
table=1 (ls_in_port_sec_ip), priority=90, match=(inport == &amp;quot;26c23a54-6a91-48fd-a019-3bd8a7e118de&amp;quot;
  &amp;amp;&amp;amp; eth.src == fa:16:3e:4f:2f:b8 &amp;amp;&amp;amp; ip4.src == {10.0.0.2}), action=(next;)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Egress ACL&lt;/strong&gt; - set of tables that implement Neutron&amp;rsquo;s Egress Port Security functionality. Default rules allow all egress traffic from a VM. The first flow below matches all new connections coming from VM1 and marks them for connection tracking with &lt;code&gt;reg0[1] = 1&lt;/code&gt;. The next table catches these marked packets and commits them to the connection tracker. Special &lt;code&gt;ct_label=0/1&lt;/code&gt; action ensures return traffic is allowed which is a standard behaviour of all stateful firewalls.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;table=6 (ls_in_acl), priority=2002 , match=(((ct.new &amp;amp;&amp;amp; !ct.est) ||
  (!ct.new &amp;amp;&amp;amp; ct.est &amp;amp;&amp;amp; !ct.rpl &amp;amp;&amp;amp; ct_label.blocked == 1)) &amp;amp;&amp;amp;
  (inport == &amp;quot;26c23a54-6a91-48fd-a019-3bd8a7e118de&amp;quot; &amp;amp;&amp;amp; ip4)),
  action=(reg0[1] = 1; next;)
table=9 (ls_in_stateful), priority=100  , match=(reg0[1] == 1),
  action=(ct_commit(ct_label=0/1); next;)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ARP Responder&lt;/strong&gt; - matches an incoming ARP/ND request and generates an appropriate ARP/ND response. The way it is accomplished is similar to Neutron&amp;rsquo;s native &lt;a href=&#34;https://networkop.co.uk/blog/2016/05/06/neutron-l2pop/&#34;&gt;ARP responder&lt;/a&gt; feature. Effectively an ARP request gets transformed into an ARP response by swapping source and destination fields.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;table=10(ls_in_arp_rsp), priority=50, match=(arp.tpa == 10.0.0.5 &amp;amp;&amp;amp; arp.op == 1),
  action=(eth.dst = eth.src; eth.src = fa:16:3e:2a:60:32; arp.op = 2; /* ARP reply */
  arp.tha = arp.sha; arp.sha = fa:16:3e:2a:60:32; arp.tpa = arp.spa; arp.spa = 10.0.0.5;
  outport = inport; flags.loopback = 1; output;)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DHCP Processing&lt;/strong&gt; - set of tables that implement the DHCP server functionality using the approach similar to the ARP responder described above.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;table=12(ls_in_dhcp_response), priority=100, match=(inport == &amp;quot;26c23a54-6a91-48fd-a019-3bd8a7e118de&amp;quot;
  &amp;amp;&amp;amp; eth.src == fa:16:3e:4f:2f:b8 &amp;amp;&amp;amp; ip4.src == 0.0.0.0 &amp;amp;&amp;amp; ip4.dst == 255.255.255.255
  &amp;amp;&amp;amp; udp.src == 68 &amp;amp;&amp;amp; udp.dst == 67 &amp;amp;&amp;amp; reg0[3]),
  action=(eth.dst = eth.src; eth.src = fa:16:3e:94:b6:bc; ip4.dst = 10.0.0.2;
  ip4.src = 10.0.0.1; udp.src = 67; udp.dst = 68; outport = inport; flags.loopback = 1; output;)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Destination Lookup&lt;/strong&gt; - implements L2 forwarding based on the destination MAC address of a frame. At this stage the &lt;strong&gt;outport&lt;/strong&gt; variable is set to the VM3&amp;rsquo;s port UUID.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;table=13(ls_in_l2_lkup), priority=50, match=(eth.dst == fa:16:3e:2a:60:32),
  action=(outport = &amp;quot;5c62cfbe-0b2f-4c2a-98c3-7ee76c9d2879&amp;quot;; output;)

&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ingress ACL&lt;/strong&gt; - set of tables that implement Neutron&amp;rsquo;s Ingress Port security. For the sake of argument let&amp;rsquo;s assume that we have enabled inbound SSH connections. The principle is same as before - the packet gets matched in one table and submitted to connection tracking in another table.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;table=4 (ls_out_acl), priority=2002 , match=(((ct.new &amp;amp;&amp;amp; !ct.est)
  || (!ct.new &amp;amp;&amp;amp; ct.est &amp;amp;&amp;amp; !ct.rpl &amp;amp;&amp;amp; ct_label.blocked == 1))
  &amp;amp;&amp;amp; (outport == &amp;quot;26c23a54-6a91-48fd-a019-3bd8a7e118de&amp;quot; &amp;amp;&amp;amp; ip4
  &amp;amp;&amp;amp; ip4.src == 0.0.0.0/0 &amp;amp;&amp;amp; tcp &amp;amp;&amp;amp; tcp.dst == 22)),
  action=(reg0[1] = 1; next;
table=6 (ls_out_stateful), priority=100  , match=(reg0[1] == 1),
  action=(ct_commit(ct_label=0/1); next;)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Port Security&lt;/strong&gt; - implements inbound port security for destination VM by checking the sanity of destination MAC and IP addresses.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;table=7 (ls_out_port_sec_ip), priority=90, match=(outport == &amp;quot;5c62cfbe-0b2f-4c2a-98c3-7ee76c9d2879&amp;quot;
  &amp;amp;&amp;amp; eth.dst == fa:16:3e:2a:60:32 &amp;amp;&amp;amp; ip4.dst == {255.255.255.255, 224.0.0.0/4, 10.0.0.5}),
  action=(next;)
table=8 (ls_out_port_sec_l2), priority=50, match=(outport == &amp;quot;5c62cfbe-0b2f-4c2a-98c3-7ee76c9d2879&amp;quot;
  &amp;amp;&amp;amp; eth.dst == {fa:16:3e:2a:60:32}),
  action=(output;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;l3-datapath&#34;&gt;L3 datapath&lt;/h2&gt;

&lt;p&gt;Similar to a logical switch pipeline, L3 datapath is split into ingress and egress parts. In this example we&amp;rsquo;ll concentrate on  the Gateway router datapath. This router is connected to a distributed logical router via a transit subnet (SWtr) and to an external network via an external bridge (SWex) and performs NAT translation for all VM traffic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ovn-zoom3-l3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here is some useful information about router interfaces and ports that will be used in the examples below.&lt;/p&gt;

&lt;hr /&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SW function&lt;/th&gt;
&lt;th&gt;IP&lt;/th&gt;
&lt;th&gt;MAC&lt;/th&gt;
&lt;th&gt;Port UUID&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;External&lt;/td&gt;
&lt;td&gt;169.254.0.54/24&lt;/td&gt;
&lt;td&gt;fa:16:3e:39:c8:d8&lt;/td&gt;
&lt;td&gt;lrp-dc1ae9e3-d8fd-4451-aed8-3d6ddc5d095b&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DVR-GW transit&lt;/td&gt;
&lt;td&gt;169.254.128.2/30&lt;/td&gt;
&lt;td&gt;fa:16:3e:7e:96:e7&lt;/td&gt;
&lt;td&gt;lrp-gtsp-186d8754-cc4b-40fd-9e5d-b0d26fc063bd&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Port security&lt;/strong&gt; - implements sanity check for all incoming packets.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;table=0 (lr_in_admission), priority=50, match=((eth.mcast || eth.dst == fa:16:3e:7e:96:e7)
 &amp;amp;&amp;amp; inport == &amp;quot;lrp-gtsp-186d8754-cc4b-40fd-9e5d-b0d26fc063bd&amp;quot;), action=(next;)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;IP Input&lt;/strong&gt; - performs additional L3 sanity checks and implements typical IP services of a router (e.g. ICMP/ARP reply)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=1 (lr_in_ip_input), priority=100, match=(ip4.src == {169.254.128.2, 169.254.128.3}),
action=(drop;)
table=1 (lr_in_ip_input), priority=90, match=(inport == &amp;quot;lrp-gtsp-186d8754-cc4b-40fd-9e5d-b0d26fc063bd&amp;quot;
&amp;amp;&amp;amp; arp.tpa == 169.254.128.2 &amp;amp;&amp;amp; arp.op == 1),
action=(eth.dst = eth.src; eth.src = fa:16:3e:7e:96:e7; arp.op = 2;
/* ARP reply */ arp.tha = arp.sha; arp.sha = fa:16:3e:7e:96:e7;
arp.tpa = arp.spa; arp.spa = 169.254.128.2;
outport = &amp;quot;lrp-gtsp-186d8754-cc4b-40fd-9e5d-b0d26fc063bd&amp;quot;; flags.loopback = 1; output;)
table=1 (lr_in_ip_input), priority=90, match=(ip4.dst == 169.254.128.2
&amp;amp;&amp;amp; icmp4.type == 8 &amp;amp;&amp;amp; icmp4.code == 0),
action=(ip4.dst &amp;lt;-&amp;gt; ip4.src; ip.ttl = 255; icmp4.type = 0; flags.loopback = 1; next; )
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;UNSNAT&lt;/strong&gt; - translates the destination IP to the real address for packets coming from &lt;strong&gt;external&lt;/strong&gt; networks&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=3 (lr_in_unsnat), priority=100, match=(ip &amp;amp;&amp;amp; ip4.dst == 169.254.0.54),
action=(ct_snat; next;)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;DNAT&lt;/strong&gt; - implements what is commonly known as static NAT, i.e. performs one-to-one destination IP translation for every configured floating IP.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=4 (lr_in_dnat), priority=100, match=(ip &amp;amp;&amp;amp; ip4.dst == 169.254.0.52),
action=(flags.loopback = 1; ct_dnat(10.0.0.5);)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;IP routing&lt;/strong&gt; - implements L3 forwarding based on the destination IP address. At this stage the &lt;code&gt;outport&lt;/code&gt; is decided, IP TTL is decremented and the new next-hop IP is set in register0.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;table=5 (lr_in_ip_routing), priority=1, match=(ip4.dst == 0.0.0.0/0),
  action=(ip.ttl--; reg0 = 169.254.0.1; reg1 = 169.254.0.54; eth.src = fa:16:3e:39:c8:d8;
  outport = &amp;quot;lrp-dc1ae9e3-d8fd-4451-aed8-3d6ddc5d095b&amp;quot;; flags.loopback = 1; next;)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Next Hop Resolver&lt;/strong&gt; - discovers the next-hop MAC address for a packet. This could either be a statically configured value when the next-hop is an OVN-managed router or a dynamic binding learned through ARP and stored in a special &lt;code&gt;MAC_Binding&lt;/code&gt; table of Southbound DB.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=6 (lr_in_arp_resolve), priority=100, match=(outport == &amp;quot;lrp-gtsp-186d8754-cc4b-40fd-9e5d-b0d26fc063bd&amp;quot;
&amp;amp;&amp;amp; reg0 == 169.254.128.1), action=(eth.dst = fa:16:3e:2a:7f:25; next;)
table=6 (lr_in_arp_resolve), priority=0, match=(ip4),
action=(get_arp(outport, reg0); next;)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;SNAT&lt;/strong&gt; - implements what is commonly known as overload NAT. Translates source IP, source UDP/TCP port number and ICMP Query ID to hide them behind a single IP address&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;table=0 (lr_out_snat), priority=25, match=(ip &amp;amp;&amp;amp; ip4.src == 10.0.0.0/24),
  action=(ct_snat(169.254.0.54);)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt; - send the packet out the port determined during the IP routing stage.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;table=1 (lr_out_delivery), priority=100, match=(outport == &amp;quot;lrp-gtsp-186d8754-cc4b-40fd-9e5d-b0d26fc063bd&amp;quot;),
  action=(output;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This was a very high-level, abridged and simplified version of how logical datapaths are built in OVN. Hopefully this lays enough groundwork to move on to the official &lt;a href=&#34;http://openvswitch.org/support/dist-docs/ovn-northd.8.html&#34; target=&#34;_blank&#34;&gt;northd documentation&lt;/a&gt; which describes both L2 and L3 datapaths in much greater detail.&lt;/p&gt;

&lt;p&gt;Apart from the logical flows, Southbound DB also contains a number of tables that establish the logical-to-physical bindings. For example, the &lt;code&gt;Port_Binding&lt;/code&gt; table establishes binding between logical switch, logical port, logical port overlay ID (a.k.a. tunnel key) and the unique hypervisor ID. In the next section we&amp;rsquo;ll see how this information is used to translate logical flows into OpenFlow flows at each compute node. For full description of Southbound DB, its tables and their properties refer to the official SB &lt;a href=&#34;http://openvswitch.org/support/dist-docs/ovn-sb.5.html&#34; target=&#34;_blank&#34;&gt;schema documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;ovn-controller-openflow-flows&#34;&gt;OVN Controller - OpenFlow flows&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://openvswitch.org/support/dist-docs/ovn-controller.8.html&#34; target=&#34;_blank&#34;&gt;OVN Controller&lt;/a&gt; process is the distributed part of OVN SDN controller. This process, running on each compute node, connects to Southbound DB via OVSDB and configures local OVS according to information received from it. It also uses Southbound DB to exchange the physical location information with other hypervisors. The two most important bits of information that OVN controller contributes to Southbound DB are physical location of logical ports and overlay tunnel IP address. These are the last two missing pieces to map logical flows to physical nodes and networks.&lt;/p&gt;

&lt;p&gt;The whole flat space of OpenFlow tables is split into multiple areas. Tables 16 to 47 implement an ingress logical pipeline and tables 48 to 63 implement an egress logical pipeline. These tables have no notion of physical ports and are functionally equivalent to logical flows in Southbound DB. Tables 0 and 65 are responsible for mapping between the physical and logical realms. In table 0 packets are matched on the physical incoming port and assigned to a correct logical datapath as was defined by the &lt;code&gt;Port_Binding&lt;/code&gt; table. In table 65 the information about the outport, that was determined during the ingress pipeline processing, is mapped to a local physical interface and the packet is sent out.&lt;/p&gt;

&lt;p&gt;To demonstrate the details of OpenFlow implementation, I&amp;rsquo;ll use the traffic flow between VM1 and external destination (8.8.8.8). For the sake of brevity I will only cover the major steps of packet processing inside OVS, omitting security checks and ARP/DHCP processing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ovn-zoom4-openflow.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;When packets traverse OpenFlow tables they get labelled or annotated with special values to simplify matching in subsequent tables. For example, when table 0 matches the incoming port, it annotates the packet with the datapath ID. Since it would have been impractical to label packets with globally unique UUIDs from Soutbound DB, these UUIDs get mapped to smaller values called &lt;strong&gt;tunnel keys&lt;/strong&gt;. To make things even more confusing, each port will have a local kernel ID, unique within each hypervisor. We&amp;rsquo;ll need both tunnel keys and local port IDs to be able to track the packets inside the OVS. The figure below depicts all port and datapath IDs that have been collected from the Soutbound DB and local OVSDB on each hypervisor. Local port numbers are attached with a dotted line to their respective tunnel keys.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ovn-zoom4-tunnelkey.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;When VM1 sends the first packet to 8.8.8.8, it reaches OVS on local port 13. OVN Controller knows that this port belongs to VM1 and installs an OpenFlow rule to match all packets from this port and annotate them with datapath ID (OXM_OF_METADATA), incoming port ID (NXM_NX_REG14), conntrack zone (NXM_NX_REG13). It then moves these annotated packets to the first table of the ingress pipeline.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=0, priority=100,in_port=13 actions=load:0x2-&amp;gt;NXM_NX_REG13[],
  load:0x2-&amp;gt;OXM_OF_METADATA[],load:0x2-&amp;gt;NXM_NX_REG14[],
  resubmit(,16)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Skipping to the L2 MAC address lookup stage, the output port (0x1) is decided based on the destination MAC address and saved in register 15.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=29, priority=50,metadata=0x2,dl_dst=fa:16:3e:0d:df:ea
  actions=load:0x1-&amp;gt;NXM_NX_REG15[],resubmit(,32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, the packet reaches the last table where it is sent out the physical patch port interface towards R1.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=65, priority=100,reg15=0x1,metadata=0x2 actions=output:1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The other end of this patch port is connected to a local instance of distributed router R1. That means our packet, unmodified, re-enters OpenFlow table 0, only this time on a different port. Local port 2 is associated with a logical pipeline of a router, hence &lt;code&gt;metadata&lt;/code&gt; for this packet is set to 4.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=0, priority=100,in_port=2 actions=load:0x4-&amp;gt;OXM_OF_METADATA[],
  load:0x1-&amp;gt;NXM_NX_REG14[],resubmit(,16)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The packet progresses through logical router datapath and finally gets to table 21 where destination IP lookup take place. It matches the catch-all &lt;strong&gt;default route&lt;/strong&gt; rule and the values for its next-hop IP (0xa9fe8002), MAC address (fa:16:3e:2a:7f:25) and logical output port (0x03) are set.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=21, priority=1,ip,metadata=0x4 actions=dec_ttl(),load:0xa9fe8002-&amp;gt;NXM_NX_XXREG0[96..127],
  load:0xa9fe8001-&amp;gt;NXM_NX_XXREG0[64..95],mod_dl_src:fa:16:3e:2a:7f:25,
  load:0x3-&amp;gt;NXM_NX_REG15[],load:0x1-&amp;gt;NXM_NX_REG10[0],resubmit(,22)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Table 65 converts the logical output port 3 to physical port 6, which is yet another patch port connected to a transit switch.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=65, priority=100,reg15=0x3,metadata=0x4 actions=output:6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The packet once again re-enters OpenFlow pipeline from table 0, this time from port 5. Table 0 maps incoming port 5 to the logical datapath of a transit switch with Tunnel key 7.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=0, priority=100,in_port=5 actions=load:0x7-&amp;gt;OXM_OF_METADATA[],
  load:0x1-&amp;gt;NXM_NX_REG14[],resubmit(,16)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Destination lookup determines the output port (2) but this time, instead of entering the egress pipeline locally, the packet gets sent out the physical tunnel port (7) which points to the IP address of a compute node hosting the GW router. The headers of an overlay packet are populated with logical datapath ID (0x7), logical input port (copied from register 14) and logical output port (0x2).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=29, priority=50,metadata=0x7,dl_dst=fa:16:3e:7e:96:e7
  actions=load:0x2-&amp;gt;NXM_NX_REG15[],resubmit(,32)
table=32, priority=100,reg15=0x2,metadata=0x7 actions=load:0x7-&amp;gt;NXM_NX_TUN_ID[0..23],
  set_field:0x2/0xffffffff-&amp;gt;tun_metadata0,move:NXM_NX_REG14[0..14]-&amp;gt;NXM_NX_TUN_METADATA0[16..30],
  output:7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When packet reaches the destination node, it once again enters the OpenFlow table 0, but this time all information is extracted from the tunnel keys.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=0, priority=100,in_port=17 actions=move:NXM_NX_TUN_ID[0..23]-&amp;gt;OXM_OF_METADATA[0..23],
  move:NXM_NX_TUN_METADATA0[16..30]-&amp;gt;NXM_NX_REG14[0..14],
  move:NXM_NX_TUN_METADATA0[0..15]-&amp;gt;NXM_NX_REG15[0..15],
  resubmit(,33)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At the end of the transit switch datapath the packet gets sent out port 12, whose peer is patch port 16.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=65, priority=100,reg15=0x2,metadata=0x7 actions=output:12
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The packet re-enters OpenFlow table 0 from port 16, where it gets mapped to the logical datapath of a gateway router.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=0, priority=100,in_port=16 actions=load:0x2-&amp;gt;NXM_NX_REG11[],
  load:0x6-&amp;gt;NXM_NX_REG12[],load:0x6-&amp;gt;OXM_OF_METADATA[],
  load:0x2-&amp;gt;NXM_NX_REG14[],resubmit(,16)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Similar to a distributed router R1, table 21 determines the next-hop MAC address for a packet and saves the output port in register 15.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=21, priority=1,ip,metadata=0x6 actions=dec_ttl(),load:0xa9fe0001-&amp;gt;NXM_NX_XXREG0[96..127],
  load:0xa9fe0036-&amp;gt;NXM_NX_XXREG0[64..95],mod_dl_src:fa:16:3e:39:c8:d8,
  load:0x1-&amp;gt;NXM_NX_REG15[],load:0x1-&amp;gt;NXM_NX_REG10[0],resubmit(,22)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first table of an egress pipeline source-NATs packets to external IP address of the GW router.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=48, priority=33,ip,metadata=0x6,nw_src=10.0.0.2
  actions=ct(commit,table=49,zone=NXM_NX_REG12[0..15],nat(src=169.254.0.56))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The modified packet is sent out the physical port 14 towards the external switch.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=65, priority=100,reg15=0x1,metadata=0x6 actions=output:14
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;External switch determines the output port connected to the &lt;code&gt;br-ex&lt;/code&gt; on a local hypervisor and send the packet out.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;table=0, priority=100,in_port=13 actions=load:0x5-&amp;gt;NXM_NX_REG11[],
  load:0x3-&amp;gt;NXM_NX_REG12[],load:0x3-&amp;gt;OXM_OF_METADATA[],
  load:0x2-&amp;gt;NXM_NX_REG14[],resubmit(,16)
table=29, priority=0,metadata=0x3 actions=load:0xfffe-&amp;gt;NXM_NX_REG15[],resubmit(,32)
table=33, priority=100,reg15=0xfffe,metadata=0x3
  actions=load:0x1-&amp;gt;NXM_NX_REG13[],load:0x1-&amp;gt;NXM_NX_REG15[],
  resubmit(,34),load:0xfffe-&amp;gt;NXM_NX_REG15[]
table=65, priority=100,reg15=0x1,metadata=0x3 actions=output:15
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As we&amp;rsquo;ve just seen, OpenFlow repeats the logical topology by interconnecting logical datapaths of switches and routers with virtual point-to-point patch cables. This may seem like an unnecessary modelling element with a potential for a performance impact. However, when flows get installed in kernel datapath, these patch ports &lt;a href=&#34;http://galsagie.github.io/2015/11/23/ovn-l3-deepdive&#34; target=&#34;_blank&#34;&gt;do not exist&lt;/a&gt;, which means that there isn&amp;rsquo;t any performance impact on packets in fastpath.&lt;/p&gt;

&lt;h1 id=&#34;physical-network-geneve-overlay&#34;&gt;Physical network - GENEVE overlay&lt;/h1&gt;

&lt;p&gt;Before we wrap up, let us have a quick look at the new overlay protocol GENEVE. The goal of any overlay protocol is to transport all the necessary tunnel keys. With VXLAN the only tunnel key that could be transported is the Virtual Network Identifier (VNI). In OVN&amp;rsquo;s case these tunnel keys include not only the logical datapath ID (commonly known as VNI) but also both input and output port IDs. You could have carved up the 24 bits of VXLAN tunnel ID to encode all this information but this would only have given you 256 unique values per key. Some other overlay protocols, like STT have even bigger tunnel ID header size but they, too, have a strict upper limit.&lt;/p&gt;

&lt;p&gt;GENEVE was designed to have a variable-length header. The first few bytes are well-defined fixed size fields followed by variable-length Options. This kind of structure allows software developers to innovate at their own pace while still getting the benefits of hardware offload for the fixed-size portion of the header. OVN developers &lt;a href=&#34;http://openvswitch.org/support/dist-docs/ovn-architecture.7.html&#34; target=&#34;_blank&#34;&gt;decided&lt;/a&gt; to use Options header type 0x80 to store the 15-bit logical ingress port ID and a 16-bit egress port ID (an extra bit is for logical multicast groups).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ovn-zoom5-geneve.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The figure above shows the ICMP ping coming from VM1(10.0.0.2) to Google&amp;rsquo;s DNS. As I&amp;rsquo;ve showed in the previous section, GENEVE is used between the ingress and egress pipelines of a transit switch (SWtr), whose datapath ID is encoded in the VNI field (0x7). Packets enter the transit switch on port 1 and leave it on port 2. These two values are encoded in the &lt;code&gt;00010002&lt;/code&gt; value of the &lt;code&gt;Options Data&lt;/code&gt; field.&lt;/p&gt;

&lt;p&gt;So now that GENEVE has taken over as the inter-hypervisor overlay protocol, does that mean that VXLAN is dead? OVN still supports VXLAN but only for interconnects with 3rd party devices like VXLAN-VLAN gateways or VXLAN TOR switches. Rephrasing the official OVN &lt;a href=&#34;http://openvswitch.org/support/dist-docs/ovn-architecture.7.html&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;, VXLAN gateways will continue to be supported but they will have a reduced feature set due to lack of extensibility.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;OpenStack networking has always been one of the first use cases of any new SDN controller. All the major SDN platforms like ACI, NSX, Contrail, VSP or ODL have some form of OpenStack integration. And it made sense, since native Neutron networking has always been one of the biggest pain points in OpenStack deployments. As I&amp;rsquo;ve just demonstrated, OVN can now do all of the common networking functionality natively, without having to rely on 3rd party agents. In addition to that it has a fantastic &lt;a href=&#34;http://openvswitch.org/support/dist-docs/&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;, implements all forwarding inside a single OVS bridge and it is an open-source project. As an OpenStack networking solution it is still, perhaps, a few months away from being production ready - active/active HA is not supported with OVSDB, GW router scheduling options are limited, lack of native support for DNS and Metadata proxy. However I anticipate that starting from the next OpenStack release (Ocata, Feb 2017) OVN will be ready for mass deployment even by companies without an army of OVS/OpenStack developers. And when that happens there will even less need for proprietary OpenStack SDN platforms.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
