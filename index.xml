<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>networkop on networkop</title>
    <link>https://networkop.co.uk/</link>
    <description>Recent content in networkop on networkop</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Michael Kashin 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Large-scale network simulations in Kubernetes, Part 1 - Building a CNI plugin</title>
      <link>https://networkop.co.uk/post/2018-11-k8s-topo-p1/</link>
      <pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2018-11-k8s-topo-p1/</guid>
      <description>

&lt;p&gt;Building virtualised network topologies has been one of the best ways to learn new technologies and to test new designs before implementing them on a production network. There are plenty of tools that can help build arbitrary network topologies, some with an interactive GUI (e.g. &lt;a href=&#34;https://www.gns3.com/&#34; target=&#34;_blank&#34;&gt;GNS3&lt;/a&gt; or &lt;a href=&#34;http://eve-ng.net/&#34; target=&#34;_blank&#34;&gt;EVE-NG/Unetlab&lt;/a&gt;) and some &amp;ldquo;headless&amp;rdquo;, with text-based configuration files (e.g. &lt;a href=&#34;https://github.com/plajjan/vrnetlab&#34; target=&#34;_blank&#34;&gt;vrnetlab&lt;/a&gt; or &lt;a href=&#34;https://github.com/CumulusNetworks/topology_converter&#34; target=&#34;_blank&#34;&gt;topology-converter&lt;/a&gt;). All of these tools work by spinning up multiple instances of virtual devices and interconnecting them according to a user-defined topology.&lt;/p&gt;

&lt;h1 id=&#34;problem-statement&#34;&gt;Problem statement&lt;/h1&gt;

&lt;p&gt;Most of these tools were primarily designed to work on a single host. This may work well for a relatively small topology but may become a problem as the number of virtual devices grows. Let&amp;rsquo;s take Juniper vMX as an example. From the official hardware requirements &lt;a href=&#34;https://www.juniper.net/documentation/en_US/vmx14.1/topics/reference/general/vmx-hw-sw-minimums.html&#34; target=&#34;_blank&#34;&gt;page&lt;/a&gt;, the smallest vMX instance will require:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2 VMs - one for control and one for data plane&lt;/li&gt;
&lt;li&gt;2 vCPUs - one for each of the VMs&lt;/li&gt;
&lt;li&gt;8 GB of RAM - 2GB for VCP and 6GB for VFP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This does not include the resources consumed by the underlying hypervisor, which can easily eat up another vCPU + 2GB of RAM. It&amp;rsquo;s easy to imagine how quickly we can hit the upper limit of devices in a single topology if we can only use a single hypervisor host. Admittedly, vMX is one of the most resource-hungry virtual routers and using other vendor&amp;rsquo;s virtual devices may increase that upper limit. However, if the requirement is to simulate topologies with 100+ devices, no single server will be able to cope with the required load and a potential resource contention may lead to instabilities and various software bugs manifesting themselves in places we don&amp;rsquo;t expect.&lt;/p&gt;

&lt;h1 id=&#34;exploring-possible-solutions&#34;&gt;Exploring possible solutions&lt;/h1&gt;

&lt;p&gt;Ideally, in large-scale simulations, we&amp;rsquo;d want to spread the devices across multiple hosts and interconnect them so that, from the device perspective, it&amp;rsquo;d look like they are still running on the same host. To take it a step further, we&amp;rsquo;d want the virtual links to be simple point-to-point L2 segments, without any bridges in between, so that we don&amp;rsquo;t have to deal with issues when virtual bridges consume or block some of the &amp;ldquo;unexpected&amp;rdquo; traffic, e.g. LACP/STP on &lt;a href=&#34;https://patchwork.ozlabs.org/patch/819153/&#34; target=&#34;_blank&#34;&gt;Linux bridges&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;containers-vs-vms&#34;&gt;Containers vs VMs&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s possible to build multi-host VM topologies on top of a private cloud like solution like OpenStack or VMware. The operational overhead involved would be minimal as all the scheduling and network plumbing should be taken care of by virtual infrastructure manager. However this approach has several disadvantages:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In order to not depend on the underlay, all inter-VM links would need to be implemented as overlay (VMware would require NSX)&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;VMs would still be interconnected via virtual switches&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Life-cycle management of virtual topologies is not trivial, e.g. VMware requires DRS, OpenStack requires masakari&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Injecting of additional data into VMs (e.g. configuration files) requires guest OS awareness and configuration (e.g. locating and mounting of a new partition)&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In contrast, containers provide an easy way to mount volumes inside a container&amp;rsquo;s filesystem, have plenty of options for resource scheduling and orchestrators and are substantially more lightweight and customizable. As a bonus, we get a unified way to package, distribute and manage lifecycle of our containers, independent from the underlying OS.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: AFAIK only Arista and Juniper build docker container images for their devices (cEOS and cSRX). However it is possible to run any VM-based network device inside a docker container, with many examples and makefiles available on &lt;a href=&#34;https://github.com/plajjan/vrnetlab&#34; target=&#34;_blank&#34;&gt;virtnetlab&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;kubernetes-vs-swarm&#34;&gt;Kubernetes vs Swarm&lt;/h2&gt;

&lt;p&gt;If we focus on Docker, the two most popular options for container orchestration would be Kubernetes and Swarm. Swarm is a Docker&amp;rsquo;s native container orchestration tool, it requires less customisation out of the box and has a simpler data model. The primary disadvantages of using Swarm for network simulations are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/moby/moby/issues/24862&#34; target=&#34;_blank&#34;&gt;Lack of support&lt;/a&gt; for privileged containers (network admin (CAP_NET_ADMIN) capabilities may be required by virtualised network devices)&lt;/li&gt;
&lt;li&gt;Unpredictable network interface &lt;a href=&#34;https://github.com/moby/moby/issues/25181&#34; target=&#34;_blank&#34;&gt;naming and order&lt;/a&gt; inside the container&lt;/li&gt;
&lt;li&gt;Docker&amp;rsquo;s main networking plugin libnetwork is &lt;a href=&#34;https://kubernetes.io/blog/2016/01/why-kubernetes-doesnt-use-libnetwork/&#34; target=&#34;_blank&#34;&gt;opinionated&lt;/a&gt; and difficult to extend or modify&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the other hand, the approach chosen by K8s provides an easier way to modify the default behaviour of a network plugin or to create a completely new implementation. However, K8s itself imposes several requirements on CNI plugins:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All containers can communicate with all other containers without NAT&lt;/li&gt;
&lt;li&gt;All nodes can communicate with all containers (and vice-versa) without NAT&lt;/li&gt;
&lt;li&gt;The IP that a container sees itself as is the same IP that others see it as&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above also implies that communication between the containers happens at L3, which means that no container should make any assumptions about the underlying L2 transport, i.e. not use any L2 protocols(apart from ARP). Another corollary of the above requirements is that every container only has a single IP and hence a single interface, which, together with the previous L2 limitation, makes network simulations in K8s nearly impossible.&lt;/p&gt;

&lt;h2 id=&#34;multus-vs-diy&#34;&gt;Multus vs DIY&lt;/h2&gt;

&lt;p&gt;There are multiple solutions that solve the problem of a single network interface per container/pod - CNI-Genie, Knitter and &lt;a href=&#34;https://github.com/intel/multus-cni&#34; target=&#34;_blank&#34;&gt;Multus&lt;/a&gt; CNI. All of them were primarily designed for containerised VNF use cases, with the assumption that connectivity would still be provided by one of the existing plugins, which still leaves us with a number of issues:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We have to be transparent to the underlay, so we can&amp;rsquo;t use plugins that interact with the underlay (e.g. macvlan, calico)&lt;/li&gt;
&lt;li&gt;Most of the CNI plugins only provide L3 connectivity between pods (e.g. flannel, ovn, calico)&lt;/li&gt;
&lt;li&gt;The few plugins that do provide L2 overlays (e.g contiv, weave) do not support multiple interfaces and still use virtual bridges underneath&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Perhaps it would have been possible to hack one of the plugins to do what I wanted but I felt like it&amp;rsquo;d be easier to build a specialised CNI plugin to do just what I want and nothing more. As I&amp;rsquo;ve mentioned previously, developing a simple CNI plugin is not that difficult, especially if you have a clearly defined use case, which is why I&amp;rsquo;ve built &lt;a href=&#34;https://github.com/networkop/meshnet-cni&#34; target=&#34;_blank&#34;&gt;meshnet&lt;/a&gt; - a CNI plugin to build arbitrary network topologies out of point-to-point links.&lt;/p&gt;

&lt;h1 id=&#34;cni-plugin-overview&#34;&gt;CNI plugin overview&lt;/h1&gt;

&lt;p&gt;At a very high level, every CNI plugin is just a binary and a configuration file installed on K8s worker nodes. When a pod is scheduled to run on a particular node, a local node agent (kubelet) calls a CNI binary and passes all the necessary information to it. That CNI binary connects and configures network interfaces and returns the result back to kubelet. The information is passed to CNI binary in two ways - through environment variables and CNI configuration file. This is how a CNI &lt;strong&gt;ADD&lt;/strong&gt; call &lt;a href=&#34;https://www.cncf.io/wp-content/uploads/2017/11/Introduction-to-CNI-2.pdf#page=7&#34; target=&#34;_blank&#34;&gt;may&lt;/a&gt; look like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CNI_COMMAND=ADD \
CNI_CONTAINERID=$id \
CNI_NETNS=/proc/$pid/ns/net \
CNI_ARGS=K8S_POD_NAMESPACE=$namepsace;K8S_POD_NAME=$name
/opt/cni/bin/my-plugin &amp;lt; /etc/cni/net.d/my-config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The runtime parameters get passed to the plugin as environment variables and CNI configuration file gets passed to stdin. The CNI binary runs to completion and is expected to return the configured network settings back to the caller. The format of input and output, as well as environment variables, are documented in a CNI &lt;a href=&#34;https://github.com/containernetworking/cni/blob/master/SPEC.md&#34; target=&#34;_blank&#34;&gt;specification document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are plenty of other resources that cover CNI plugin development in much greater detail, I would recommend reading at least these four:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://schd.ws/hosted_files/kccnceu18/64/Kubernetes-and-the-CNI-Kubecon-218.pdf&#34; target=&#34;_blank&#34;&gt;CNI plugins best practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.altoros.com/blog/kubernetes-networking-writing-your-own-simple-cni-plug-in-with-bash/&#34; target=&#34;_blank&#34;&gt;Writing a sample CNI plugin in bash&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://logingood.github.io/kubernetes/cni/2016/05/14/netns-and-cni.html&#34; target=&#34;_blank&#34;&gt;EVPN CNI plugin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://dougbtv.com/nfvpe/2017/06/22/cni-tutorial/&#34; target=&#34;_blank&#34;&gt;Workflow for writing CNI plugins&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;meshnet-cni-architecture&#34;&gt;Meshnet CNI architecture&lt;/h1&gt;

&lt;p&gt;The goal of &lt;a href=&#34;https://github.com/networkop/meshnet-cni&#34; target=&#34;_blank&#34;&gt;meshnet&lt;/a&gt; plugin is to interconnect pods via direct point-to-point links according to some user-defined topology.  To do that, the plugin uses two types of links:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;veth&lt;/strong&gt; - to interconnect pods running on the same node&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;vxlan&lt;/strong&gt; - to interconnect pods running on different nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One thing to note is that point-to-point links are connected directly between pods, without any software bridges in between, which makes the design a lot simpler and provides a cleaner abstraction of a physical connection between network devices.&lt;/p&gt;

&lt;p&gt;The plugin consists of three main components:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;etcd&lt;/strong&gt; - a private cluster storing topology information and runtime pod metadata (e.g. pod IP address and NetNS fd)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;meshnet&lt;/strong&gt; - a CNI binary called by kubelet, responsible for pod&amp;rsquo;s network configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;meshnetd&lt;/strong&gt; - a daemon responsible for Vxlan link configuration updates&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Just like Multus, meshnet has the concept of master/default plugin, which sets up the first interface of the pod. This interface is setup by one of the existing plugins (e.g. bridge or flannel) and is used for pod&amp;rsquo;s external connectivity. The rest of the interfaces are setup according to a topology information stored in etcd.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/meshnet-arch.png&#34; alt=&#34;Meshnet Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Although the original idea of a CNI plugin was to have a single stateless binary, most of the time there&amp;rsquo;s a need to maintain some runtime state (e.g. ip routes, ip allocations etc.), which is why a lot of CNI plugins have daemons. In our case, daemon&amp;rsquo;s role is to ensure Vxlan link configurations are correct across different hosts. Using the above diagram as an example, if pod-2 comes up after pod-3, there must be a way of signalling the (node-1) VTEP IP to the remote node (node-2) and making sure that the Vxlan link on node-2 is moved into pod-3&amp;rsquo;s namespace. This is accomplished by meshnet binary issuing an HTTP PUT request to the remote node&amp;rsquo;s daemon with all the required Vxlan link attributes attached as a payload.&lt;/p&gt;

&lt;h1 id=&#34;meshnet-design-walkthrough&#34;&gt;Meshnet design walkthrough&lt;/h1&gt;

&lt;p&gt;One of the assumptions I made in the design is that topology information is uploaded into the etcd cluster before we spin up the first pod. I&amp;rsquo;ll focus on how exactly this can be done in the next post but for now, let&amp;rsquo;s assume that it&amp;rsquo;s is already there. This information needs to be structured in a very specific way and must cover every interface of every pod. The presence of this information in etcd tells meshnet binary what p2p interfaces (if any) need to be setup for the pod. Below is a sample definition of a link from pod2 to pod3:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{ &amp;quot;uid&amp;quot;:          21,
  &amp;quot;peer_pod&amp;quot;:     &amp;quot;pod3&amp;quot;,
  &amp;quot;local_intf&amp;quot;:   &amp;quot;eth2&amp;quot;,
  &amp;quot;local_ip&amp;quot;:     &amp;quot;23.23.23.2/24&amp;quot;,
  &amp;quot;peer_intf&amp;quot;:    &amp;quot;eth2&amp;quot;,
  &amp;quot;peer_ip&amp;quot;:      &amp;quot;23.23.23.3/24&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Meshnet binary is written in go and, like many other CNI plugins, contains a common skeleton code which parses input arguments and variables. Most of the plugin logic goes into &lt;code&gt;cmdAdd&lt;/code&gt; and &lt;code&gt;cmdDel&lt;/code&gt; functions that get called automatically when CNI binary is invoked by kubelet.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (
    &amp;quot;github.com/containernetworking/cni/pkg/skel&amp;quot;
    &amp;quot;github.com/containernetworking/cni/pkg/types&amp;quot;
)
func cmdAdd(args *skel.CmdArgs) error {
    // Parsing cni .conf file
    n, err := loadConf(args.StdinData)
    // Parsing CNI_ARGS environment variable
    cniArgs := k8sArgs{}
    types.LoadArgs(args.Args, &amp;amp;cniArgs)
}
func main() {
	skel.PluginMain(cmdAdd, cmdGet, cmdDel, version.All, &amp;quot;TODO&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One of the first things that happen in a &lt;code&gt;cmdAdd&lt;/code&gt; function is a &lt;code&gt;DelegateAdd&lt;/code&gt; call to let the master plugin setup the first interface of the pod. Master plugin configuration is extracted from the &lt;code&gt;delegate&lt;/code&gt; field of the meshnet CNI configuration file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func cmdAdd(args *skel.CmdArgs) error {
    ...
    r, err := delegateAdd(ctx, n.Delegate, args.IfName)
    ...
}
func delegateAdd(ctx context.Context, netconf map[string]interface{}, 
                  intfName string) 
                 (types.Result, error) {
	...
    result, err = invoke.DelegateAdd(ctx, netconf[&amp;quot;type&amp;quot;].(string), netconfBytes, nil)
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When master plugin is finished, we upload current pod&amp;rsquo;s runtime metadata to etcd. This is required so that peer pods can find and connect to our pod when needed. Specifically, they would need VTEP IP for remote vxlan links and namespace file descriptor for local veth links.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (pod *podMeta) setPodAlive(ctx context.Context, kv clientv3.KV, 
                                 netNS, srcIP string) error {

	srcIPKey := fmt.Sprintf(&amp;quot;/%s/src_ip&amp;quot;, pod.Name)
	_, err := kv.Put(ctx, srcIPKey, srcIP)

	NetNSKey := fmt.Sprintf(&amp;quot;/%s/net_ns&amp;quot;, pod.Name)
	_, err = kv.Put(ctx, NetNSKey, netNS)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this stage, we&amp;rsquo;re ready to setup pod&amp;rsquo;s links. Instead of manipulating netlink directly, I&amp;rsquo;m using &lt;a href=&#34;https://github.com/redhat-nfvpe/koko&#34; target=&#34;_blank&#34;&gt;koko&lt;/a&gt; - a high-level library that creates veth and vxlan links for containers. The simplified logic of what happens at this stage is summarised in the following code snippet:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt; // Iterate over each link of the local pod
for _, link := range *localPod.Links { 

    // Download peer pod&#39;s runtime metadata
    peerPod := &amp;amp;podMeta{Name: link.PeerPod}
    peerPod.getPodMetadata(ctx, kv)

    if peerPod.isAlive() { // If SrcIP and NetNS keys are set

        if peerPod.SrcIP == localPod.SrcIP { // If we&#39;re on the same host

            koko.MakeVeth(*myVeth, *peerVeth)

        } else  { // If we&#39;re on different hosts

            koko.MakeVxLan(*myVeth, *vxlan)
            putRequest(remoteUrl, bytes.NewBuffer(jsonPayload))

        }
    } else {
        // skip and continue
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We start by downloading metadata for each pod that we have a link to and check if it has already come up. The value of &lt;code&gt;peerPod.SrcIP&lt;/code&gt; determines whether we&amp;rsquo;re on the same node and need to setup a veth link or on different nodes and we need to setup a vxlan tunnel between them. The latter is done in two steps - first, a local Vxlan link is setup and moved to a pod&amp;rsquo;s namespace, followed by an HTTP PUT sent to the remote node&amp;rsquo;s meshnet daemon to setup a similar link on the other end.&lt;/p&gt;

&lt;h1 id=&#34;meshnet-cni-demo&#34;&gt;Meshnet CNI demo&lt;/h1&gt;

&lt;p&gt;The easiest way to walk through this demo is by running it inside a docker:dind container, with a few additional packages installed on top of it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run --rm -it --privileged docker:dind sh
# /usr/local/bin/dockerd-entrypoint.sh &amp;amp;
# apk add --no-cache jq sudo wget git bash curl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/meshnet-demo.png&#34; alt=&#34;Meshnet Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this demo, we&amp;rsquo;ll build a simple triangle 3-node topology as shown in the figure above. We start by cloning the meshnet &lt;a href=&#34;https://github.com/networkop/meshnet-cni&#34; target=&#34;_blank&#34;&gt;Github repository&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/networkop/meshnet-cni.git &amp;amp;&amp;amp; cd meshnet-cni
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, create a local 3-node K8s cluster using &lt;a href=&#34;https://github.com/kubernetes-sigs/kubeadm-dind-cluster&#34; target=&#34;_blank&#34;&gt;kubeadm-dind-cluster&lt;/a&gt;, which uses docker-in-docker to simulate individual k8s nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://raw.githubusercontent.com/kubernetes-sigs/kubeadm-dind-cluster/master/fixed/dind-cluster-v1.11.sh 
chmod +x ./dind-cluster-v1.11.sh 
./dind-cluster-v1.11.sh up
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last command may take a few minutes to download all the required images. Once the K8s cluster is ready, we can start by deploying the private etcd cluster&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export PATH=&amp;quot;$HOME/.kubeadm-dind-cluster:$PATH&amp;quot;
kubectl create -f utils/etcd.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;./tests&lt;/code&gt; directory already contains link databases for our 3-node test topology, ready to be uploaded to etcd:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ETCD_HOST=$(kubectl get service etcd-client -o json |  jq -r &#39;.spec.clusterIP&#39;)
ENDPOINTS=$ETCD_HOST:2379

echo &amp;quot;Copying JSON files to kube-master&amp;quot;
sudo cp tests/*.json /var/lib/docker/volumes/kubeadm-dind-kube-master/_data/

echo &amp;quot;Copying etcdctl to kube-master&amp;quot;
sudo cp utils/etcdctl /var/lib/docker/volumes/kubeadm-dind-kube-master/_data/
docker exec kube-master cp /dind/etcdctl /usr/local/bin/

for pod in pod1 pod2 pod3
do
    # First cleanup any existing state
    docker exec -it kube-master sh -c &amp;quot;ETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS del --prefix=true \&amp;quot;/$pod\&amp;quot;&amp;quot;

    # Next Update the links database
    docker exec -it kube-master sh -c &amp;quot;cat /dind/$pod.json | ETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS put /$pod/links&amp;quot;

    # Print the contents of links databse
    docker exec -it kube-master sh -c &amp;quot;ETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS get --prefix=true \&amp;quot;/$pod\&amp;quot;&amp;quot;

done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final missing piece is the meshnet daemonset, which installs the binary, configuration file and the meshnet daemon on every node.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create -f kube-meshnet.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only thing that&amp;rsquo;s required now is the master plugin configuration update. Since different K8s clusters can use a different plugins, the configuration file installed by the daemonset contains a dummy value which needs to be overwritten. In our case, the kubeadm-dind-cluster we&amp;rsquo;ve installed should use a default &lt;code&gt;bridge&lt;/code&gt; plugin which can be merged into our meshnet configuration file like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ETCD_HOST=$(kubectl get service etcd-client -o json |  jq -r &#39;.spec.clusterIP&#39;)
for container in kube-master kube-node-1 kube-node-2
do
    # Merge the default CNI plugin with meshnet
    docker exec $container bash -c &amp;quot;jq  -s &#39;.[1].delegate = (.[0]|del(.cniVersion))&#39; /etc/cni/net.d/cni.conf /etc/cni/net.d/meshnet.conf  | jq .[1] &amp;gt; /etc/cni/net.d/00-meshnet.conf&amp;quot;
    docker exec $container bash -c &amp;quot;sed -i &#39;s/ETCD_HOST/$ETCD_HOST/&#39; /etc/cni/net.d/00-meshnet.conf&amp;quot;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now meshnet CNI plugin is installed and configured and everything&amp;rsquo;s ready for us to create our test topology.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat tests/2node.yml | kubectl create -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command will verify that the topology has been created and confirm that pods are scheduled to the correct nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl --namespace=default get pods -o wide  |  grep pod
pod1    1/1 Running 0   1m  10.244.2.7  kube-node-1
pod2    1/1 Running 0   1m  10.244.2.6  kube-node-1
pod3    1/1 Running 0   1m  10.244.3.5  kube-node-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we can do a simple ping test to verify that we have connectivity between all 3 pods:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl exec pod1 -- sudo ping -c 1 12.12.12.2
kubectl exec pod2 -- sudo ping -c 1 23.23.23.3
kubectl exec pod3 -- sudo ping -c 1 13.13.13.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;coming-up&#34;&gt;Coming up&lt;/h1&gt;

&lt;p&gt;The process demonstrated above is quite rigid and requires a lot of manual effort to create a required topology inside a K8s cluster. In the next post, we&amp;rsquo;ll have a look at &lt;a href=&#34;https://github.com/networkop/k8s-topo&#34; target=&#34;_blank&#34;&gt;k8s-topo&lt;/a&gt; - a simple tool that orchestrates most of the above steps - generates topology data and creates pods based on a simple YAML-based topology definition file.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Serverless SDN - Network Engineering Analysis of Appswitch</title>
      <link>https://networkop.co.uk/post/2018-05-29-appswitch-sdn/</link>
      <pubDate>Thu, 21 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2018-05-29-appswitch-sdn/</guid>
      <description>

&lt;p&gt;Virtual networking has been one of the hottest areas of research and development in recent years. Kubernetes alone has, at the time of writing, 20 different &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/networking/&#34; target=&#34;_blank&#34;&gt;networking plugins&lt;/a&gt;, some of which can be &lt;a href=&#34;https://github.com/projectcalico/canal&#34; target=&#34;_blank&#34;&gt;combined&lt;/a&gt; to build even more plugins. However, if we dig a bit deeper, most of these plugins and solutions are built out of two very simple constructs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a virtual switch - anything from a linux bridge through VPP and IOVisor to OVS&lt;/li&gt;
&lt;li&gt;ACL/NAT - most commonly implemented as iptables, with anything from netfilter to eBPF under the hood&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Note1: for the purpose of this article I won&amp;rsquo;t consider service meshes as a network solution, although it clearly is one, simply because it operates higher than TCP/IP and ultimately still requires network plumbing to be in place&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If those look familiar, you&amp;rsquo;re not mistaken, they are the &lt;strong&gt;same exact&lt;/strong&gt; things that were used to connect VMs together and enforce network security policies at the dawn of SDN era almost a decade ago. Although some of these technologies have gone a long way in both features and performance, they still treat containers the same way they treated VMs. There are a few exceptions that don&amp;rsquo;t involve the above constructs, like SR-IOV, macvlan/ipvlan and running containers in host namespace, however they represent a small fraction of corner case solutions and can be safely ignored for the purpose of this discussion. That&amp;rsquo;s why for networking folk it won&amp;rsquo;t be too big a mistake to think of containers as VMs, let&amp;rsquo;s see why:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/docker-vs-vm.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At a high level both container and VM networking are exactly the same, doesn&amp;rsquo;t matter what plugin, what networking model (CNM/CNI), what vSwitch flavour or what offload technology you use. Any virtual workload must have a virtual patch cable connecting it to a vSwitch, which implements forwarding and security policies programmed by an SDN controller. These tenets have gone unchallenged since the early days of containers and this is how I, personally, always imagined a typical virtual networking solution would look like. Until I read about &lt;a href=&#34;https://apporbit.com/a-test-drive-of-appswitch-the-network-stack-from-the-future/&#34; target=&#34;_blank&#34;&gt;AppSwitch&lt;/a&gt; and got so excited I decided to sign up for a &lt;a href=&#34;https://apporbit.com/appswitch/&#34; target=&#34;_blank&#34;&gt;beta program&lt;/a&gt; just to take it apart and see how it works. But before I dive deep into its architecture, I need to provide some theoretical background and I&amp;rsquo;ll do that by zooming in on a part of the (Linux) networking stack that sits right above TCP/IP.&lt;/p&gt;

&lt;h1 id=&#34;network-socket-api&#34;&gt;Network Socket API&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s start our exploration by examining what happens when a TCP client wants to communicate with a TCP server on a remote host. The first thing that the client library does is it creates a socket by making a &lt;code&gt;socket()&lt;/code&gt; system call for a specific address family (local, IPv4, IPv6) and transport protocol (TCP, UDP). The returned value is a file descriptor that points to a socket. This file descriptor is used in all subsequent network calls.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;sockfd = socket(AF_INET, SOCK_STREAM, 0) /* Create an IPv4 TCP socket */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next the client issues a &lt;code&gt;connect()&lt;/code&gt; system call, where it passes the new socket file descriptor along with a pointer to &lt;code&gt;serv_addr&lt;/code&gt; data structures that contains the destination IP and port of the TCP server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;connect(sockfd, &amp;amp;serv_addr, sizeof(serv_addr)) /* Returns 0 if connected */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Behind the scenes the last call initiates a TCP 3-way handshake with the remote server and returns 0 if TCP sessions transitions to the &lt;strong&gt;Established&lt;/strong&gt; state.&lt;/p&gt;

&lt;p&gt;Finally, when TCP session is established, the client interacts with the socket the same way it does with any normal file by calling &lt;code&gt;read()&lt;/code&gt; and &lt;code&gt;write()&lt;/code&gt; to receive and send data to the remote server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;read(sockfd, buffer, strlen(buffer))  /* Returns the number of bytes read */
write(sockfd, buffer, strlen(buffer)) /* Returns the number of bytes written */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below is a simplified diagram that shows the above syscalls and maps them to different stages of TCP connection. Note that only the layers relevant to the purpose of this discussion are shown. For a more comprehensive overview of Linux networking stack refer to &lt;a href=&#34;http://140.120.7.21/LinuxRef/Network/LinuxNetworkStack.html&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;http://www.microhowto.info/howto/listen_for_and_accept_tcp_connections_in_c.html&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt; and &lt;a href=&#34;https://www.ibm.com/developerworks/aix/library/au-tcpsystemcalls/index.html&#34; target=&#34;_blank&#34;&gt;3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/tcp-syscalls.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;On the server side the sequence of system calls is a little different. After a server creates a socket with the &lt;code&gt;socket()&lt;/code&gt; call, it tries to &lt;code&gt;bind()&lt;/code&gt; to a particular IP and port number of the host:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;bind(sockfd, &amp;amp;serv_addr, sizeof(serv_addr)) /* Returns 0 if successful */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to start accepting incoming TCP SYN requests, the socket needs to be marked as &lt;strong&gt;passive&lt;/strong&gt; by making a &lt;code&gt;listen()&lt;/code&gt; call, which also specifies the maximum size of a queue for pending TCP connections:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;listen(sockfd, SOMAXCONN) /* Returns 0 if successful */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the TCP 3-way handshake with the client is complete and the connection transitions to the &lt;strong&gt;Established&lt;/strong&gt; state, it will be pulled off the queue by an &lt;code&gt;accept()&lt;/code&gt; call, running in an infinite loop, which returns a new &lt;strong&gt;connected&lt;/strong&gt; socket file descriptor back to the server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;newsockfd = accept(sockfd, &amp;amp;client_addr, &amp;amp;client_len) /* Create a new socket */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The server application now proceeds to read and write data similar to the client side.&lt;/p&gt;

&lt;h1 id=&#34;appswitch-high-level-architecture&#34;&gt;Appswitch high-level architecture&lt;/h1&gt;

&lt;p&gt;Appswitch is a distributed virtual networking solution that intercepts application&amp;rsquo;s network events at the system call interface level, before they get to the TCP/IP stack of the host. This allows it to make routing decisions and enforce security policies without the need for vSwitches or iptables. The way it abstracts host&amp;rsquo;s TCP/IP stack (underlay) from the Appswitch-managed application IP address space (overlay) is not too much dissimilar from the way traditional routing protocols operate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ax-overview.png&#34; alt=&#34;Appswitch architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Just like a typical distributed routing protocol, Appswitch builds a database of all known local endpoint addresses at each node, distributes it to other members of a cluster with itself as the next-hop, calculates how to forward connection setup calls between local and remote endpoints and uses those results to steer network traffic between them. Sounds very much like something BGP/OSPF/IS-IS would do, doesn&amp;rsquo;t it? Let&amp;rsquo;s have a closer look at the actual elements involved, using the diagram from the original Appswitch &lt;a href=&#34;http://hci.stanford.edu/cstr/reports/2017-01.pdf&#34; target=&#34;_blank&#34;&gt;research paper&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ax-highlevel.png&#34; alt=&#34;Appswitch architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The first thing Appswitch does when it starts is neighbor discovery. The element responsible for this is a &lt;strong&gt;Service Router&lt;/strong&gt; which uses &lt;a href=&#34;https://www.serf.io/docs/internals/gossip.html&#34; target=&#34;_blank&#34;&gt;Serf&lt;/a&gt; to efficiently discover and disseminate information in a cluster of Appswitch nodes. Serf is Hashicorp&amp;rsquo;s implementation of a &lt;a href=&#34;https://pdfs.semanticscholar.org/8712/3307869ac84fc16122043a4a313604bd948f.pdf&#34; target=&#34;_blank&#34;&gt;gossip protocol&lt;/a&gt; - a cluster membership and communication protocol. Serf should be very easy to understand for those familiar with flood-and-learn behaviour of OSPF and IS-IS on broadcast multiaccess links, with the biggest distinction being that designated routers are chosen randomly and independently by each node of the link.&lt;/p&gt;

&lt;p&gt;Whenever a new application is launched inside Appswitch, it gets moved to a dedicated network namespace, where a &lt;strong&gt;Trap Generator&lt;/strong&gt; listens for network system calls and forwards them to a &lt;strong&gt;Trap Handler&lt;/strong&gt; over a shared Unix domain socket. Together these two elements form the forwarding plane for network system calls. Every time an application issues a &lt;code&gt;socket()&lt;/code&gt; syscall, the trap generator forwards it to the trap handler, which in turn creates a socket in the host namespace and passes its reference all the way back to the calling application. Once the connection is established, all &lt;code&gt;read()&lt;/code&gt; and &lt;code&gt;write()&lt;/code&gt; syscalls will be made using the returned &lt;code&gt;sockfd&lt;/code&gt;, effectively achieving the same performance as if our application was running directly on host OS.&lt;/p&gt;

&lt;p&gt;The central element in Appswitch architecture is a &lt;strong&gt;Service Table&lt;/strong&gt; - a distributed, eventually consistent database which stores mappings between running applications and their next-hop transport addresses. This information is used by the Trap Handler in &lt;code&gt;connect()&lt;/code&gt; syscall to build the &lt;code&gt;serv_addr&lt;/code&gt; data structure with the real IP and port of the target application. In a steady state each node in an Appswitch cluster will have the same view of this database and all updates to this database will be gossiped to other members of a cluster by a Service Router.&lt;/p&gt;

&lt;h1 id=&#34;appswitch-detailed-overview&#34;&gt;Appswitch detailed overview&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s have a closer look at what happens at different stages of Appswitch lifecycle.&lt;/p&gt;

&lt;h2 id=&#34;1-installation-and-startup&#34;&gt;1. Installation and Startup&lt;/h2&gt;

&lt;p&gt;At the time of writing Appswitch is being distributed as a docker container hosted in a private docker repository on docker hub (access provided based on request). It contains a single binary executable called &lt;code&gt;ax&lt;/code&gt; which needs to be started in PID and network namespaces of the host in order to be able to talk to host&amp;rsquo;s syscall interface and track application threads. By default, Appswitch attaches to the TCP/IP stack of the host in two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It starts management REST API and Serf processes that bind to ports 6664 and 7946.&lt;/li&gt;
&lt;li&gt;It selects one of the host&amp;rsquo;s IP addresses and reserves a pool of ports (e.g. 40000-60000) that will be dynamically allocated to applications in response to &lt;code&gt;bind()&lt;/code&gt; and &lt;code&gt;connect()&lt;/code&gt; system calls.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;2-joining-a-cluster&#34;&gt;2. Joining a cluster&lt;/h2&gt;

&lt;p&gt;One of the first things a running Appswitch instance has to do is discover all members of a cluster. This is accomplished by providing an IP address of at least one node of an existing cluster as a startup parameter, which is then used to discover all other members. The information exchanged during neighbor discovery phase, which includes host IPs, names and roles, gets recorded in the Service Table and can be viewed with &lt;code&gt;ax get nodes&lt;/code&gt; command. The mechanism of neighbor discovery and monitoring inside a cluster is not Appswitch-specific so I won&amp;rsquo;t spend much time on it here, however I highly recommend reading at least the protocol overview on the official &lt;a href=&#34;https://www.serf.io/docs/internals/gossip.html&#34; target=&#34;_blank&#34;&gt;Serf website&lt;/a&gt;, as this is a very common approach in modern-day eventually consistent cluster architectures.&lt;/p&gt;

&lt;h2 id=&#34;3-cluster-scale-out&#34;&gt;3. Cluster scale-out&lt;/h2&gt;

&lt;p&gt;A cluster is a set of nodes running Appswitch daemons that can directly communicate with one another and all use Serf to monitor state of other cluster members. Like any other cluster protocol, Serf has its limitations and Appswitch has a very neat mechanism to allow services to scale beyond a single cluster boundary, e.g. between a LAN and a WAN, called Federation. In principle, it&amp;rsquo;s somewhat similar to hierarchical BGP RR design with route reflectors doing &lt;code&gt;next-hop self&lt;/code&gt;. A set of Appswitch nodes can be designated as Federation gateways, which allows them to propagate information about running applications between two different clusters. When doing so they change the IP/port information to point to themselves, which forces all inter-cluster traffic to go via one of these nodes.&lt;/p&gt;

&lt;h2 id=&#34;4-registering-a-server-application&#34;&gt;4. Registering a server application&lt;/h2&gt;

&lt;p&gt;Once Appswitch instance has joined a cluster, it&amp;rsquo;s ready to onboard its first application. Let&amp;rsquo;s start with a web server that needs to be accessible from other Appswitch-managed applications (East-West) and externally (North-South). When starting a server application, we need to provide a unique identifier that will be used by other endpoints to reach it. That identifier can be either an IP address (provided with &lt;code&gt;--ip&lt;/code&gt; argument) or a hostname (provided with &lt;code&gt;--name&lt;/code&gt; argument). If only hostname is specified, IP address will still be allocated behind the scenes and Appswitch will program its embedded DNS to make sure all other applications can reach our web server by its name.&lt;/p&gt;

&lt;p&gt;Moments after it&amp;rsquo;s been started, the web server issues a &lt;code&gt;bind()&lt;/code&gt; syscall, specifying the IP/port it wants to bind to, which gets intercepted by the trap generator and forwarded to the trap handler. The latter generates another &lt;code&gt;bind()&lt;/code&gt; syscall, however the new &lt;code&gt;bind()&lt;/code&gt; has two notable differences:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It contains the host IP and one of the reserved ports (40000 - 60000) specified at the startup&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s destined towards a host&amp;rsquo;s syscall interface&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The resulting mappings between the internal socket address of the web server, the overlay IP address assigned to it by the Appswitch and the &amp;ldquo;real&amp;rdquo; or underlay address of the host get recorded in the Service Table and gossiped to all other members of the cluster.&lt;/p&gt;

&lt;h2 id=&#34;3-processing-client-requests&#34;&gt;3. Processing client requests&lt;/h2&gt;

&lt;p&gt;Now let&amp;rsquo;s see what happens when another Appswitch application tries to connect to the previously started web server. The client may try to connect to the web server by its hostname, assuming the server was configured with one, in which case the DNS request will get handled by an embedded DNS server running on all Appswitch nodes, as a part of the same binary that runs service table and service router, and the client library will receive the overlay IP address of the web server.&lt;/p&gt;

&lt;p&gt;As soon as the &lt;code&gt;connect()&lt;/code&gt; syscall gets intercepted by the Trap Generator and forwarded to the Trap Handler, the latter consults the local Service Table and finds an entry for the web server, which was previously broadcasted to all members of the cluster. This is also a point at which Appswitch performs security isolation and load balancing:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It checks whether security zone of a client matches the one of the server and only proceeds if they are the same. This allows users to split application into different security groups and enforce isolation between them without the need for any dataplane ACLs&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;It also determines which exact server to include in the &lt;code&gt;connect()&lt;/code&gt; syscall, in case multiple servers have registered with the same IP or hostname. This provides a fully-distributed client-side load-balancing solution without the need for any middleware.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once all the checks have passed and a destination server has been determined, Trap Handler issues a &lt;code&gt;connect()&lt;/code&gt; syscall with the underlay IP and port of that server app. At the end of the 3-way TCP handshake the &lt;code&gt;connect()&lt;/code&gt; call returns 0 and the two applications continue to &lt;code&gt;read()&lt;/code&gt; and &lt;code&gt;write()&lt;/code&gt; using the sockfd provided by Appswitch, as if they were running directly in host network namespace.&lt;/p&gt;

&lt;h2 id=&#34;4-ingress-forwarding&#34;&gt;4. Ingress forwarding&lt;/h2&gt;

&lt;p&gt;When starting a server-side application, we have an option to make it available externally to clients outside of an Appswitch cluster, by mapping a server port to an arbitrary port of the host. This is accomplished with an &lt;code&gt;--expose INSIDE:OUTSIDE&lt;/code&gt; argument, which will map the &lt;strong&gt;INSIDE&lt;/strong&gt; application port to a specified &lt;strong&gt;OUTSIDE&lt;/strong&gt; port on the host. We can also simulate the behaviour of k8s &lt;strong&gt;NodePort&lt;/strong&gt; by changing this argument to &lt;code&gt;--expose INSIDE:0.0.0.0:OUTSIDE&lt;/code&gt;, which will expose the same port on ALL members of an Appswitch cluster.&lt;/p&gt;

&lt;h2 id=&#34;5-egress-forwarding&#34;&gt;5. Egress forwarding&lt;/h2&gt;

&lt;p&gt;All outbound client-side syscalls that don&amp;rsquo;t find a match in the local Service Table, will be forwarded to one of the Egress Gateway nodes. Any number of Appswitch instances can be designated as Egress Gateway nodes at startup, which makes them pick a random port from a reserved pool, broadcast their presence to the rest of the cluster and start listening for incoming connections from other members. When the client-side Trap Handler intercepts the &lt;code&gt;connect()&lt;/code&gt; syscall to an external service, it tweaks the address and sends it to one of the egress gateways instead. At the same time it communicates the real external service address to the egress gateway out-of-band. When the egress gateway receives the external service address, it will splice the two TCP sessions together connecting the Appswitch application to its intended external destination.&lt;/p&gt;

&lt;h1 id=&#34;demo&#34;&gt;Demo&lt;/h1&gt;

&lt;p&gt;Finally, the time has come for the proof of the pudding. To demonstrate how Appswitch works in real life I&amp;rsquo;ll use the famous Docker &lt;a href=&#34;https://github.com/dockersamples/example-voting-app&#34; target=&#34;_blank&#34;&gt;voting app&lt;/a&gt; - a simple application comprised of 5 (micro)services that all interact over standard APIs. To make it more realistic, I&amp;rsquo;ll split the voting app between two Docker hosts to demonstrate how different parts of the same application are able to communicate remotely, assisted by Appswitch. The following diagram shows how different application components are mapped to a pair docker hosts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ax-voting.png&#34; alt=&#34;Appswitch demo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Unlike the original voting app &lt;a href=&#34;https://github.com/dockersamples/example-voting-app/blob/master/architecture.png&#34; target=&#34;_blank&#34;&gt;diagram&lt;/a&gt; which shows the flow of data within the app, the arrows in the diagram above show the direction and destination of the initial TCP SYN, which will be important for the following explanation.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that in the demo we&amp;rsquo;ll deploy all necessary components manually, using &lt;code&gt;docker-compose&lt;/code&gt; and &lt;code&gt;docker run&lt;/code&gt; commands, which doesn&amp;rsquo;t mean the same can&amp;rsquo;t be done by an orchestrator. One of the goals of this demo is to be easy to reproduce and understand and I felt like using k8s would make it more confusing and distract the reader from the main point. For demonstration of how to use Appswitch inside k8s environment refer to the official &lt;a href=&#34;http://appswitch.readthedocs.io/en/latest/integrations.html&#34; target=&#34;_blank&#34;&gt;integration manual&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;1-installation&#34;&gt;1. Installation&lt;/h2&gt;

&lt;p&gt;In order to deploy all 5 components of this app, I&amp;rsquo;ll use two docker-compose files, which are based on the original &lt;a href=&#34;https://github.com/dockersamples/example-voting-app/blob/master/docker-compose.yml&#34; target=&#34;_blank&#34;&gt;docker-compose file&lt;/a&gt;, with a few modifications to make them work with Appswitch:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The command argument is now prepended with &lt;code&gt;/usr/bin/ax run&lt;/code&gt;, which is an Appswitch wrapper that containts the Trap Generator to tracks the network system calls&lt;/li&gt;
&lt;li&gt;Network mode is set to none - we won&amp;rsquo;t need &lt;strong&gt;any&lt;/strong&gt; network interface inside a Docker container whatsoever&lt;/li&gt;
&lt;li&gt;Volumes now include three additional items:

&lt;ul&gt;
&lt;li&gt;/var/run/appswitch - a communication channel between a trap generator running inside a container and a trap handler running in the host namespace&lt;/li&gt;
&lt;li&gt;/usr/bin/ax - an Appswitch binary made available inside a container&lt;/li&gt;
&lt;li&gt;/etc/resolv.conf - overrides container DNS settings to redirect queries to an embedded Appswitch DNS service.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These two docker-compose files are stored in my personal fork of the voting app and are the only thing that is different between my fork and the original repo. So the first step is to download the voting app:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/networkop/example-voting-app.git ax; cd ax
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To be able to run everything on a single laptop, I simulate docker hosts by running Docker daemon inside a Docker container, a pattern commonly known as docker-in-docker or &lt;a href=&#34;https://github.com/jpetazzo/dind&#34; target=&#34;_blank&#34;&gt;dind&lt;/a&gt;. Additionally I expose the Docker API ports of the two dind containers to be able to manage them from my host OS.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d --privileged --name worker-1 --hostname=worker-1 \
-p 5000:5000 -p 12375:2375 \
-v /usr/local/bin/docker-compose:/usr/local/bin/docker-compose \
-v $(pwd):$(pwd) \
docker:stable-dind

docker run -d --privileged --name worker-2 --hostname=worker-2 \
-p 5001:5001 -p 22375:2375 \
-v /usr/local/bin/docker-compose:/usr/local/bin/docker-compose \
-v $(pwd):$(pwd) \
docker:stable-dind
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These two commands will create a simple two-node topology as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ax-dind.png&#34; alt=&#34;Appswitch dind&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each docker host will have its own unique IP assigned to interface &lt;code&gt;eth0&lt;/code&gt; and will most likely have the same IP assigned to internal &lt;code&gt;docker0&lt;/code&gt; bridge. We can safely ignore the latter since docker bridge is not used by Appswitch, however we would need to know the IPs assigned to &lt;code&gt;eth0&lt;/code&gt; in order to be able to bootstrap the Serf cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export IP_1=$(docker inspect worker-1 -f &amp;quot;{{ .NetworkSettings.Networks.bridge.IPAddress }}&amp;quot;)
export IP_2=$(docker inspect worker-2 -f &amp;quot;{{ .NetworkSettings.Networks.bridge.IPAddress }}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, on each node we need to start an Appswitch daemon and pass the above IPs as input parameters.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker --host=localhost:12375 run -d --name=ax \
--pid=host `# To identify and track application threads from host namespace` \
--net=host `# To make host network available for the application` \
--privileged `# To set seccomp filter` \
-v /usr/bin:/hostbin `# To install ax to /hostbin/` \
-v /var/run/appswitch:/var/run/appswitch `# To share the UNIX socket between app and ax daemon` \
-e AX_NODE_INTERFACE=${IP_1} `# Make sure the right IP is selected`\
-e AX_NEIGHBORS=${IP_2} `# IP of peer container`\
docker.io/appswitch/ax `# Starting the main process`

docker --host=localhost:22375 run -d --name=ax \
--pid=host `# To identify and track application threads from host namespace` \
--net=host `# To make host network available for the application` \
--privileged `# To set seccomp filter` \
-v /usr/bin:/hostbin `# To install ax to /hostbin/` \
-v /var/run/appswitch:/var/run/appswitch `# To share the UNIX socket between app and ax daemon` \
-e AX_NODE_INTERFACE=${IP_2} `# Make sure the right IP is selected`\
-e AX_NEIGHBORS=${IP_1} `# IP of peer container`\
docker.io/appswitch/ax `# Starting the main process`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point our Appswitch cluster should be bootstrapped and fully functional, which we can verify by listing its members from either one of the worker nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker exec -it worker-1 ax get nodes
    NAME     CLUSTER       IP      EXTERNALIP    ROLE     APPCOUNT  
------------------------------------------------------------------
  worker-1  appswitch  172.17.0.2              [compute]  0         
  worker-2  appswitch  172.17.0.3              [compute]  0         

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the only thing that is left is to deploy our voting app using docker compose:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose --host localhost:12375 --file ./docker-compose-1.yml up -d
docker-compose --host localhost:22375 --file ./docker-compose-2.yml up -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A few minutes later the voting side of the app should become available on &lt;a href=&#34;http://localhost:5000/&#34; target=&#34;_blank&#34;&gt;localhost:5000&lt;/a&gt;, while the results can be viewed on &lt;a href=&#34;http://localhost:5001/&#34; target=&#34;_blank&#34;&gt;localhost:5001&lt;/a&gt;.You can put in more votes for cats or dogs by using &amp;ldquo;a&amp;rdquo; or &amp;ldquo;b&amp;rdquo; in the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sS -X POST --data &amp;quot;vote=a&amp;quot; http://localhost:5000 &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-verification&#34;&gt;2. Verification&lt;/h2&gt;

&lt;p&gt;Now let&amp;rsquo;s examine how Appswitch has managed to establish connectivity between all components of our voting app. The two internal server-side components are DB and Redis, and all client-side apps (vote, worker and result) are expecting to connect to them  by using their respective hostnames - &lt;code&gt;db&lt;/code&gt; and &lt;code&gt;redis&lt;/code&gt;. Appswitch enables that by running an embedded DNS server which gets programmed with hostnames based on the &lt;code&gt;--name&lt;/code&gt; argument of &lt;code&gt;ax run&lt;/code&gt; command, as shown in the following snippet from the Redis service in docker-compose-1.yml:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;command: /usr/bin/ax run --name redis redis-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Embedded DNS server returns the overlay IP assigned to Redis, which will be used later by client-side apps in their syscalls. In our case we didn&amp;rsquo;t specify this IP explicitly, so Appswitch picked a random IP address which can be viewed with &lt;code&gt;ax get apps&lt;/code&gt; command:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;APPID&lt;/th&gt;
&lt;th&gt;NODEID&lt;/th&gt;
&lt;th&gt;CLUSTER&lt;/th&gt;
&lt;th&gt;APPIP&lt;/th&gt;
&lt;th&gt;DRIVER&lt;/th&gt;
&lt;th&gt;LABELS&lt;/th&gt;
&lt;th&gt;ZONES&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;redis&lt;/td&gt;
&lt;td&gt;f00005bb&lt;/td&gt;
&lt;td&gt;worker-2&lt;/td&gt;
&lt;td&gt;appswitch&lt;/td&gt;
&lt;td&gt;10.244.17.175&lt;/td&gt;
&lt;td&gt;user&lt;/td&gt;
&lt;td&gt;zone=default&lt;/td&gt;
&lt;td&gt;[zone==default]&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;db&lt;/td&gt;
&lt;td&gt;f00004cd&lt;/td&gt;
&lt;td&gt;worker-1&lt;/td&gt;
&lt;td&gt;appswitch&lt;/td&gt;
&lt;td&gt;10.12.33.253&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;zone=default&lt;/td&gt;
&lt;td&gt;[zone==default]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As soon as DB and Redis are fully initialised and issue a &lt;code&gt;listen()&lt;/code&gt; syscall, Appswitch broadcasts the overlay-underlay address mapping to all other members of a cluster, so that each node ends up with an identical view of this table:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;CLUSTER&lt;/th&gt;
&lt;th&gt;APPID&lt;/th&gt;
&lt;th&gt;PROTO&lt;/th&gt;
&lt;th&gt;SERVICEADDR&lt;/th&gt;
&lt;th&gt;IPV4ADDR&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;worker-2&lt;/td&gt;
&lt;td&gt;appswitch&lt;/td&gt;
&lt;td&gt;f00005b9&lt;/td&gt;
&lt;td&gt;tcp&lt;/td&gt;
&lt;td&gt;redis:6379&lt;/td&gt;
&lt;td&gt;172.17.0.3:40000&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;worker-1&lt;/td&gt;
&lt;td&gt;appswitch&lt;/td&gt;
&lt;td&gt;f000056d&lt;/td&gt;
&lt;td&gt;tcp&lt;/td&gt;
&lt;td&gt;db:5432&lt;/td&gt;
&lt;td&gt;172.17.0.2:40001&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Now, when a client-side app decides to establish a TCP session with &lt;code&gt;redis&lt;/code&gt; on port 6379, it tries to &lt;code&gt;connect()&lt;/code&gt; to &lt;code&gt;10.244.17.175:6379&lt;/code&gt;, which makes the Trap Handler issue a new &lt;code&gt;connect()&lt;/code&gt; to &lt;code&gt;172.17.0.3:40000&lt;/code&gt;, the real/underlay address of redis on worker-2 node.&lt;/p&gt;

&lt;p&gt;At the same time we have two client-side apps that act as server-side apps for external connections - vote and result. Both of these apps were started with their internal http ports exposed, like in the following example from the vote service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;command: /usr/bin/ax run --expose 80:5000 python app.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When they attempt to &lt;code&gt;bind()&lt;/code&gt; to port 80, Appswitch will not try to use the dynamic port range and will try to bind to the port specified in the expose command instead:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;CLUSTER&lt;/th&gt;
&lt;th&gt;APPID&lt;/th&gt;
&lt;th&gt;PROTO&lt;/th&gt;
&lt;th&gt;SERVICEADDR&lt;/th&gt;
&lt;th&gt;IPV4ADDR&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;worker-1&lt;/td&gt;
&lt;td&gt;appswitch&lt;/td&gt;
&lt;td&gt;f00004cc&lt;/td&gt;
&lt;td&gt;tcp&lt;/td&gt;
&lt;td&gt;172.17.0.2:80&lt;/td&gt;
&lt;td&gt;172.17.0.2:5000&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;worker-2&lt;/td&gt;
&lt;td&gt;appswitch&lt;/td&gt;
&lt;td&gt;f0000627&lt;/td&gt;
&lt;td&gt;tcp&lt;/td&gt;
&lt;td&gt;172.17.0.3:80&lt;/td&gt;
&lt;td&gt;172.17.0.3:5001&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This port mapping will only make these ports available inside their respective docker hosts, which is why we&amp;rsquo;ve exposed ports 5000 and 5001 when starting dind containers earlier.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CONTAINER ID&lt;/th&gt;
&lt;th&gt;IMAGE&lt;/th&gt;
&lt;th&gt;COMMAND&lt;/th&gt;
&lt;th&gt;CREATED&lt;/th&gt;
&lt;th&gt;STATUS&lt;/th&gt;
&lt;th&gt;PORTS&lt;/th&gt;
&lt;th&gt;NAMES&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;303d5ae4d63d&lt;/td&gt;
&lt;td&gt;docker:stable-dind&lt;/td&gt;
&lt;td&gt;&amp;ldquo;dockerd-entrypoint.&amp;rdquo;&lt;/td&gt;
&lt;td&gt;12 hours ago&lt;/td&gt;
&lt;td&gt;Up 7 hours&lt;/td&gt;
&lt;td&gt;0.0.0.0:5001-&amp;gt;5001/tcp, 0.0.0.0:22375-&amp;gt;2375/tcp&lt;/td&gt;
&lt;td&gt;worker-2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2421e7556bc1&lt;/td&gt;
&lt;td&gt;docker:stable-dind&lt;/td&gt;
&lt;td&gt;&amp;ldquo;dockerd-entrypoint.&amp;rdquo;&lt;/td&gt;
&lt;td&gt;12 hours ago&lt;/td&gt;
&lt;td&gt;Up 7 hours&lt;/td&gt;
&lt;td&gt;0.0.0.0:5000-&amp;gt;5000/tcp, 0.0.0.0:12375-&amp;gt;2375/tcp&lt;/td&gt;
&lt;td&gt;worker-1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So a TCP SYN towards localhost:5000 will get PAT&amp;rsquo;ed by Docker-managed iptables of the Docker host, will hit the TCP/IP stack of the worker-1 node, which, once the handshake is complete, will issue an accept() syscall towards a Trap Handler and ultimately towards port 80 of the vote app.&lt;/p&gt;

&lt;h1 id=&#34;outro&#34;&gt;Outro&lt;/h1&gt;

&lt;p&gt;The reason why I called this article &amp;ldquo;Serverless SDN&amp;rdquo; is because I find this to be a rather fitting description of what Appswitch is. Just like serverless computing, which abstracts away the underlying OS and server management, Appswitch abstracts away the networking stack of the host and provides networking abstractions at a well-defined socket layer. Reading through the &lt;a href=&#34;https://appswitch.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34;&gt;official documentation&lt;/a&gt; and Appswitch &lt;a href=&#34;http://hci.stanford.edu/cstr/reports/2017-01.pdf&#34; target=&#34;_blank&#34;&gt;research paper&lt;/a&gt;, I can&amp;rsquo;t get rid of the thought that this is what container networking should have looked like in the first place - not linux bridges and veth pairs and not even macvlan and ipvlan devices. The original goal of containers was to encapsulate a single process and in majority of cases that single process does not need to have a full TCP/IP stack with its own interface, IP address, MAC address - all what it cares about is sending and receiving data - and this is exactly what Appswitch provides.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The problem of unpredictable interface order in multi-network Docker containers</title>
      <link>https://networkop.co.uk/post/2018-03-03-docker-multinet/</link>
      <pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2018-03-03-docker-multinet/</guid>
      <description>

&lt;p&gt;Whether we like it or not, the era of DevOps is upon us, fellow network engineers, and with it come opportunities to approach and solve common networking problems
in new, innovative ways. One such problem is automated network change validation and testing in virtual environments, something I&amp;rsquo;ve already &lt;a href=&#34;https://networkop.co.uk/blog/2016/02/19/network-ci-intro/&#34;&gt;written about&lt;/a&gt; a few years ago. The biggest problem with my original approach was that I had to create a custom &lt;a href=&#34;https://networkop.co.uk/blog/2016/01/01/rest-for-neteng/&#34;&gt;REST API SDK&lt;/a&gt; to work with a network simulation environment (UnetLab) that was never designed to be interacted with in a programmatic way. On the other hand, technologies like Docker have been very interesting since they were built around the idea of non-interactive lifecycle management and came with all &lt;a href=&#34;http://docker-py.readthedocs.io/en/stable/containers.html&#34; target=&#34;_blank&#34;&gt;API batteries&lt;/a&gt; already included. However, Docker was never intended to be used for network simulations and its support for multiple network interfaces is&amp;hellip; somewhat problematic.&lt;/p&gt;

&lt;h1 id=&#34;problem-demonstration&#34;&gt;Problem demonstration&lt;/h1&gt;

&lt;p&gt;The easiest way to understand the problem is to see it. Let&amp;rsquo;s start with a blank Docker host and create a few networks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker network create net1
docker network create net2
docker network create net3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s see what prefixes have been allocated to those networks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker network inspect -f &amp;quot;{{range .IPAM.Config }}{{.Subnet}}{{end}}&amp;quot; net1
172.17.0.0/16
docker network inspect -f &amp;quot;{{range .IPAM.Config }}{{.Subnet}}{{end}}&amp;quot; net2
172.18.0.0/16
docker network inspect -f &amp;quot;{{range .IPAM.Config }}{{.Subnet}}{{end}}&amp;quot; net3
172.19.0.0/16
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, let&amp;rsquo;s create a container and attach it to these networks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker create --name test -it alpine sh
docker network connect net1 test
docker network connect net2 test
docker network connect net3 test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now obviously you would expect for networks to appear in the same order as they were attached, right? Let&amp;rsquo;s see if it&amp;rsquo;s true:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker start test
docker exec -it test sh -c &amp;quot;ip a | grep &#39;inet&#39;&amp;quot;
inet 127.0.0.1/8 scope host lo
inet 172.26.0.2/16 brd 172.26.255.255 scope global eth0
inet 172.17.0.2/16 brd 172.17.255.255 scope global eth1
inet 172.18.0.2/16 brd 172.18.255.255 scope global eth2
inet 172.19.0.2/16 brd 172.19.255.255 scope global eth3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks good so far. The first interface (172.26.0.2/16) is the docker bridge that was attached by default in &lt;code&gt;docker create&lt;/code&gt; command. Now let&amp;rsquo;s add another network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker network create net4
docker stop test
docker network connect net4 test
docker start test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s examine our interfaces again:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker exec -it test sh -c &amp;quot;ip a | grep &#39;inet&#39;&amp;quot;
inet 127.0.0.1/8 scope host lo
inet 172.26.0.2/16 brd 172.26.255.255 scope global eth0
inet 172.20.0.2/16 brd 172.20.255.255 scope global eth3
inet 172.17.0.2/16 brd 172.17.255.255 scope global eth2
inet 172.18.0.2/16 brd 172.18.255.255 scope global eth1
inet 172.19.0.2/16 brd 172.19.255.255 scope global eth4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;re seeing that networks are in a completely different order. Looks like net1 is connected to eth2, net2 to eth1, net3 to eth4 and net4 to eth3. In fact, this issue should manifest itself even with 2 or 3 networks, however, I&amp;rsquo;ve found that it doesn&amp;rsquo;t always reorder them in that case.&lt;/p&gt;

&lt;h1 id=&#34;cnm-and-libnetwork-architecture&#34;&gt;CNM and libnetwork architecture&lt;/h1&gt;

&lt;p&gt;In order to better understand the issue, it helps to know the CNM terminology and network lifecycle events which are explained in libnetwork&amp;rsquo;s &lt;a href=&#34;https://github.com/docker/libnetwork/blob/master/docs/design.md&#34; target=&#34;_blank&#34;&gt;design document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/docker/libnetwork/raw/master/docs/cnm-model.jpg?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each time we run a &lt;code&gt;docker network create&lt;/code&gt; command a new &lt;strong&gt;CNM network&lt;/strong&gt; object is created. This object has a specific network type (&lt;code&gt;bridge&lt;/code&gt; by default) which identifies the driver to be used for the actual network implementation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;network, err := controller.NewNetwork(&amp;quot;bridge&amp;quot;, &amp;quot;net1&amp;quot;, &amp;quot;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When container gets attached to its networks, first time in &lt;code&gt;docker create&lt;/code&gt; and subsequently in &lt;code&gt;docket network connect&lt;/code&gt; commands, an &lt;strong&gt;endpoint object&lt;/strong&gt; is created on each of the networks being connected. This endpoint object represents container&amp;rsquo;s point of attachment (similar to a switch port) to docker networks and may allocate IP settings for a future network interface.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;ep, err := network.CreateEndpoint(&amp;quot;ep1&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At the time when container gets attached to its first network, a &lt;strong&gt;sandbox object&lt;/strong&gt; is created. This object represents a container inside CNM object model and stores pointers to all attached network endpoints.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;sbx, err := controller.NewSandbox(&amp;quot;test&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, when we start a container using &lt;code&gt;docker start&lt;/code&gt; command, the corresponding &lt;strong&gt;sandbox gets attached&lt;/strong&gt; to all associated network endpoints using the &lt;code&gt;ep.Join(sandbox)&lt;/code&gt; call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;for _, ep := range epList {
	if err := ep.Join(sb); err != nil {
		logrus.Warnf(&amp;quot;Failed attach sandbox %s to endpoint %s: %v\n&amp;quot;, sb.ID(), ep.ID(), err)
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;going-down-the-rabbit-hole&#34;&gt;Going down the rabbit hole&lt;/h1&gt;

&lt;p&gt;Looking at the above snippet from &lt;code&gt;sandbox.go&lt;/code&gt;, we can assume that the order in which networks will be attached to a container will depend on the order of elements inside the &lt;code&gt;epList&lt;/code&gt; array, which gets built earlier in the function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;epList := sb.getConnectedEndpoints()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s see what happens inside that method call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (sb *sandbox) getConnectedEndpoints() []*endpoint {
	sb.Lock()
	defer sb.Unlock()

	eps := make([]*endpoint, len(sb.endpoints))
	for i, ep := range sb.endpoints {
		eps[i] = ep
	}

	return eps
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So &lt;code&gt;epList&lt;/code&gt; is just an array of endpoints that gets built by copying values from &lt;code&gt;sb.endoints&lt;/code&gt;, which itself is an attribute (or field) inside the &lt;code&gt;sb&lt;/code&gt; struct.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type epHeap []*endpoint

type sandbox struct {
  id                 string
  containerID        string
...
  endpoints          epHeap
...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point it looks like &lt;code&gt;endpoints&lt;/code&gt; is just an array of pointers to endpoint objects, which still doesn&amp;rsquo;t explain the issue we&amp;rsquo;re investigating. Perhaps it would make more sense if we saw how a sandbox object gets created.&lt;/p&gt;

&lt;p&gt;Since sandbox object gets created by calling &lt;code&gt;controller.NewSandbox()&lt;/code&gt; method, let&amp;rsquo;s see exactly how this is done by looking at the code inside the &lt;code&gt;controller.go&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (c *controller) NewSandbox(containerID string, options ...SandboxOption) (Sandbox, error) {
...
  // Create sandbox and process options first. Key generation depends on an option
  if sb == nil {
    sb = &amp;amp;sandbox{
      id:                 sandboxID,
      containerID:        containerID,
      endpoints:          epHeap{},
...
    }
  }

  heap.Init(&amp;amp;sb.endpoints)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last statement explains why sandbox connects networks in random order. The &lt;code&gt;endpoints&lt;/code&gt; array is, in fact, a &lt;a href=&#34;https://golang.org/pkg/container/heap/&#34; target=&#34;_blank&#34;&gt;heap&lt;/a&gt; - an ordered tree, where parent node is always smaller than (or equal to) its children (minheap). Heap is used to implement a priority queue, which should be familiar to every network engineer who knows QoS. One of heap&amp;rsquo;s properties is that it re-orders elements every time an element gets added or removed, in order to maintain the heap invariant (parent &amp;lt;= child).&lt;/p&gt;

&lt;h1 id=&#34;problem-solution&#34;&gt;Problem solution&lt;/h1&gt;

&lt;p&gt;It turns out the problem demonstrated above is a very well-known problem with multiple opened issues on Github [&lt;a href=&#34;https://github.com/moby/moby/issues/25181&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;,&lt;a href=&#34;https://github.com/moby/moby/issues/23742&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;,&lt;a href=&#34;https://github.com/moby/moby/issues/35221&#34; target=&#34;_blank&#34;&gt;3&lt;/a&gt;]. I was lucky enough to have discovered this problem right after &lt;a href=&#34;https://github.com/docker/libnetwork/issues/2093&#34; target=&#34;_blank&#34;&gt;this pull request&lt;/a&gt; got submitted, which is what helped me understand what the issue was in the first place. This pull request reference a &lt;a href=&#34;https://github.com/cziebuhr/libnetwork/commit/d047825d4d156bc4cf01bfe410cb61b3bc33f572&#34; target=&#34;_blank&#34;&gt;patch&lt;/a&gt; that swaps the heapified array with a normal one. Below I&amp;rsquo;ll show how to build a custom docker daemon binary using this patch. We&amp;rsquo;ll start with a privileged centos-based Docker container:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Update 2018-04-28&lt;/strong&gt;: Much easier procedure is documented &lt;a href=&#34;https://github.com/networkop/libnetwork-multinet.git&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --privileged -it centos bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inside this container we need to install all the dependencies along with Docker. Yes, you need Docker to build Docker:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yum install -y git iptables \
            make &amp;quot;Development Tools&amp;quot; \
            yum-utils device-mapper-persistent-data \
            lvm2

yum-config-manager --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

yum install docker-ce -y

# Start docker in the background
/usr/bin/dockerd &amp;gt;/dev/null &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next let&amp;rsquo;s clone the Docker master branch and the patched fork of libnetwork:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --depth=1 https://github.com/docker/docker.git /tmp/docker-repo
git clone https://github.com/cziebuhr/libnetwork.git /tmp/libnetwork-patch
cd /tmp/libnetwork-patch
git checkout d047825d4d156bc4cf01bfe410cb61b3bc33f572
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I tried using &lt;a href=&#34;https://github.com/LK4D4/vndr&#34; target=&#34;_blank&#34;&gt;VNDR&lt;/a&gt; to update the libnetwork files inside the Docker repository, however I ran into problems with incompatible git options on CentOS. So instead I&amp;rsquo;ll update libnetwork manually, with just the files that are different from the original repo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /tmp/libnetwork-patch
/usr/bin/cp controller.go endpoint.go sandbox.go sandbox_store.go /tmp/docker-repo/vendor/github.com/docker/libnetwork/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Final step is to build docker binaries. This step may require up to 100G of free disk space and may take up to 60 minutes depending on your network speed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /tmp/docker-repo
make build
make binary
...
Created binary: bundles/binary-daemon/dockerd-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;verification&#34;&gt;Verification&lt;/h1&gt;

&lt;p&gt;Once done, we can retrieve the binaries outside of the build container:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;find /var/lib/docker -name dockerd
/var/lib/docker/overlay2/ac310ef5172acac7e8cb748092a9c9d1ddc3c25a91e636ab581cfde0869f5d76/diff/tmp/docker-repo/bundles/binary-daemon/dockerd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can swap the current docker daemon with the patched one:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yum install which -y
systemctl stop docker.service
DOCKERD=$(which dockerd)
mv $DOCKERD $DOCKERD-old
cp /tmp/docker-repo/bundles/latest/binary-daemon/dockerd $DOCKERD
systemctl start docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure that SELinux security context on both $DOCKERD and $DOCKERD-old are the same&lt;/p&gt;

&lt;p&gt;If we re-run our tests now, the interfaces are returned in the same exact order they were added:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker start test
docker exec -it test sh -c &amp;quot;ip a | grep &#39;inet&#39;&amp;quot;
inet 127.0.0.1/8 scope host lo
inet 172.26.0.2/16 brd 172.26.255.255 scope global eth0
inet 172.17.0.2/16 brd 172.17.255.255 scope global eth1
inet 172.18.0.2/16 brd 172.18.255.255 scope global eth2
inet 172.19.0.2/16 brd 172.19.255.255 scope global eth3
inet 172.20.0.2/16 brd 172.20.255.255 scope global eth4
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Huge kudos to the original &lt;a href=&#34;https://github.com/cziebuhr&#34; target=&#34;_blank&#34;&gt;author&lt;/a&gt; of the &lt;a href=&#34;https://github.com/cziebuhr/libnetwork/commit/d047825d4d156bc4cf01bfe410cb61b3bc33f572&#34; target=&#34;_blank&#34;&gt;libnetwork patch&lt;/a&gt; which is the sole reason this blogpost exists. I really hope that this issue will get resolved, in this form or another (could it be possible to keep track of the order in which endpoints are added to a sandbox and use that as a criteria for heap sort?), as this will make automated network testing much more approachable.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN</title>
      <link>https://networkop.co.uk/tags/openstack-sdn/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/tags/openstack-sdn/</guid>
      <description>

&lt;h2 id=&#34;openstack-sdn-tags-openstack-sdn-learn-through-hands-on-experience-everything-you-need-to-know-about-vanilla-openstack-neutron-implementation-of-virtual-networks-including-custom-sdn-controllers-like-ovn-opendaylight-and-opencontrail&#34;&gt;&lt;a href=&#34;https://networkop.co.uk/tags/openstack-sdn/&#34;&gt;OpenStack SDN&lt;/a&gt; - Learn through hands-on experience everything you need to know about vanilla OpenStack Neutron implementation of virtual networks, including custom SDN controllers like OVN, OpenDaylight and OpenContrail&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN - OpenContrail With BGP VPN</title>
      <link>https://networkop.co.uk/blog/2018/01/02/os-contrail/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2018/01/02/os-contrail/</guid>
      <description>

&lt;p&gt;Continuing on the trend started in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/12/15/os-odl-netvirt/&#34;&gt;previous post about OpenDaylight&lt;/a&gt;, I&amp;rsquo;ll move on to the next open-source product that uses BGP VPNs for optimal North-South traffic forwarding. OpenContrail is one of the most popular SDN solutions for OpenStack. It was one of the first hybrid SDN solutions, offering both pure overlay and overlay/underlay integration. It is the default SDN platform of choice for Mirantis Cloud Platform, it has multiple large-scale deployments in companies like Workday and AT&amp;amp;T. I, personally, don&amp;rsquo;t have any production experience with OpenContrail, however my impression, based on what I&amp;rsquo;ve heard and seen in the last 2-3 years that I&amp;rsquo;ve been following Telco SDN space, is that OpenContrail is the most mature SDN platform for Telco NFVs not least because of its unique feature set.&lt;/p&gt;

&lt;p&gt;During the time of production deployment at AT&amp;amp;T, Contrail has added a lot of features required by Telco NFVs like QoS, VLAN trunking and BGP-as-a-service. My first acquaintance with BGPaaS took place when I started working on Telco DCs and I remember being genuinely shocked when I first saw the requirement for dynamic routing exchange with VNFs. To me this seemed to break one of the main rules of cloud networking - a VM is not to have any knowledge or interaction with the underlay. I gradually went though all stages of grief, all the way to acceptance and although it still feels &amp;ldquo;wrong&amp;rdquo; now, I can at least understand why it&amp;rsquo;s needed and what are the pros/cons of different BGPaaS solutions.&lt;/p&gt;

&lt;h1 id=&#34;bgp-as-a-service&#34;&gt;BGP-as-a-Service&lt;/h1&gt;

&lt;p&gt;There&amp;rsquo;s a certain range of VNFs that may require to advertise a set of IP addresses into the existing VPNs inside Telco network. The most notable example is PGW inside EPC. I won&amp;rsquo;t pretend to be an expert in this field, but based on my limited understanding PGW needs to advertise IP networks into various customer VPNs, for example to connect private APNs to existing customer L3VPNs. Obviously, when this kind of network function gets virtualised, it still retains this requirement which now needs to be fulfilled by DC SDN.&lt;/p&gt;

&lt;p&gt;This requirement catches a lot of big SDN vendors off guard and the best they come up with is connecting those VNFs, through VLANs, directly to underlay TOR switches. Although this solution is easy to implement, it has an incredible amount of drawbacks since a single VNF can now affect the stability of the whole POD or even the whole DC network. Some VNFs vendors also require BFD to monitor liveliness of those BGP sessions which, in case a L3 boundary is higher than the TOR, may create even a bigger number of issues on a POD spine.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a small range of SDN platforms that run a full routing stack on each compute node (e.g. Cumulus, Calico). These solutions are the best fit for this kind of scenarios since they allow BGP sessions to be established over a single hop (VNF &amp;lt;-&amp;gt; virtual switch). However they represent a small fraction of total SDN solutions space with majority of vendors implementing a much simpler OpenFlow or XMPP-based flow push model.&lt;/p&gt;

&lt;p&gt;OpenContrail, as far as I know, is the only SDN controller that doesn&amp;rsquo;t run a full routing stack on compute nodes but still fulfills this requirement in a very elegant way. When &lt;a href=&#34;https://www.juniper.net/documentation/en_US/contrail3.2/topics/concept/bgp-as-a-service-overview.html&#34; target=&#34;_blank&#34;&gt;BGPaaS&lt;/a&gt; is enabled for a particular VM&amp;rsquo;s interface, controller programs vRouter to proxy BGP TCP connections coming to virtual network&amp;rsquo;s default gateway IP and forward them to the controller. This way VNF thinks it peers with a next hop IP, however all BGP state and path computations still happen on the controller.&lt;/p&gt;

&lt;p&gt;The diagram below depicts a sample implementation of BGPaaS using OpenContrail. VNF is connected to a vRouter using a dot1Q trunk interface (to allow multiple VRFs over a single vEth link). Each VRF has its own BGPaaS session setup to advertise network ranges (NET1-3) into customer VPNs. These BGP sessions get proxied to the controller which injects those prefixes into their respective VPNs. These updates are then sent to DC gateways using either a VPNv4/6 or EVPN and the traffic is forwarded through DC underlay with VPN segregation preserved by either an MPLS tag (for MPLSoGRE or MPLSoUDL encapsulation) or a VXLAN VNI.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/contrail-bgpaas.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now let me briefly go over the lab that I&amp;rsquo;ve built to showcase the BGPaaS and DC-GW integration features.&lt;/p&gt;

&lt;h1 id=&#34;lab-setup-overview&#34;&gt;Lab setup overview&lt;/h1&gt;

&lt;p&gt;OpenContrail follows a familiar pattern of DC SDN architecture with central controller orchestrating the work of multiple virtual switches. In case of OpenContrail, these switches are called vRouters and they communicate with controller using XMPP-based extension of BGP as described in &lt;a href=&#34;https://www.ietf.org/archive/id/draft-ietf-l3vpn-end-system-06.txt&#34; target=&#34;_blank&#34;&gt;this RFC draft&lt;/a&gt;. A very detailed description of its internal architecture is available on &lt;a href=&#34;http://www.opencontrail.org/opencontrail-architecture-documentation/&#34; target=&#34;_blank&#34;&gt;OpenContrail&amp;rsquo;s website&lt;/a&gt; so it would be pointless to repeat all of this information here. That&amp;rsquo;s why I&amp;rsquo;ll concentrate on how to get things done rather then on the architectural aspects. However to get things started, I always like to have a clear picture of what I&amp;rsquo;m trying to achieve. The below diagram depicts a high-level architecture of my lab setup. Although OpenContrail supports BGP VPNv4/6 with multiple dataplane encapsulations, in this post I&amp;rsquo;ll use EVPN as the only control plane protocol to communicate with MX80 and use VXLAN encapsulation in the dataplane.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/contrail-lab.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;EVPN as a DC-GW integration protocol is relatively new to OpenContrail and comes with a few limitations. One of them is the absence of EVPN type-5 routes, which means I can&amp;rsquo;t use it in the same way I did in &lt;a href=&#34;https://networkop.co.uk/blog/2017/12/15/os-odl-netvirt/&#34;&gt;OpenDaylight&amp;rsquo;s case&lt;/a&gt;. Instead I&amp;rsquo;ll demonstrate a DC-GW IRB scenario, which extends the existing virtual network to a DC-GW and makes IRB/SVI interface on that DC-GW act as a default gateway for this network. This is a very common scenario for L2 DCI and active-active DC deployment models. To demonstrate this scenario I&amp;rsquo;m going to setup a single OpenStack virtual network with a couple of VMs whose gateway will reside on MX80. Since I only have a single OpenStack instance and a single MX80, I&amp;rsquo;ll setup one half of L2 DCI and setup a mutual redistribution to make our overlay network reachable from MX80&amp;rsquo;s global routing table.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/contrail-overlay.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;all-in-one-vm-setup&#34;&gt;All-in-one VM setup&lt;/h1&gt;

&lt;p&gt;Physically, my lab will consist of a single hypervisor running an all-in-one VM with &lt;a href=&#34;https://docs.openstack.org/kolla/latest/&#34; target=&#34;_blank&#34;&gt;kolla-openstack&lt;/a&gt; and &lt;a href=&#34;https://github.com/Juniper/contrail-docker/wiki/OpenContrail-Kolla&#34; target=&#34;_blank&#34;&gt;kolla-contrail&lt;/a&gt; and a physical Juniper MX80 playing the role of a DC-GW.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/contrail-setup.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;OpenContrail&amp;rsquo;s &lt;a href=&#34;https://github.com/Juniper/contrail-docker/wiki/OpenContrail-Kolla&#34; target=&#34;_blank&#34;&gt;kolla github page&lt;/a&gt; contains a set of instructions to setup the environment. As usual, I have automated all of these steps which can be setup from a hypervisor with the following commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/networkop/kolla-odl-bgpvpn &amp;amp;&amp;amp; cd kolla-odl-bgpvpn
./1-create.sh do 
./2-contrail.sh do
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;openstack-setup&#34;&gt;OpenStack setup&lt;/h1&gt;

&lt;p&gt;Once installation is complete and all docker containers are up and running, we can setup the OpenStack side of our test environment. The script below will do the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download cirros and CumulusVX images and upload them to Glance&lt;/li&gt;
&lt;li&gt;Create a virtual network&lt;/li&gt;
&lt;li&gt;Update security rules to allow inbound ICMP and SSH connections&lt;/li&gt;
&lt;li&gt;Create a pair of VMs - one based on cirros and one based on CumulusVX image&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;curl -L -o ./cirros http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img
curl -L -o ./cumulusVX http://cumulusfiles.s3.amazonaws.com/cumulus-linux-3.5.0-vx-amd64.qcow2

openstack image create --disk-format qcow2 --container-format bare --public \
--property os_type=linux --file ./cirros cirros
rm ./cirros

openstack image create --disk-format qcow2 --container-format bare --public \
--property os_type=linux --file ./cumulusVX cumulus
rm ./cumulusVX

openstack network create --provider-network-type vxlan irb-net

openstack subnet create --subnet-range 10.0.100.160/27 --network irb-net \
      --host-route destination=0.0.0.0/0,gateway=10.0.100.190 \
      --gateway 10.0.100.161 --dns-nameserver 8.8.8.8 irb-subnet

openstack flavor create --id 1 --ram 256 --disk 1 --vcpus 1 m1.nano
openstack flavor create --id 2 --ram 512 --disk 10 --vcpus 1 m1.tiny

ADMIN_PROJECT_ID=$(openstack project show &#39;admin&#39; -f value -c id)
ADMIN_SEC_GROUP=$(openstack security group list --project ${ADMIN_PROJECT_ID} | awk &#39;/ default / {print $2}&#39;)
openstack security group rule create --ingress --ethertype IPv4 \
    --protocol icmp ${ADMIN_SEC_GROUP}
openstack security group rule create --ingress --ethertype IPv4 \
    --protocol tcp --dst-port 22 ${ADMIN_SEC_GROUP}

openstack server create --image cirros --flavor m1.nano --net irb-net VM1
openstack server create --image cumulus --flavor m1.tiny --net irb-net VR1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only thing worth noting in the above script is that a default gateway &lt;code&gt;10.0.100.161&lt;/code&gt; gets overridden by a default host route pointing to &lt;code&gt;10.0.100.190&lt;/code&gt;. Normally, to demonstrate DC-GW IRB scenario, I would have setup a gateway-less L2 only subnet, however in that case I wouldn&amp;rsquo;t have been able to demonstrate BGPaaS on the same network, since this feature relies on having a gateway IP setup (which later acts as a BGP session termination endpoint). So instead of setting up two separate networks I&amp;rsquo;ve decided to implement this hack to minimise the required configuration.&lt;/p&gt;

&lt;h1 id=&#34;evpn-integration-with-mx80&#34;&gt;EVPN integration with MX80&lt;/h1&gt;

&lt;p&gt;DC-GW integration procedure is very simple and requires only a few simple steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Make sure VXLAN VNI is matched on both ends&lt;/li&gt;
&lt;li&gt;Configure import/export route targets&lt;/li&gt;
&lt;li&gt;Setup BGP peering with DC-GW&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All of these steps can be done very easily through OpenContrail&amp;rsquo;s GUI. However as I&amp;rsquo;ve mentioned before, I always prefer to use API when I have a chance and in this case I even have a python library for OpenContrail&amp;rsquo;s REST API available on Juniper&amp;rsquo;s &lt;a href=&#34;https://github.com/Juniper/contrail-python-api&#34; target=&#34;_blank&#34;&gt;github page&lt;/a&gt;, which I&amp;rsquo;m going to use below to implement the above three steps.&lt;/p&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;Before we can begin working with OpenContrail&amp;rsquo;s API, we need to authenticate with the controller and get a REST API connection handler.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pycontrail.client as client
CONTRAIL_API = &#39;http://10.0.100.140:8082&#39;
AUTH_URL = &#39;http://10.0.100.140:5000/v2.0&#39;
AUTH_PARAMS = {
    &#39;type&#39;: &#39;keystone&#39;,
    &#39;username&#39;: &#39;admin&#39;,
    &#39;password&#39;: &#39;mypassword&#39;,
    &#39;tenant_name&#39;: &#39;admin&#39;,
    &#39;auth_url&#39;: AUTH_URL
}
conn = client.Client(url=CONTRAIL_API,auth_params=AUTH_PARAMS)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first thing I&amp;rsquo;m going to do is override the default VNI setup by OpenContrail for &lt;code&gt;irb-net&lt;/code&gt; to a pre-defined value of &lt;code&gt;5001&lt;/code&gt;. To do that I first need to get a handler for &lt;code&gt;irb-net&lt;/code&gt; object and extract the &lt;code&gt;virtual_network_properties&lt;/code&gt; object containing a &lt;code&gt;vxlan_network_identifier&lt;/code&gt; property. Once it&amp;rsquo;s overridden, I just need to update the parent &lt;code&gt;irb-net&lt;/code&gt; object to apply the change to the running configuration on the controller.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;irb_net = conn.virtual_network_read(fq_name = [ &#39;default-domain&#39;, &#39;admin&#39; ,&#39;irb-net&#39;] )
vni_props=irb_net.get_virtual_network_properties()
vni_props.set_vxlan_network_identifier(5001)
irb_net.set_virtual_network_properties(vni_props)
conn.virtual_network_update(irb_net)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next thing I need to do is explicitly set the import/export route-target properties for the &lt;code&gt;irb-net&lt;/code&gt; object. This will require a new &lt;code&gt;RouteTargetList&lt;/code&gt; object which then gets referenced by a &lt;code&gt;route_target_list&lt;/code&gt; property of the &lt;code&gt;irb-net&lt;/code&gt; object.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pycontrail import types as t
new_rtl = t.RouteTargetList([&#39;target:200:200&#39;])
irb_net.set_route_target_list(new_rtl)
conn.virtual_network_update(irb_net)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final step is setting up a peering with MX80. The main object that needs to be created is &lt;code&gt;BgpRouter&lt;/code&gt;, which contains a pointer to BGP session parameters object with session-specific values like ASN and remote peer IP. BGP router is defined in a global context (default domain and default project) which will make it available to all configured virtual networks.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pycontrail import types as t
ctx = [&#39;default-domain&#39;, &#39;default-project&#39;, &#39;ip-fabric&#39;, &#39;__default__&#39;]
af = t.AddressFamilies(family=[&#39;inet-vpn&#39;, &#39;e-vpn&#39;])
bgp_params = t.BgpRouterParams(vendor=&#39;Juniper&#39;, \
                               autonomous_system=65411, \
                               address=&#39;10.0.101.15&#39;, \
                               address_families=af)
vrf = conn.routing_instance_read(fq_name = ctx)
bgp_router = t.BgpRouter(name=&#39;MX80&#39;, display_name=&#39;MX80&#39;, \
                         bgp_router_parameters=bgp_params,
                         parent_obj=vrf)
contrail = conn.bgp_router_read(fq_name = ctx + [&#39;controller-1&#39;])
bgp_router.set_bgp_router(contrail,t.BgpPeeringAttributes())
conn.bgp_router_create(bgp_router)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the sake of brevity, I will not cover MX80&amp;rsquo;s configuration in details and simply include it here for reference with some minor explanatory comments.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Interface and global settings configuration
set interfaces irb unit 5001 family inet address 10.0.100.190/27
set interfaces lo0 unit 0 family inet address 10.0.101.15/32
set routing-options router-id 10.0.101.15
set routing-options autonomous-system 65411

# Setup BGP peering with OpenContrail
set protocols bgp group CONTRAIL multihop
set protocols bgp group CONTRAIL local-address 10.0.101.15
set protocols bgp group CONTRAIL family inet-vpn unicast
set protocols bgp group CONTRAIL family evpn signaling
set protocols bgp group CONTRAIL peer-as 64512
set protocols bgp group CONTRAIL neighbor 10.0.100.140

# Setup EVPN instance type with IRB interface and matching RT and VNI
set routing-instances EVPN-L2-IRB vtep-source-interface lo0.0
set routing-instances EVPN-L2-IRB instance-type evpn
set routing-instances EVPN-L2-IRB vlan-id 501
set routing-instances EVPN-L2-IRB routing-interface irb.5001
set routing-instances EVPN-L2-IRB vxlan vni 5001
set routing-instances EVPN-L2-IRB route-distinguisher 200:200
set routing-instances EVPN-L2-IRB vrf-target target:200:200
set routing-instances EVPN-L2-IRB protocols evpn encapsulation vxlan

# Setup VRF instance with IRB interface
set routing-instances EVPN-L3-IRB instance-type vrf
set routing-instances EVPN-L3-IRB interface irb.5001
set routing-instances EVPN-L3-IRB route-distinguisher 201:200
set routing-instances EVPN-L3-IRB vrf-target target:200:200

# Setup route redistribution between EVPN and Global VRFs
set routing-options rib-groups CONTRAIL-TO-GLOBAL import-rib EVPN-L3-IRB.inet.0
set routing-options rib-groups CONTRAIL-TO-GLOBAL import-rib inet.0
set routing-options rib-groups GLOBAL-TO-CONTRAIL import-rib inet.0
set routing-options rib-groups GLOBAL-TO-CONTRAIL import-rib EVPN-L3-IRB.inet.0
set routing-options interface-routes rib-group inet CONTRAIL-TO-GLOBAL
set routing-instances EVPN-L3-IRB routing-options interface-routes rib-group inet CONTRAIL-TO-GLOBAL
set protocols bgp group EXTERNAL-BGP family inet unicast rib-group GLOBAL-TO-CONTRAIL
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;verification&#34;&gt;Verification&lt;/h2&gt;

&lt;p&gt;The easiest way to verify that BGP peering has been established is to query OpenContrail&amp;rsquo;s introspection API:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl  -s http://10.0.100.140:8083/Snh_BgpNeighborReq?ip_address=10.0.101.15 | \
  xmllint --xpath &#39;/BgpNeighborListResp/neighbors[1]/list/BgpNeighborResp/state&#39; -
&amp;lt;state type=&amp;quot;string&amp;quot; identifier=&amp;quot;8&amp;quot;&amp;gt;Established&amp;lt;/state&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Datapath verification can be done from either side, in this case I&amp;rsquo;m showing a ping from MX80&amp;rsquo;s global VRF towards one of the OpenStack VMs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;admin@MX80&amp;gt; ping 10.0.100.164 count 2 
PING 10.0.100.164 (10.0.100.164): 56 data bytes
64 bytes from 10.0.100.164: icmp_seq=0 ttl=64 time=3.836 ms
64 bytes from 10.0.100.164: icmp_seq=1 ttl=64 time=3.907 ms

--- 10.0.100.164 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max/stddev = 3.836/3.872/3.907/0.035 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;bgp-as-a-service-1&#34;&gt;BGP-as-a-Service&lt;/h1&gt;

&lt;p&gt;To keep things simple I will not use multiple dot1Q interfaces and setup a BGP peering with CumulusVX over a normal, non-trunk interface. From CumulusVX I will inject a loopback IP &lt;code&gt;1.1.1.1/32&lt;/code&gt; into the &lt;code&gt;irb-net&lt;/code&gt; network. Since REST API python library I&amp;rsquo;ve used above is two major releases behind the current version of OpenContrail, it cannot be used to setup BGPaaS feature. Instead I will demonstrate how to use REST API directly from the command line of all-in-one VM using cURL.&lt;/p&gt;

&lt;h2 id=&#34;configuration-1&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;In order to start working with OpenContrail&amp;rsquo;s API, I first need to obtain an authentication token from OpenStack&amp;rsquo;s keystone. With that token I can now query the list of IPs assigned to all OpenStack instances and pick the one assigned to CumulusVX. I need the UUID of that particular IP address in order to extract the ID of the VM interface this IP is assigned to.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;source /etc/kolla/admin-openrc.sh 
TOKEN=$(openstack token issue -f value -c id)
CONTRAIL_AUTH=&amp;quot;X-AUTH-TOKEN: $TOKEN&amp;quot;
CTYPE=&amp;quot;Content-Type: application/json; charset=UTF-8&amp;quot;
curl -H &amp;quot;$CONTRAIL_AUTH&amp;quot; http://10.0.100.140:8082/instance-ips | jq
VMI_ID=$(curl -H &amp;quot;$CONTRAIL_AUTH&amp;quot; http://10.0.100.140:8082/instance-ip/2e7987be-3f53-4296-905a-0c64793307a9 | \
         jq &#39;.[&amp;quot;instance-ip&amp;quot;] .virtual_machine_interface_refs[0].uuid&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With VM interface ID saved in a &lt;code&gt;VMI_ID&lt;/code&gt; variable I can create a BGPaaS service and link it to that particular VM interface.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./bgpaas.json
{
    &amp;quot;bgp-as-a-service&amp;quot;:
    {
        &amp;quot;fq_name&amp;quot;: [&amp;quot;default-domain&amp;quot;, &amp;quot;admin&amp;quot;, &amp;quot;cumulusVX-bgp&amp;quot; ],
        &amp;quot;autonomous_system&amp;quot;: 321,
        &amp;quot;bgpaas_session_attributes&amp;quot;: {
            &amp;quot;address_families&amp;quot;: {&amp;quot;family&amp;quot;: [&amp;quot;inet&amp;quot;] }
            },
        &amp;quot;parent_type&amp;quot;: &amp;quot;project&amp;quot;,
        &amp;quot;virtual_machine_interface_refs&amp;quot;: [{
            &amp;quot;attr&amp;quot;: null,
            &amp;quot;to&amp;quot;: [&amp;quot;default-domain&amp;quot;, &amp;quot;admin&amp;quot;, ${VMI_ID}]
            }],
        &amp;quot;bgpaas-shared&amp;quot;: false,
        &amp;quot;bgpaas-ip-address&amp;quot;: &amp;quot;10.0.100.164&amp;quot;
    }
}
EOF

curl -X POST -H &amp;quot;$CONTRAIL_AUTH&amp;quot; -H &amp;quot;$CTYPE&amp;quot; -d @bgpaas.json http://10.0.100.140:8082/bgp-as-a-services
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final step is setting up a BGP peering on the CumulusVX side. CumulusVX configuration is very simple and self-explanatory. The BGP neighbor IP is the IP of virtual network&amp;rsquo;s default gateway located on local vRouter.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;!
interface lo
 ip address 1.1.1.1/32
!
router bgp 321
 neighbor 10.0.100.161 remote-as 64512
 !
 address-family ipv4 unicast
  network 1.1.1.1/32
 exit-address-family
!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;verification-1&#34;&gt;Verification&lt;/h2&gt;

&lt;p&gt;Here&amp;rsquo;s where we come across another limitation of EVPN. The loopback prefix &lt;code&gt;1.1.1.1/32&lt;/code&gt; does not get injected into EVPN address family, however it does show up automatically in the VPNv4 address family which can be verified from the MX80:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;admin@MX80&amp;gt; show route table bgp.l3vpn.0 hidden 1.1.1.1/32 extensive    

bgp.l3vpn.0: 6 destinations, 6 routes (3 active, 0 holddown, 3 hidden)
10.0.100.140:2:1.1.1.1/32 (1 entry, 0 announced)
         BGP    Preference: 170/-101
                Route Distinguisher: 10.0.100.140:2
                Next hop type: Unusable, Next hop index: 0
                Next-hop reference count: 6
                State: &amp;lt;Hidden Ext ProtectionPath ProtectionCand&amp;gt;
                Local AS: 65411 Peer AS: 64512
                Age: 37:44 
                Validation State: unverified 
                Task: BGP_64512.10.0.100.140
                AS path: 64512 321 I
                Communities: target:200:200 target:64512:8000003 encapsulation:unknown(0x2) encapsulation:mpls-in-udp(0xd) unknown type 8004 value fc00:7a1201 unknown type 8071 value fc00:1389
                Import Accepted
                VPN Label: 31
                Localpref: 100
                Router ID: 10.0.100.140
                Secondary Tables: EVPN-L3-IRB.inet.0
                Indirect next hops: 1
                        Protocol next hop: 10.0.100.140
                        Label operation: Push 31
                        Label TTL action: prop-ttl
                        Load balance label: Label 31: None; 
                        Indirect next hop: 0x0 - INH Session ID: 0x0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s hidden since I haven&amp;rsquo;t configured MPLSoUDP &lt;a href=&#34;https://www.juniper.net/documentation/en_US/junos/topics/example/example-next-hop-based-dynamic-mpls-udp-tunnel-configuring.html&#34; target=&#34;_blank&#34;&gt;dynamic tunnels&lt;/a&gt; on MX80. However this proves that the prefix does get injected into customer VPNs and become available on all devices with the matching import route-target communities.&lt;/p&gt;

&lt;h1 id=&#34;outro&#34;&gt;Outro&lt;/h1&gt;

&lt;p&gt;This post concludes Series 2 of my OpenStack SDN saga. I&amp;rsquo;ve covered quite an extensive range of topics in my two-part series, however, OpenStack networking landscape is so big, it&amp;rsquo;s simply impossible to cover everything I find interesting. I started writing about OpenStack SDN when I first learned I got a job with Nokia. Back then I knew little about VMware NSX and even less about OpenStack. That&amp;rsquo;s why I started researching topics that I found interesting and branching out into adjacent areas as I went along. Almost 2 years later, looking back I can say I&amp;rsquo;ve learned a lot about the internals of SDN in general and hopefully so have my readers. Now I&amp;rsquo;m leaving Nokia to rediscover my networking roots at Arista. I&amp;rsquo;ll dive into DC networking from a different perspective now and it may be awhile before I accumulate a critical mass of interesting material to start spilling it out in my blog again. I still may come back to OpenStack some day but for now I&amp;rsquo;m gonna take a little break, maybe do some house keeping (e.g. move my blog from Jekyll to &lt;a href=&#34;https://gohugo.io/&#34; target=&#34;_blank&#34;&gt;Hugo&lt;/a&gt;, add TLS support) and enjoy my time being a farther.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN - OpenDaylight With BGP VPN</title>
      <link>https://networkop.co.uk/blog/2017/12/15/os-odl-netvirt/</link>
      <pubDate>Fri, 15 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/12/15/os-odl-netvirt/</guid>
      <description>

&lt;p&gt;For the last 5 years OpenStack has been the training ground for a lot of emerging DC SDN solutions. OpenStack integration use case was one of the most compelling and easiest to implement thanks to the limited and suboptimal implementation of the native networking stack. Today, in 2017, features like &lt;a href=&#34;https://networkop.co.uk/blog/2016/05/06/neutron-l2pop/&#34;&gt;L2 population&lt;/a&gt;, local &lt;a href=&#34;https://networkop.co.uk/blog/2016/05/06/neutron-l2pop/&#34;&gt;ARP responder&lt;/a&gt;, &lt;a href=&#34;https://networkop.co.uk/blog/2016/05/21/neutron-l2gw/&#34;&gt;L2 gateway integration&lt;/a&gt;, &lt;a href=&#34;https://networkop.co.uk/blog/2016/10/13/os-dvr/&#34;&gt;distributed routing&lt;/a&gt; and &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/15/os-sfc-skydive/&#34;&gt;service function chaining&lt;/a&gt; have all become available in vanilla OpenStack and don&amp;rsquo;t require a proprietary SDN controller anymore. Admittedly, some of the features are still not (and may never be) implemented in the most optimal way (e.g. DVR). This is where new opensource SDN controllers, the likes of &lt;a href=&#34;https://networkop.co.uk/blog/2016/12/10/ovn-part2/&#34;&gt;OVN&lt;/a&gt; and &lt;a href=&#34;https://docs.openstack.org/developer/dragonflow/distributed_dragonflow.html&#34; target=&#34;_blank&#34;&gt;Dragonflow&lt;/a&gt;, step in to provide scalable, elegant and efficient implementation of these advanced networking features. However one major feature still remains outside of the scope of a lot of these new opensource SDN projects, and that is data centre gateway (DC-GW) integration. Let me start by explain why you would need this feature in the first place.&lt;/p&gt;

&lt;h1 id=&#34;optimal-forwarding-of-north-south-traffic&#34;&gt;Optimal forwarding of North-South traffic&lt;/h1&gt;

&lt;p&gt;OpenStack Neutron and VMware NSX, both being pure software solutions, rely on a special type of node to forward traffic between VMs and hosts outside of the data centre. This node acts as a L2/L3 gateway for all North-South traffic and is often implemented as either a VM or a network namespace. This kind of solution gives software developers greater independence from the underlying networking infrastructure which makes it easier for them to innovate and introduce new features.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sdn-ns.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;However, from the traffic forwarding point of view, having a gateway/network node is not a good solution at all. There is no technological reason for a packet to have to go through this node when after all it ends up on a DC-GW anyway. In fact, this solution introduces additional complexity which needs to be properly managed (e.g. designed, configured and troubleshooted) and a potential bottleneck for high-throughput traffic flows.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s clear that the most optimal way to forward traffic is directly from a compute node to a DC-GW. The only question is how can this optimal forwarding be achieved? SDN controller needs to be able to exchange reachability information with DC-GW using a common protocol understood by most of the existing routing stacks. One such protocol, becoming very common in DC environments, is BGP, which has two address families we can potentially use:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;VPNv4/6 will allow routes to be exchanged and the dataplance to use MPLSoGRE encapsulation. This should be considered a &amp;ldquo;legacy&amp;rdquo; approach since for a very long time DC-GWs did not have the VXLAN ecap/decap capabilities.&lt;/li&gt;
&lt;li&gt;EVPN with VXLAN-based overlays. EVPN makes it possible to exchange both L2 and L3 information under the same AF, which means we have the flexibility of doing not only a L3 WAN integration, but also a L2 data centre interconnect with just a single protocol.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In OpenStack specifically, BGPVPN project was created to provide a pluggable driver framework for 3rd party BGP implementations. Apart from a reference BaGPipe driver (BaGPipe is an ExaBGP fork with lightweight implementation of BGP VPNs), which relies on a default &lt;code&gt;openvswitch&lt;/code&gt; ML2 mechanism driver, only Nuage, OpenDaylight and OpenContrail have contributed their drivers to this project. In this post I will focus on OpenDaylight and show how to install containerised OpenStack with OpenDaylight and integrate it with Cisco CSR using EVPN.&lt;/p&gt;

&lt;h1 id=&#34;opendaylight-integration-with-openstack&#34;&gt;OpenDaylight integration with OpenStack&lt;/h1&gt;

&lt;p&gt;Historically, OpenDaylight has had multiple projects implementing custom OpenStack networking drivers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VTN&lt;/strong&gt; (Virtual Tenant Networking) - spearheaded by NEC was the first project to provide OpenStack networking implementation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GBP&lt;/strong&gt; (Group Based Policy) - a project led by Cisco, one of the first (if not THE first) commercial implementation of Intent-based networking&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NetVirt&lt;/strong&gt; - currently a default Neutron plugin from ODL, developed jointly by Brocade (RIP), RedHat, Ericsson, Intel and many others.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NetVirt provides several common Neutron services including L2 and L3 forwarding, ACL and NAT, as well as advanced services like L2 gateway, QoS and SFC. To do that it assumes full control over an OVS switch inside each compute node and implements the above services inside a single &lt;code&gt;br-int&lt;/code&gt; OVS bridge. L2/L3 forwarding tables are built based on tenant IP/MAC addresses that have been allocated by Neutron and the current network topology. For high-level overview of NetVirt&amp;rsquo;s forwarding pipeline you can refer to &lt;a href=&#34;https://docs.google.com/presentation/d/15h4ZjPxblI5Pz9VWIYnzfyRcQrXYxA1uUoqJsgA53KM/edit#slide=id.g1c73ae9953_2_0&#34; target=&#34;_blank&#34;&gt;this document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It helps to think of an ODL-managed OpenStack as a big chassis switch. NetVirt plays the role of a supervisor by managing control plane and compiling RIB based on the information received from Neutron. Each compute node running an OVS is a linecard with VMs connected to its ports. Unlike the distributed architecture of &lt;a href=&#34;https://networkop.co.uk/blog/2016/12/10/ovn-part2/&#34;&gt;OVN&lt;/a&gt; and Dragonflow, compute nodes do not contain any control plane elements and each OVS gets its FIB programmed directly by the supervisor. DC underlay is a backplane, interconnecting all linecards and a supervisor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/odl-netvirt-chassis.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;opendaylight-bgp-vpn-service-architecture&#34;&gt;OpenDaylight BGP VPN service architecture&lt;/h1&gt;

&lt;p&gt;In order to provide BGP VPN functionality, NetVirt employs the use of three service components:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FIB service&lt;/strong&gt; - maintains L2/L3 forwarding tables and reacts to topology changes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BGP manager&lt;/strong&gt; - provides a translation of information sent to and received from an external BGP stack (Quagga BGP)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VPN Manager&lt;/strong&gt; - ties together the above two components, creates VRFs and keeps track of RD/RT values&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to exchange BGP updates with external DC-GW, NetVirt requires a BGP stack with EVPN and VPNV4/6 capabilities. Ideally, internal ODL BGP stack could have been used for that, however it didn&amp;rsquo;t meet all the performance requirements (injecting/withdrawing thousand of prefixes at the same time). Instead, an external &lt;a href=&#34;https://github.com/6WIND/quagga/tree/qthrift_mpbgp_evpn&#34; target=&#34;_blank&#34;&gt;Quagga fork&lt;/a&gt; with EVPN add-ons is connected to BGP manager via a high-speed Apache Thrift interface. This interface defines the &lt;a href=&#34;https://github.com/6WIND/quagga/blob/qthrift_mpbgp_evpn/qthriftd/vpnservice.thrift&#34; target=&#34;_blank&#34;&gt;format&lt;/a&gt; of data to be exchanged between Quagga (a.k.a QBGP) and NetVirt&amp;rsquo;s BGP Manager in order to do two things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Configure BGP settings like ASN and BGP neighbors&lt;/li&gt;
&lt;li&gt;Read/Write RIB entries inside QBGP&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;BGP session is established between QBGP and external DC-GW, however next-hop values installed by NetVirt and advertised by QBGP have IPs of the respective compute nodes, so that traffic is sent directly via the most optimal path.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/odl-netvirt.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;demo&#34;&gt;Demo&lt;/h1&gt;

&lt;p&gt;Enough of the theory, let&amp;rsquo;s have a look at how to configure a L3VPN between QBGP (advertising ODL&amp;rsquo;s distributed router subnets) and IOS-XE DC-GW using EVPN route type 5 or, more specifically, &lt;a href=&#34;https://tools.ietf.org/html/draft-ietf-bess-evpn-prefix-advertisement-09#section-4.4.1&#34; target=&#34;_blank&#34;&gt;Interface-less IP-VRF-to-IP-VRF model&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/odl-evpn-topo.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;My lab environment is still based on a pair of nested VMs running containerised Kolla OpenStack I&amp;rsquo;ve described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/08/os-lab-docker/&#34;&gt;earlier post&lt;/a&gt;. A few months ago OpenDaylight role has been added to kolla-ansible so now it is possible to install OpenDaylight-intergrated OpenStack automatically. However, there is no option to install QBGP so I had to augment the default &lt;a href=&#34;https://github.com/openstack/kolla&#34; target=&#34;_blank&#34;&gt;Kolla&lt;/a&gt; and &lt;a href=&#34;https://github.com/openstack/kolla-ansible&#34; target=&#34;_blank&#34;&gt;Kolla-ansible&lt;/a&gt; repositories to include the QBGP &lt;a href=&#34;https://github.com/networkop/kolla-odl-bgpvpn/blob/master/roles/kolla_build/templates/quagga-Dockerfile.j2&#34; target=&#34;_blank&#34;&gt;Dockerfile template&lt;/a&gt; and QBGP &lt;a href=&#34;https://github.com/networkop/kolla-odl-bgpvpn/blob/master/roles/kolla_deploy/tasks/create.yml#L90-L120&#34; target=&#34;_blank&#34;&gt;ansible role&lt;/a&gt;. So the first step is to download my latest automated installer and make sure &lt;code&gt;enable_opendaylight&lt;/code&gt; global variable is set to &lt;code&gt;yes&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/networkop/kolla-odl-bgpvpn.git &amp;amp;&amp;amp; cd kolla-odl-bgpvpn
mkdir group_vars
echo &amp;quot;enable_opendaylight: \&amp;quot;yes\&amp;quot;&amp;quot; &amp;gt;&amp;gt; group_vars/all.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At the time of writing I was relying on a couple of latest bug fixes inside OpenDaylight, so I had to modify the default ODL role to install the latest master-branch ODL build. Make sure the link below is pointing to the latest &lt;code&gt;zip&lt;/code&gt; file in &lt;code&gt;0.8.0-SNAPSHOT&lt;/code&gt; directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt;&amp;gt; group_vars/all.yaml
odl_latest_enabled: true
odl_latest_url: https://nexus.opendaylight.org/content/repositories/opendaylight.snapshot/org/opendaylight/integration/netvirt/karaf/0.8.0-SNAPSHOT/karaf-0.8.0-20171106.102232-1767.zip
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next few steps are similar to what I&amp;rsquo;ve described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/08/os-lab-docker/&#34;&gt;Kolla lab post&lt;/a&gt;, will create a pair of VMs, build all Kolla containers, push them to a local Docker repo and finally deploy OpenStack using Kolla-ansible playbooks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./1-create.sh do
./2-bootstrap.sh do
./3-build.sh do 
./4-deploy.sh do
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final &lt;code&gt;4-deploy.sh&lt;/code&gt; script will also create a simple &lt;code&gt;init.sh&lt;/code&gt; script inside the controller VM that can be used to setup a test topology with a single VM connected to a &lt;code&gt;10.0.0.0/24&lt;/code&gt; subnet:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh kolla-controller
source /etc/kolla/admin-openrc.sh
./init.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Of course, another option to build a lab is to follow the official &lt;a href=&#34;https://docs.openstack.org/kolla-ansible/latest/user/quickstart.html&#34; target=&#34;_blank&#34;&gt;Kolla documentation&lt;/a&gt; to create your own custom test environment.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;Assuming the test topology was setup with no issues and a test VM can ping its default gateway &lt;code&gt;10.0.0.1&lt;/code&gt;, we can start configuring BGP VPNs. Unfortunately, we won&amp;rsquo;t be able to use OpenStack BGPVPN API/CLI, since ODL requires an extra parameter (L3 VNI for symmetric IRB) which is not available in OpenStack BGPVPN API, but we still can configure everything directly through ODL&amp;rsquo;s API. My interface of choice is always REST, since it&amp;rsquo;s easier to build it into a fully programmatic plugin, so even though all of the below steps can be accomplished through karaf console CLI, I&amp;rsquo;ll be using cURL to send and retrieve data from ODL&amp;rsquo;s REST API.&lt;/p&gt;

&lt;h3 id=&#34;1-source-admin-credentials-and-setup-odl-s-rest-variables&#34;&gt;1. Source admin credentials and setup ODL&amp;rsquo;s REST variables&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;source /etc/kolla/admin-openrc.sh
export ODL_URL=&#39;http://192.168.133.100:8181/restconf&#39;
export CT_JSON=&amp;quot;Content-Type: application/json&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-configure-local-bgp-settings-and-bgp-peering-with-dc-gw&#34;&gt;2. Configure local BGP settings and BGP peering with DC-GW&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./bgp-full.json
{
    &amp;quot;bgp&amp;quot;: {
        &amp;quot;as-id&amp;quot;: {
            &amp;quot;announce-fbit&amp;quot;: false,
            &amp;quot;local-as&amp;quot;: 100,
            &amp;quot;router-id&amp;quot;: &amp;quot;192.168.133.100&amp;quot;,
            &amp;quot;stalepath-time&amp;quot;: 0
        },
        &amp;quot;logging&amp;quot;: {
            &amp;quot;file&amp;quot;: &amp;quot;/var/log/bgp_debug.log&amp;quot;,
            &amp;quot;level&amp;quot;: &amp;quot;errors&amp;quot;
        },
        &amp;quot;neighbors&amp;quot;: [
            {
                &amp;quot;address&amp;quot;: &amp;quot;192.168.133.50&amp;quot;,
                &amp;quot;remote-as&amp;quot;: 100,
                &amp;quot;address-families&amp;quot;: [
                   {
                     &amp;quot;ebgp:afi&amp;quot;: &amp;quot;3&amp;quot;,
                     &amp;quot;ebgp:peer-ip&amp;quot;: &amp;quot;192.168.133.50&amp;quot;,
                     &amp;quot;ebgp:safi&amp;quot;: &amp;quot;6&amp;quot;
                   }
                ]
            }
        ]
    }
}
EOF

curl -X PUT -u admin:admin -k -v -H &amp;quot;$CT_JSON&amp;quot;  \
     $ODL_URL/config/ebgp:bgp -d @bgp-full.json
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-define-l3vpn-instance-and-associate-it-with-openstack-admin-tenant&#34;&gt;3. Define L3VPN instance and associate it with OpenStack &lt;code&gt;admin&lt;/code&gt; tenant&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TENANT_UUID=$(openstack project show admin -f value -c id | \
            sed &#39;s/\(........\)\(....\)\(....\)\(....\)\(.*\)/\1-\2-\3-\4-\5/&#39;)

cat &amp;lt;&amp;lt; EOF &amp;gt; ./l3vpn-full.json
{
   &amp;quot;input&amp;quot;: {
      &amp;quot;l3vpn&amp;quot;:[
         {
            &amp;quot;id&amp;quot;:&amp;quot;f503fcb0-3fd9-4dee-8c3a-5034cf707fd9&amp;quot;,
            &amp;quot;name&amp;quot;:&amp;quot;L3EVPN&amp;quot;,
            &amp;quot;route-distinguisher&amp;quot;: [&amp;quot;100:100&amp;quot;],
            &amp;quot;export-RT&amp;quot;: [&amp;quot;100:100&amp;quot;],
            &amp;quot;import-RT&amp;quot;: [&amp;quot;100:100&amp;quot;],
            &amp;quot;l3vni&amp;quot;: &amp;quot;5000&amp;quot;,
            &amp;quot;tenant-id&amp;quot;:&amp;quot;${TENANT_UUID}&amp;quot;
         }
      ]
   }
}
EOF

curl -X POST -u admin:admin -k -v -H &amp;quot;$CT_JSON&amp;quot;  \
      $ODL_URL/operations/neutronvpn:createL3VPN -d @l3vpn-full.json
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-inject-prefixes-into-l3vpn-by-associating-the-previously-created-l3vpn-with-a-demo-router&#34;&gt;4. Inject prefixes into L3VPN by associating the previously created L3VPN with a &lt;code&gt;demo-router&lt;/code&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ROUTER_UUID=$(openstack router show demo-router -f value -c id)

cat &amp;lt;&amp;lt; EOF &amp;gt; ./l3vpn-assoc.json
{
  &amp;quot;input&amp;quot;:{
     &amp;quot;vpn-id&amp;quot;:&amp;quot;f503fcb0-3fd9-4dee-8c3a-5034cf707fd9&amp;quot;,
     &amp;quot;router-id&amp;quot;:[ &amp;quot;${ROUTER_UUID}&amp;quot; ]
   }
}
EOF

curl -X POST -u admin:admin -k -v -H &amp;quot;$CT_JSON&amp;quot;  \
     $ODL_URL/operations/neutronvpn:associateRouter -d @l3vpn-assoc.json
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-configure-dc-gw-vtep-ip&#34;&gt;5. Configure DC-GW VTEP IP&lt;/h3&gt;

&lt;p&gt;ODL cannot automatically extract VTEP IP from updates received from DC-GW, so we need to explicitly configure it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./tep.json
{
  &amp;quot;input&amp;quot;: {
    &amp;quot;destination-ip&amp;quot;: &amp;quot;1.1.1.1&amp;quot;,
    &amp;quot;tunnel-type&amp;quot;: &amp;quot;odl-interface:tunnel-type-vxlan&amp;quot;
  }
}
EOF
curl -X POST -u admin:admin -k -v -H &amp;quot;$CT_JSON&amp;quot;  \
     $ODL_URL/operations/itm-rpc:add-external-tunnel-endpoint -d @tep.json
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;6-dc-gw-configuration&#34;&gt;6. DC-GW configuration&lt;/h3&gt;

&lt;p&gt;That is all what needs to be configured on ODL. Although I would consider this to be outside of the scope of the current post, for the sake of completeness I&amp;rsquo;m including the relevant configuration from the DC-GW:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;!
vrf definition ODL
 rd 100:100
 route-target export 100:100
 route-target import 100:100
 !        
 address-family ipv4
  route-target export 100:100 stitching
  route-target import 100:100 stitching
 exit-address-family
!
bridge-domain 5000 
 member vni 5000
!
interface Loopback0
 ip address 1.1.1.1 255.255.255.255
!
interface GigabitEthernet1
 ip address 192.168.133.50 255.255.255.0
!
interface nve1
 no ip address
 source-interface Loopback0
 host-reachability protocol bgp
 member vni 5000 vrf ODL
!
interface BDI5000
 vrf forwarding ODL
 ip address 8.8.8.8 255.255.255.0
 encapsulation dot1Q 500
!
router bgp 100
 bgp log-neighbor-changes
 no bgp default ipv4-unicast
 neighbor 192.168.133.100 remote-as 100
 !
 address-family l2vpn evpn
  import vpnv4 unicast
  neighbor 192.168.133.100 activate
 exit-address-family
 !
 address-family ipv4 vrf ODL
  advertise l2vpn evpn
  redistribute connected
 exit-address-family
!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For detailed explanation of how EVPN RT5 is configured on Cisco CSR refer to the &lt;a href=&#34;https://www.cisco.com/c/en/us/td/docs/ios-xml/ios/cether/configuration/xe-16/ce-xe-16-book/evpn-vxlan-l3.html&#34; target=&#34;_blank&#34;&gt;following guide&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;verification&#34;&gt;Verification&lt;/h2&gt;

&lt;p&gt;There are several things that can be checked to verify that the DC-GW integration is working. One of the first steps would be to check if BGP session with CSR is up.
This can be done from the CSR side, however it&amp;rsquo;s also possible to check this from the QBGP side. First we need to get into the QBGP&amp;rsquo;s interactive shell from the controller node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[centos@controller-1 ~]$ sudo docker exec -it quagga /opt/quagga/bin/vtysh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From here, we can check that the BGP session has been established:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;controller-1# sh bgp neighbors 192.168.133.50     
BGP neighbor is 192.168.133.50, remote AS 100, local AS 100, internal link
  BGP version 4, remote router ID 1.1.1.1
  BGP state = Established, up for 00:03:05
&amp;lt;snip&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can also check the contents of EVPN RIB compiled by QBGP&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;controller-1# sh bgp evpn rd 100:100
BGP table version is 0, local router ID is 192.168.133.100
Status codes: s suppressed, d damped, h history, * valid, &amp;gt; best, i - internal
Origin codes: i - IGP, e - EGP, ? - incomplete

   Network          Next Hop            Metric LocPrf Weight Path
Route Distinguisher: as2 100:100
*&amp;gt; [0][fa:16:3e:37:42:d8/48][10.0.0.2/32]
                    192.168.133.100         0          32768 i
*&amp;gt; [0][fa:16:3e:dc:77:65/48][10.0.0.3/32]
                    192.168.133.101         0          32768 i
*&amp;gt;i8.8.8.0/24       1.1.1.1         0     100       0 ?
*&amp;gt; 10.0.0.0/24      192.168.133.100         0          32768 i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we can verify that the prefix &lt;code&gt;8.8.8.0/24&lt;/code&gt; advertised from DC-GW is being passed by QBGP and accepted by NetVirt&amp;rsquo;s FIB Manager:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -u admin:admin -k -v  $ODL_URL/config/odl-fib:fibEntries/\
  vrfTables/100%3A100/vrfEntry/8.8.8.0%2F24 | python -m json.tool
{
    &amp;quot;vrfEntry&amp;quot;: [
        {
            &amp;quot;destPrefix&amp;quot;: &amp;quot;8.8.8.0/24&amp;quot;,
            &amp;quot;encap-type&amp;quot;: &amp;quot;vxlan&amp;quot;,
            &amp;quot;gateway_mac_address&amp;quot;: &amp;quot;00:1e:49:69:24:bf&amp;quot;,
            &amp;quot;l3vni&amp;quot;: 5000,
            &amp;quot;origin&amp;quot;: &amp;quot;b&amp;quot;,
            &amp;quot;route-paths&amp;quot;: [
                {
                    &amp;quot;nexthop-address&amp;quot;: &amp;quot;1.1.1.1&amp;quot;
                }
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last output confirms that the prefix is being received and accepted by ODL. To do a similar check on CSR side we can run the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CSR1k#show bgp l2vpn evpn 
&amp;lt;snip&amp;gt;
     Network          Next Hop            Metric LocPrf Weight Path
Route Distinguisher: 100:100 (default for vrf ODL)
 *&amp;gt;i  [2][100:100][0][48][FA163E3742D8][32][10.0.0.2]/24
                      192.168.133.100          0    100      0 i
 *&amp;gt;i  [2][100:100][0][48][FA163EDC7765][32][10.0.0.3]/24
                      192.168.133.101          0    100      0 i
 *&amp;gt;   [5][100:100][0][24][8.8.8.0]/17
                      0.0.0.0                  0         32768 ?
 *&amp;gt;i  [5][100:100][0][24][10.0.0.0]/17
                      192.168.133.100          0    100      0 i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This confirms that the control plane information has been successfully exchanged between NetVirt and Cisco CSR.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;At the time of writing, there was an &lt;a href=&#34;https://git.opendaylight.org/gerrit/#/c/63324/&#34; target=&#34;_blank&#34;&gt;open bug&lt;/a&gt; in ODL master branch that prevented the forwarding entries from being installed in OVS datapath. Once the bug is fixed I will update this post with the dataplance verification, a.k.a ping&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;OpenDaylight is a pretty advanced OpenStack SDN platform. Its functionality includes clustering, site-to-site federation (without EVPN) and L2/L3 EVPN DC-GW integration for both IPv4 and IPv6. It is yet another example of how an open-source platform can match even the most advanced proprietary SDN solutions from incumbent vendors. This is all thanks to the companies involved in OpenDaylight development. I also want to say special thanks to Vyshakh Krishnan, Kiran N Upadhyaya and Dayavanti Gopal Kamath from Ericsson for helping me clear up some of the questions I posted on netvirt-dev mailing list.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN - NFV Management and Orchestration</title>
      <link>https://networkop.co.uk/blog/2017/11/23/os-nfv-mano/</link>
      <pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/11/23/os-nfv-mano/</guid>
      <description>

&lt;p&gt;In the ongoing hysteria surrounding all things SDN, one important thing gets often overlooked. You don&amp;rsquo;t build SDN for its own sake. SDN is just a little cog in a big machine called &amp;ldquo;cloud&amp;rdquo;. To take it even further, I would argue that the best SDN solution is the one that you don&amp;rsquo;t know even exists. Despite what the big vendors tell you, operators are not supposed to interact with SDN interface, be it GUI or CLI. If you dig up some of the earliest presentation about Cisco ACI, when the people talking about it were the actual people who designed the product, you&amp;rsquo;ll notice one common motif being repeated over and over again. That is that ACI was never designed for direct human interaction, but rather was supposed to be configured by a higher level orchestrating system. In data center environments such orchestrating system may glue together services of virtualization layer and SDN layer to provide a seamless &amp;ldquo;cloud&amp;rdquo; experience to the end users. The focus of this post will be one incarnation of such orchestration system, specific to SP/Telco world, commonly known as NFV MANO.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;nfv-mano-for-telco-sdn&#34;&gt;NFV MANO for Telco SDN&lt;/h1&gt;

&lt;p&gt;At the early dawn of SDN/NFV era a lot of people got very excited by &lt;strong&gt;&amp;ldquo;the promise&amp;rdquo;&lt;/strong&gt; and started applying the disaggregation and virtualization paradigms to all areas of networking. For Telcos that meant virtualizing network functions that built the service core of their networks - EPC, IMS, RAN. Traditionally those network functions were a collection of vertically-integrated baremetal appliances that took a long time to commission and had to be overprovisioned to cope with the peak-hour demand. Virtualizing them would have made it possible to achieve quicker time-to-market, elasticity to cope with a changing network demand and hardware/software disaggregation.&lt;/p&gt;

&lt;p&gt;As expected however, such fundamental change has to come at price. Not only do Telcos get a new virtualization platform to manage but they also need to worry about lifecycle management and end-to-end orchestration (MANO) of VNFs. Since any such change presents an opportunity for new streams of revenue, it didn&amp;rsquo;t take long for vendors to jump on the bandwagon and start working on a new architecture designed to address those issues.&lt;/p&gt;

&lt;p&gt;The first problem was the easiest to solve since VMware and OpenStack already existed at that stage and could be used to host VNFs with very little modifications. The management and orchestration problem, however, was only partially solved by existing orchestration solutions. There were a lot of gaps between the current operational model and the new VNF world and although these problems could have been solved by Telcos engaging themselves with the open-source community, this proved to be too big of a change for them and they&amp;rsquo;ve turned to the only thing they could trust - the standards bodies.&lt;/p&gt;

&lt;h1 id=&#34;etsi-mano&#34;&gt;ETSI MANO&lt;/h1&gt;

&lt;p&gt;ETSI NFV MANO working group has set out to define a reference architecture for management and orchestration of virtualized resources in Telco data centers. The goal of NFV MANO initiative was to do a research into what&amp;rsquo;s required to manage and orchestrate VNFs, what&amp;rsquo;s currently available and identify potential gaps for other standards bodies to fill. Initial ETSI NFV Release 1 (2014) defined a base framework through relatively weak requirements and recommendations and was followed by Release 2 (2016) that made them more concrete by locking down the interfaces and data model specifications. For a very long time Release 1 was the only available NFV MANO standard, which led to a lot of inconsistencies in each vendors&amp;rsquo; implementations of it. This was very frustrating for Telcos since it required a lot of integration effort to build a multi-vendor MANO stack. Another potential issue with ETSI MANO standard is its limited scope - a lot of critical components like OSS and EMS are left outside of it which created a lot of confusion for Telcos and resulted in other standardisation efforts addressing those gaps.&lt;/p&gt;

&lt;p&gt;On the below diagram I have shown an adbridged version of the original ETSI MANO &lt;a href=&#34;https://www.ietf.org/proceedings/88/slides/slides-88-opsawg-6.pdf&#34; target=&#34;_blank&#34;&gt;reference architecture diagram&lt;/a&gt; adapted to the use case I&amp;rsquo;ll be demonstrating in this post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/etsi-mano.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This architecture consists of the following building blocks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NFVI&lt;/strong&gt; (NFV Infrastructure) - OpenStacks compute or VMware&amp;rsquo;s ESXI nodes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VIM&lt;/strong&gt; (Virtual Infrastructure Manager) - OpenStack&amp;rsquo;s controller/API or VMware&amp;rsquo;s vCenter nodes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VNFM&lt;/strong&gt; (VNF Manager) - an element responsible for lifecycle management (create,delete,scale) and monitoring of VNFs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NFVO&lt;/strong&gt; (NFV Orchestrator) - an element responsible for lifecyle management of Network Services (described below)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these elements are working together towards a single goal - managing and orchestrating a Network Service (NS), which itself is comprised of multiple VNFs, Virtual Links (VLs), VNF Forwarding Graphs (VNFFGs) and Physical Network Functions (PNFs). In this post I create a NS for a simple virtual IDS use case, described in my previous &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/15/os-sfc-skydive/&#34;&gt;SFC post&lt;/a&gt;. The goal is to steer all ICMP traffic coming from VM1 through a vIDS VNF which will forward the traffic to its original destination.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/vids-created.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Before I get to the implementation, let me give a quick overview of how a Network Service is build from its constituent parts, in the context of our vIDS use case.&lt;/p&gt;

&lt;h1 id=&#34;relationship-between-ns-vnf-and-vnffg&#34;&gt;Relationship between NS, VNF and VNFFG&lt;/h1&gt;

&lt;p&gt;According to ETSI MANO, a &lt;strong&gt;Network Service&lt;/strong&gt; (NS) is a subset of end-to-end service implemented by VNFs and instantiated on the NFVI. As I&amp;rsquo;ve mentioned before, some examples of a NS would be vEPC, vIMS or vCPE. NS can be described in either a YANG or a Tosca template called NS Descriptor (NSD). The main goal of a NSD is to tie together VNFs, VLs, VNFFGs and PNFs by defining relationship between various templates describing those objects (VNFDs, VLDs, VNFFGDs). Once NSD is onboarded (uploaded), it can be instantiated by NFVO, which communicates with VIM and VNFM to create the constituent components and stitch them together as described in a template. NSD normally does not contain VNFD or VNFFGD templates, but imports them through their names, which means that in order to instantiate a NSD, the corresponding VNFDs and VNFFGDs should already be onboarded.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/vids-nsd.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VNF Descriptor&lt;/strong&gt; is a template describing the compute and network parameters of a single VNF. Each VNF consists of one or more VNF components (VNFCs), represented in Tosca as Virtual Deployment Units (VDUs). A VDU is the smallest part of a VNF and can be implemented as either a container or, as it is in our case, a VM. Apart from the usual set of parameters like CPU, RAM and disk, VNFD also describes all the virtual networks required for internal communication between VNFCs, called internal VLs. VNFM can ask VIM to create those networks when the VNF is being instantiated. VNFD also contains a reference to external networks, which are supposed to be created by NFVO. Those networks are used to connect different VNFs together or to connect VNFs to PNFs and other elements outside of NFVI platform. If external VLs are defined in a VNFD, VNFM will need to source them externally, either as input parameters to VNFM or from NFVO. In fact, VNF instantiation by VNFM, as described in Tacker &lt;a href=&#34;https://docs.openstack.org/tacker/latest/user/vnfm_usage_guide.html&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;, is only used for testing purposes and since a VNF only makes sense as a part of a Network Service, the intended way is to use a NSD to instantiate all VNFs in production environment.&lt;/p&gt;

&lt;p&gt;The final component that we&amp;rsquo;re going to use is VNF Forwarding Graph. &lt;strong&gt;VNFFG Descriptor&lt;/strong&gt; is an optional component that describes how different VNFs are supposed to be chained together to form a Network Service. In the absence of VNFFG, VNFs will fall back to the default destination-based forwarding, when the IPs of VNFs forming a NS are either automatically discovered (e.g. through DNS) or provisioned statically. Tacker&amp;rsquo;s implementation of VNFFG is not fully integrated with NSD yet and VNFFGD has to be instantiated separately and, as will be shown below, linked to an already running instance of a Network Service through its ID.&lt;/p&gt;

&lt;h1 id=&#34;using-tacker-to-orchestrate-a-network-service&#34;&gt;Using Tacker to orchestrate a Network Service&lt;/h1&gt;

&lt;p&gt;Tacker is an OpenStack project implementing a generic VNFM and NFVO. At the input it consumes Tosca-based templates, converts them to Heat templates which are then used to spin up VMs on OpenStack. This diagram from Brocade, the biggest Tacker contributor (at least until its acquisition), is the best overview of internal Tacker architecture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/brocade-tacker.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For this demo environment I&amp;rsquo;ll keep using my OpenStack Kolla lab environment described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/08/os-lab-docker/&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;step-1-vim-registration&#34;&gt;Step 1 - VIM registration&lt;/h2&gt;

&lt;p&gt;Before we can start using Tacker, it needs to know how to reach the OpenStack environment, so the first step in the workflow is OpenStack or VIM registration. We need to provide the address of the keystone endpoint along with the admin credentials to give Tacker enough rights to create and delete VMs and SFC objects:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./vim.yaml
auth_url: &#39;http://192.168.133.254:35357/v3&#39;
username: &#39;admin&#39;
password: &#39;admin&#39;
project_name: &#39;admin&#39;
project_domain_name: &#39;Default&#39;
user_domain_name: &#39;Default&#39;
EOF

tacker vim-register --is-default --config-file vim.yaml --description MYVIM KOLLA-OPENSTACK
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The successful result can be checked with &lt;code&gt;tacker vim-list&lt;/code&gt; which should report that registered VIM is now reachable.&lt;/p&gt;

&lt;h2 id=&#34;step-2-onboarding-a-vnfd&#34;&gt;Step 2 - Onboarding a VNFD&lt;/h2&gt;

&lt;p&gt;VNFD defines a set of VMs (VNFCs), network ports (CPs) and networks (VLs) and their relationship. In our case we have a single cirros-based VM with a pair of ingress/egress ports. In this template we also define a special node type &lt;code&gt;tosca.nodes.nfv.vIDS&lt;/code&gt; which will be used by NSD to pass the required parameters for ingress and egress VLs. These parameters are going to be used by VNFD to attach network ports (CPs) to virtual networks (VLs) as defined in the &lt;code&gt;substitution_mappings&lt;/code&gt; section.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./vnfd.yaml
tosca_definitions_version: tosca_simple_profile_for_nfv_1_0_0
description = Cirros vIDS example

node_types:
  tosca.nodes.nfv.vIDS:
    requirements:
      - INGRESS_VL:
          type: tosca.nodes.nfv.VL
          required: true
      - EGRESS_VL:
          type: tosca.nodes.nfv.VL
          required: true

topology_template:
  substitution_mappings:
    node_type: tosca.nodes.nfv.vIDS
    requirements:
      INGRESS_VL: [CP1, virtualLink]
      EGRESS_VL:  [CP2, virtualLink]

  node_templates:
    VDU1:
      type: tosca.nodes.nfv.VDU.Tacker
      properties:
        availability_zone: nova
        flavor: m1.nano
        image: cirros
        mgmt_driver: noop
        user_data_format: RAW
        user_data: |
          #!/bin/sh
          sudo cirros-dhcpc up eth1
          sudo ip rule add iif eth0 table default
          sudo ip route add default via 10.0.0.1 dev eth1 table default
          sudo sysctl -w net.ipv4.ip_forward=1

    CP1:
      type: tosca.nodes.nfv.CP.Tacker
      properties:
        anti_spoofing_protection: false
      requirements:
        - virtualBinding:
            node: VDU1

    CP2:
      type: tosca.nodes.nfv.CP.Tacker
      properties:
        anti_spoofing_protection: false
      requirements:
        - virtualBinding:
            node: VDU1
EOF

tacker vnfd-create --vnfd-file vnfd.yaml vIDS-TEMPLATE
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-4-onboarding-a-nsd&#34;&gt;Step 4 - Onboarding a NSD&lt;/h2&gt;

&lt;p&gt;In our use case the NSD template is going to really small. All what we need to define is a single VNF of the &lt;code&gt;tosca.nodes.nfv.vIDS&lt;/code&gt; type that was defined previously in the VNFD. We also define a VL node which points to the pre-existing &lt;code&gt;demo-net&lt;/code&gt; virtual network and pass this VL to both INGRESS_VL and EGRESS_VL parameters of the VNFD.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./nsd.yaml
tosca_definitions_version: tosca_simple_profile_for_nfv_1_0_0
imports:
  - vIDS-TEMPLATE

topology_template:
  node_templates:
    vIDS:
      type: tosca.nodes.nfv.vIDS
      requirements:
        - INGRESS_VL: VL1
        - EGRESS_VL: VL1
    VL1:
      type: tosca.nodes.nfv.VL
      properties:
          network_name: demo-net
          vendor: tacker
EOF

tacker nsd-create --nsd-file nsd.yaml NSD-vIDS-TEMPLATE
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-5-instantiating-a-nsd&#34;&gt;Step 5 - Instantiating a NSD&lt;/h2&gt;

&lt;p&gt;As I&amp;rsquo;ve mentioned before, VNFFG is not integrated with NSD yet, so we&amp;rsquo;ll add it later. For now, we have provided enough information to instantiate our NSD.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tacker ns-create --nsd-name NSD-vIDS-TEMPLATE NS-vIDS-1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This last command creates a cirros-based VM with two interfaces and connects them to &lt;code&gt;demo-net&lt;/code&gt; virtual network. All ICMP traffic from VM1 still goes directly to its default gateway so the last thing we need to do is create a VNFFG.&lt;/p&gt;

&lt;h2 id=&#34;step-6-onboarding-and-instantiating-a-vnffg&#34;&gt;Step 6 - Onboarding and Instantiating a VNFFG&lt;/h2&gt;

&lt;p&gt;VNFFG consists of two two types of nodes. The first type defines a Forwarding Path (FP) as a set of virtual ports (CPs) and a flow classifier to build an equivalent service function chain inside the VIM. The second type groups multiple forwarding paths to build a complex service chain graphs, however only one FP is supported by Tacker at the time of writing.&lt;/p&gt;

&lt;p&gt;The following template demonstrates another important feature - template parametrization. Instead of defining all parameters statically in a template, they can be provided as inputs during instantiation, which allows to keep templates generic. In this case I&amp;rsquo;ve replaced the network port id parameter with &lt;code&gt;PORT_ID&lt;/code&gt; variable which will be provided during VNFFGD instantiation.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; ./vnffg.yaml
tosca_definitions_version: tosca_simple_profile_for_nfv_1_0_0

description = vIDS VNFFG tosca

topology_template:
  inputs:
    PORT_ID:
      type: string
description = Port ID of the target VM

  node_templates:

    Forwarding_Path-1:
      type: tosca.nodes.nfv.FP.Tacker
description = creates path (CP1-&amp;gt;CP2)
      properties:
        id: 51
        policy:
          type: ACL
          criteria:
            - network_src_port_id: { get_input: PORT_ID }
            - ip_proto: 1
        path:
          - forwarder: vIDS-TEMPLATE
            capability: CP1
          - forwarder: vIDS-TEMPLATE
            capability: CP2


  groups:
    VNFFG1:
      type: tosca.groups.nfv.VNFFG
description = Set of Forwarding Paths
      properties:
        vendor: tacker
        version: 1.0
        number_of_endpoints: 1
        dependent_virtual_link: [VL1]
        connection_point: [CP1]
        constituent_vnfs: [vIDS-TEMPLATE]
      members: [Forwarding_Path-1]
EOF

tacker vnffgd-create --vnffgd-file vnffgd.yaml VNFFG-TEMPLATE
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that the VNFFGD has been updated to support multiple flow classifiers which means you many need to update the above template as per the &lt;a href=&#34;https://github.com/openstack/tacker/blob/master/samples/tosca-templates/vnffgd/tosca-vnffgd-multiple-classifiers-sample.yaml&#34; target=&#34;_blank&#34;&gt;sample VNFFGD template&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In order to instantiate a VNFFGD we need to provide two runtime parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;OpenStack port ID of VM1 for forwarding path flow classifier&lt;/li&gt;
&lt;li&gt;ID of the VNF created by the Network Service&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these parameters can be obtained using the CLI commands as shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CLIENT_IP=$(openstack server list | grep VM1 | grep -Eo &#39;[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+&#39;)
PORT_ID=$(openstack port list | grep $CLIENT_IP | awk &#39;{print $2}&#39;)
echo &amp;quot;PORT_ID: $PORT_ID&amp;quot; &amp;gt; params-vnffg.yaml
vIDS_ID=$(tacker ns-show NS-vIDS-1 -f value -c vnf_ids | sed &amp;quot;s/&#39;/\&amp;quot;/g&amp;quot; | jq &#39;.vIDS&#39; | sed &amp;quot;s/\&amp;quot;//g&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command creates a VNFFG and an equivalent SFC to steer all ICMP traffic from VM1 through vIDS VNF. The result can be verified using Skydive following the procedure described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/15/os-sfc-skydive/&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tacker vnffg-create --vnffgd-name VNFFG-TEMPLATE \
                    --vnf-mapping vIDS-TEMPLATE:$vIDS_ID \
                    --param-file params-vnffg.yaml VNFFG-1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;other-tacker-features&#34;&gt;Other Tacker features&lt;/h1&gt;

&lt;p&gt;This post only scratches the surface of what&amp;rsquo;s available in Tacker with a lot of other salient features left out of scope, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;VNF monitoring - through monitoring driver its possible to do VNF monitoring from VNFM using various methods ranging from a single ICMP/HTTP ping to Alarm-based monitoring using OpenStack&amp;rsquo;s &lt;a href=&#34;https://wiki.openstack.org/wiki/Telemetry&#34; target=&#34;_blank&#34;&gt;Telemetry framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Enhanced Placement Awareness - VNFD Tosca template extensions that allow the definition of required performance features like NUMA topology mapping, SR-IOV and CPU pinning.&lt;/li&gt;
&lt;li&gt;Mistral workflows - ability to drive Tacker workflows through Mistral&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Tacker is one of &lt;a href=&#34;https://thenewstack.io/opensource-nfv-part-4-opensource-mano/&#34; target=&#34;_blank&#34;&gt;many&lt;/a&gt; NFV orchestration platforms in a very competitive environment. Other &lt;a href=&#34;https://www.mirantis.com/blog/which-nfv-orchestration-platform-best-review-osm-open-o-cord-cloudify/&#34; target=&#34;_blank&#34;&gt;open-source initiatives&lt;/a&gt; have been created in response to the shortcomings of the original ETSI Release 1 reference architecture. The fact the some of the biggest Telcos have finally realised that the only way to achieve the goal of NFV orchestration is to get involved with open-source and do it themselves, may be a good sign for the industry and maybe not so good for the ETSI NFV MANO working group. Whether ONAP with its broader scope becomes a new de-facto standard for NFV orchestration, still remains to be seen, until then ETSI MANO remains the only viable standard for NFV lifecycle management and orchestration.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN - Skydiving Into Service Function Chaining</title>
      <link>https://networkop.co.uk/blog/2017/09/15/os-sfc-skydive/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/09/15/os-sfc-skydive/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;abbr:Service Function Chaining&#34; target=&#34;_blank&#34;&gt;SFC&lt;/a&gt; is another SDN feature that for a long time only used to be available in proprietary SDN solutions and that has recently become available in vanilla OpenStack. It serves as another proof that proprietary SDN solutions are losing the competitive edge, especially for Telco SDN/NFV use cases. Hopefully, by the end of this series of posts I&amp;rsquo;ll manage do demonstrate how to build a complete open-source solution that has feature parity (in terms of major networking features) with all the major proprietary data centre SDN platforms. But for now, let&amp;rsquo;s just focus on SFC.&lt;/p&gt;

&lt;h1 id=&#34;sfc-high-level-overview&#34;&gt;SFC High-level overview&lt;/h1&gt;

&lt;p&gt;In most general terms, SFC refers to packet forwarding technique that uses more than just destination IP address to decide how to forward packets. In more specific terms, SFC refers to &amp;ldquo;steering&amp;rdquo; of traffic through a specific set of endpoints (a.k.a Service Functions), overriding the default destination-based forwarding. For those coming from a traditional networking background, think of SFC as a set of policy-based routing instances orchestrated from a central element (SDN controller). Typical use cases for SFC would be things like firewalling, IDS/IPS, proxying, NAT&amp;rsquo;ing, monitoring.&lt;/p&gt;

&lt;p&gt;SFC is usually modelled as a directed (acyclic) graph, where the first and the last elements are the source and destination respectively and each vertex inside the graph represents a SF to be chained. IETF RFC7665 defines the reference architecture for SFC implementations and establishes some of the basic terminology. A simplified SFC architecture consists of the following main components:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Classifier - a network element that matches and redirects traffic flows to a chain&lt;/li&gt;
&lt;li&gt;Service Function - an element responsible for packet processing&lt;/li&gt;
&lt;li&gt;Service Function Forwarder - a network element that forwards traffic to and from a directly connected SF&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sfc-overview.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;One important property of a SF is elasticity. More instances of the same type can be added to a pool of SF and SFF will load-balance the traffic between them. This is the reason why, as we&amp;rsquo;ll see in the next section, SFF treats connections to a SF as a group of ports rather than just a single port.&lt;/p&gt;

&lt;h1 id=&#34;insertion-modes-and-implementation-models&#34;&gt;Insertion modes and implementation models&lt;/h1&gt;

&lt;p&gt;In legacy, pre-SDN environments SFs had no idea if they were a part of a service chain and network devices (routers and switches) had to &amp;ldquo;insert&amp;rdquo; the interesting traffic into the service function using one of the following two modes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;L2 mode&lt;/strong&gt; is when SF is physically inserted between the source and destination inside a single broadcast domain, so traffic flows through a SF without any intervention from a switch. Example of this mode could be a firewall in transparent mode, physically connected between a switch and a default gateway router. All packets entering a SF have their original source and destination MAC addresses, which requires SF to be in promiscuous mode.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;L3 mode&lt;/strong&gt; is when a router overrides its default destination-based forwarding and redirects the interesting traffic to a SF. In legacy networks this could have been achieved with PBR or WCCP. In this case SF needs to be L2-attached to a router and all redirected packets have their destination MAC updated to that of a SF&amp;rsquo;s ingress interface.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Modern SDN networks make it really easy to modify forwarding behaviour of network elements, both physical and virtual. There is no need for policy-based routing or bump-in-the-wire designs anymore. When flow needs to be redirected to a SF on a virtual switch, all what&amp;rsquo;s required is a matching OpenFlow entry with a high enough priority. However redirecting traffic to a SF is just one part of the problem. Another part is how to make SFs smarter, to provide greater visibility of end-to-end service function path.&lt;/p&gt;

&lt;p&gt;So far SFs have only been able to extract metadata from the packet itself. This limited the flexibility of SF logic and became computationally expensive in case many SFs need to access some L7 header information. Ideal way would be to have an additional header which can be used to read and write arbitrary information and pass it along the service function chain. RFC7665 defines requirements for &amp;ldquo;SFC Encapsulation&amp;rdquo; header which can be used to uniquely identify an instance of a chain as well as share metadata between all its elements. Neutron API refers to SFC encapsulation as &lt;em&gt;correlation&lt;/em&gt; since its primary function is to identify a particular service function path. There are two implementations of SFC encapsulation in use today:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MPLS&lt;/strong&gt; - used by current OVS agent driver (as of Pike). This method does not provide any means to share metadata and serves only for SFP identification. It is intended as an interim solution until NSH becomes available upstream in OVS.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NSH&lt;/strong&gt; - complete implementation of SFC encapsulation defined in RFC7665. This method is currently implemented in Opendaylight where NSH is used as a shim between VXLAN-GPE and the encapsulated packet&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It should be noted that the new approach with SFC encapsulation still allows for legacy, non-SFC-aware SFs to be chained. In this case SFC encapsulation is stripped off the packet by an &amp;ldquo;SFC proxy&amp;rdquo; before the packet is sent to the ingress port of a service function. All logical elements forming an SFC forwarding pipeline, including SFC proxy, Classifier and Forwarder, are implemented inside the same OVS bridges (br-int and br-tun) used by vanilla OVS-agent driver.&lt;/p&gt;

&lt;h1 id=&#34;configuring-neutron-sfc&#34;&gt;Configuring Neutron SFC&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;ll pick up where we left off in the &lt;a href=&#34;https://networkop.co.uk/blog/2017/09/08/os-lab-docker/&#34;&gt;previous post&lt;/a&gt;. All Neutron and ML2 configuration files have already been updated thanks to the &lt;code&gt;enable_sfc=&amp;quot;yes&amp;quot;&lt;/code&gt; setting in the global Kolla-Ansible configuration file. If not, you can change it in &lt;code&gt;/etc/kolla/globals.yaml&lt;/code&gt; and re-run kolla-ansible deployment script.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s generate OpenStack credentials using a post-deployment script. We later can use a default bootstrap script to downloads the cirros image and set up some basic networking and security rules.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kolla-ansible post-deploy
source /etc/kolla/admin-openrc.sh
/usr/share/kolla-ansible/init-runonce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The goal for this post is to create a simple uni-directional SFC to steer the ICMP requests from VM1 to its default gateway through another VM that will be playing the role of a firewall.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sfc-example.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The network was already created by the bootstrap script so all what we have to do is create a test VM. I&amp;rsquo;m creating a port in a separate step simply so that I can refer to it by name instead of UUID.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack port create --network demo-net P0
openstack server create --image cirros --flavor m1.tiny --port P0 VM1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;rsquo;ll go over all the necessary steps to setup SFC, but will only provide a brief explanation. Refer to the official OpenStack &lt;a href=&#34;https://docs.openstack.org/newton/networking-guide/config-sfc.html&#34; target=&#34;_blank&#34;&gt;Networking Guide&lt;/a&gt; for a complete SFC configuration guide.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s create a FW VM with two ports - P1 and P2.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack port create --network demo-net P1
openstack port create --network demo-net P2
openstack server create --image cirros --flavor m1.tiny --port P1 --port P2 FW
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we need create an ingress/egress port pair and assign it to a port pair group. The default setting for &lt;strong&gt;correlation&lt;/strong&gt; in a port pair (not shown) is &lt;code&gt;none&lt;/code&gt;. That means that SFC encapsulation header (MPLS) will get stripped before the packet is sent to P1.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack sfc port pair create --ingress P1 --egress P2 PPAIR
openstack sfc port pair group create --port-pair PPAIR PPGROUP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Port pair group also allows to specify the L2-L4 headers which to use for load-balancing in OpenFlow groups, overriding the default behaviour described in the next section.&lt;/p&gt;

&lt;p&gt;Another required element is a flow classifier. We will be redirecting ICMP traffic coming from VM1&amp;rsquo;s port P0&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack sfc flow classifier create --protocol icmp --logical-source-port P0 FLOW-ICMP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we can tie together flow classifier with a previously created port pair group. The default setting for &lt;strong&gt;correlation&lt;/strong&gt; (not shown again) in this case is &lt;code&gt;mpls&lt;/code&gt;. That means that each chain will have its own unique MPLS label to be used as an SFC encapsulation.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack sfc port chain create --port-pair-group PPGROUP --flow-classifier FLOW-ICMP PCHAIN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s all the configuration needed to setup SFC. However if you login VM1&amp;rsquo;s console and try pinging default gateway, it will fail. Next, I&amp;rsquo;m going to give a quick demo of how to use a real-time network analyzer tool called Skydive to troubleshoot this issue.&lt;/p&gt;

&lt;h1 id=&#34;using-skydive-to-troubleshoot-sfc&#34;&gt;Using Skydive to troubleshoot SFC&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://skydive-project.github.io/skydive/&#34; target=&#34;_blank&#34;&gt;Skydive&lt;/a&gt; is a new open-source distributed network probing and traffic analyzing tool. It consists of a set of agents running on compute nodes, collecting topology and flow information and forwarding it to a central element for analysis.&lt;/p&gt;

&lt;p&gt;The idea of using Skydive to analyze and track SFC is not new. In fact, for anyone interested in this topic I highly recommend the &lt;a href=&#34;http://blog.cafarelli.fr/2017/02/tracking-service-function-chaining-with-skydive/&#34; target=&#34;_blank&#34;&gt;following blogpost&lt;/a&gt;. In my case I&amp;rsquo;ll show how to use Skydive from a more practical perspective - troubleshooting multiple SFC issues.&lt;/p&gt;

&lt;p&gt;Skydive CLI client is available inside the &lt;code&gt;skydive_analyzer&lt;/code&gt; container. We need to start an interactive bash session inside this container and set some environment variables:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker exec -it skydive_analyzer bash
export SKYDIVE_ANALYZERS=192.168.133.100:8085
export SKYDIVE_USERNAME=admin
export SKYDIVE_PASSWORD=admin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first thing we can do to troubleshoot is see if ICMP traffic is entering the &lt;code&gt;ingress&lt;/code&gt; port of the FW VM. Based on the output of &lt;code&gt;openstack port list&lt;/code&gt; command I know that P1 has got an IP of &lt;code&gt;10.0.0.8&lt;/code&gt;. Let&amp;rsquo;s if we can identify a tap port corresponding to P1:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;skydive client topology query --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.8&#39;, &#39;Type&#39;, &#39;tun&#39;).Values(&#39;Neutron&#39;)&amp;quot;
{
  &amp;quot;IPs&amp;quot;: &amp;quot;10.0.0.8&amp;quot;,
  &amp;quot;NetworkID&amp;quot;: &amp;quot;8eabb451-b026-417c-b54b-8e79ee6e71c3&amp;quot;,
  &amp;quot;NetworkName&amp;quot;: &amp;quot;demo-net&amp;quot;,
  &amp;quot;PortID&amp;quot;: &amp;quot;e6334df9-a5c4-4e86-a5f3-671760c2bbbe&amp;quot;,
  &amp;quot;TenantID&amp;quot;: &amp;quot;bd5829e0cb5b40b68ab4f8e7dc68b14d&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output above proves that skydive agent has successfully read the configuration of the port and we can start a capture on that object to see any packets arriving on P1.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;skydive client capture create --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.8&#39;, &#39;Type&#39;, &#39;tun&#39;)&amp;quot;
skydive client topology query --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.8&#39;, &#39;Type&#39;, &#39;tun&#39;).Flows().Has(&#39;Application&#39;,&#39;ICMPv4&#39;).Values(&#39;Metric.ABPackets&#39;)&amp;quot;
[
  7
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you &lt;code&gt;watch&lt;/code&gt; the last command for several seconds you should see that the number in brackets is increasing. That means that packets are hitting the ingress port of the FW VM. Now let&amp;rsquo;s repeat the same test on &lt;code&gt;egress&lt;/code&gt; port P2.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;skydive client capture create --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.4&#39;, &#39;Type&#39;, &#39;tun&#39;)&amp;quot;
skydive client topology query --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.4&#39;, &#39;Type&#39;, &#39;tun&#39;).Flows()&amp;quot;
[]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output above tells us that there are no packets coming out of the FW VM. This is expected since we haven&amp;rsquo;t done any changes to the blank cirros image to make it forward the packets between the two interfaces. If we examine the IP configuration of the FW VM, we would see that it doesn&amp;rsquo;t have an IP address configured on the second interface. We would also need to create a source-based routing policy to force all traffic from VM1 (&lt;code&gt;10.0.0.6&lt;/code&gt;) to egress via interface &lt;code&gt;eth2&lt;/code&gt; and make sure IP forwarding is turned on. The following commands would need to be executed on FW VM:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo cirros-dhcpc up eth1
sudo ip rule add from 10.0.0.6 table default
sudo ip route add default via 10.0.0.1 dev eth1 table default
sudo sysctl -w net.ipv4.ip_forward=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Having done that, we should see some packets coming out of &lt;code&gt;egress&lt;/code&gt; port P2.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;skydive client topology query --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.4&#39;, &#39;Type&#39;, &#39;tun&#39;).Flows().Has(&#39;Application&#39;,&#39;ICMPv4&#39;).Values(&#39;Metric.ABPackets&#39;)&amp;quot;
[
  7
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However form the VM1&amp;rsquo;s perspective the ping is still failing. Next step would be to see if the packets are hitting the integration bridge that port P2 is attached to:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;skydive client capture create --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.4&#39;, &#39;Type&#39;, &#39;veth&#39;)&amp;quot;
skydive client topology query --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.4&#39;, &#39;Type&#39;, &#39;veth&#39;).Flows()&amp;quot;
[]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;No packets means they are getting dropped somewhere between the P2 and the integration bridge. This can only be done by security groups. In fact, source MAC/IP anti-spoofing is enabled by default which would only allow packets matching the source MAC/IP addresses assigned to P2 and would drop any packets coming from VM1&amp;rsquo;s IP address. The easiest fix would be to disable security groups for P2 completely:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openstack port set --no-security-group --disable-port-security P2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After this step the counters should start incrementing and the ping from VM1 to its default gateway is resumed.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;skydive client topology query --gremlin &amp;quot;G.V().Has(&#39;Neutron.IPs&#39;, &#39;10.0.0.4&#39;, &#39;Type&#39;, &#39;veth&#39;).Flows().Has(&#39;Application&#39;,&#39;ICMPv4&#39;).Values(&#39;Metric.ABPackets&#39;)&amp;quot;
[
  79
]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;sfc-implementation-in-ovs-forwarding-pipeline&#34;&gt;SFC implementation in OVS forwarding pipeline&lt;/h1&gt;

&lt;p&gt;The only element being affected in our case (both VM1 and FW are on the same compute node) is the integration bridge. Refer to my &lt;a href=&#34;http://networkop.co.uk/blog/2016/04/22/neutron-native/&#34; target=&#34;_blank&#34;&gt;older post&lt;/a&gt; about vanilla OpenStack networking for a refresher of the vanilla OVS-agent architecture.&lt;/p&gt;

&lt;p&gt;Normally, I would start by collecting all port and flow details from the integration bridge with the following commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ovs-ofctl dump-ports-desc br-int  | grep addr
ovs-ofctl dump-flows br-int | cut -d &#39;,&#39; -f3-
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, for the sake of brevity, I will omit the actual outputs and only show graphical representation of forwarding tables and packet flows. The tables below have two columns - first showing what is being matched and second showing the resulting action. Let&amp;rsquo;s start with the OpenFlow rules in an integration bridge before SFC is configured:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sfc-before-tables.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, the table structure is quite simple, since integration bridge mostly relies on data-plane MAC learning. A couple of MAC and ARP anti-spoofing tables will check the validity of a packet and send it to table 60 where &lt;code&gt;NORMAL&lt;/code&gt; action will trigger the &amp;ldquo;flood-and-learn&amp;rdquo; behaviour. Therefore, an ICMP packet coming from VM1 will take the following path:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sfc-before-packet.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After we&amp;rsquo;ve configured SFC, the forwarding pipeline is changed and now looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sfc-after-tables.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First, we can see that table 0 acts as a classifier, by redirecting the &amp;ldquo;interesting&amp;rdquo; packets towards &lt;code&gt;group 1&lt;/code&gt;. This groups is an &lt;a href=&#34;https://floodlight.atlassian.net/wiki/spaces/floodlightcontroller/pages/7995427/How+to+Work+with+Fast-Failover+OpenFlow+Groups&#34; target=&#34;_blank&#34;&gt;OpenFlow Group&lt;/a&gt; of type &lt;code&gt;select&lt;/code&gt;, which load-balances traffic between multiple destinations. By default OVS will use a combination of L2-L4 header as described &lt;a href=&#34;http://docs.openvswitch.org/en/latest/faq/openflow/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; to calculate a hash which determines the output bucket, similar to how per-flow load-balancing works in traditional routers and switches. This behaviour can be overridden with a specific set of headers in &lt;code&gt;lb_fields&lt;/code&gt; setting of a port pair group.&lt;/p&gt;

&lt;p&gt;In our case we&amp;rsquo;ve only got a single SF, so the packet gets its destination MAC updated to that of SF&amp;rsquo;s ingress port and is forwarded to a new table 5. Table 5 is where all packets destined for a SF are aggregated with a single MPLS label which uniquely identifies the service function path. The packet is then forwarded to table 10, which I&amp;rsquo;ve called &lt;code&gt;SFC Ingress&lt;/code&gt;. This is where the packets are distributed to SF&amp;rsquo;s ingress ports based on the assigned MPLS label.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/sfc-after-packet.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After being processed by a SF, the packet leaves the &lt;code&gt;egress&lt;/code&gt; port and re-enters the integration bridge. This time table 0 knows that the packet has already been processed by a SF and, since the anti-spoofing rules have been disabled, simply floods the packet out of all ports in the same VLAN. The packet gets flooded to the tunnel bridge where it gets replicated and delivered to the &lt;code&gt;qrouter&lt;/code&gt; sitting on the controller node as per the &lt;a href=&#34;http://networkop.co.uk/blog/2016/04/22/neutron-native/&#34; target=&#34;_blank&#34;&gt;default behaviour&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;upcoming-enhancements&#34;&gt;Upcoming enhancements&lt;/h1&gt;

&lt;p&gt;SFC is a pretty vast topic and is still under active development. Some of the upcoming enhancement to the current implementation of SFC will include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NSH&lt;/strong&gt; header for SFC correlation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TAP&lt;/strong&gt; functionality which can replace the separate Tap-as-a-service OpenStack project&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service graphs&lt;/strong&gt; allowing multiple chains to be interconnected to create more complex service chain scenarios&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;coming-up&#34;&gt;Coming Up&lt;/h1&gt;

&lt;p&gt;SFC is one of the major features in Telco SDN and, like many things, it&amp;rsquo;s not meant to be configured manually. In fact, Telco SDN have their own framework for management and orchestration of VNFs (a.k.a. VMs) and VNF forwarding graphs (a.k.a. SFCs) called ETSI MANO. As it is expected from a Telco standard, it abounds with acronyms and confuses the hell out of anyone who&amp;rsquo;s name is not on the list of authors or contributors. That&amp;rsquo;s why in the next post I will try to provide a brief overview of what Telco SDN is and use Tacker, a software implementation of NFVO and VNFM, to automatically build a firewall VNF and provision a SFC, similar to what has been done in this post manually.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN - Building a Containerized OpenStack Lab</title>
      <link>https://networkop.co.uk/blog/2017/09/08/os-lab-docker/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/09/08/os-lab-docker/</guid>
      <description>

&lt;p&gt;For quite a long time installation and deployment have been deemed as major barriers for OpenStack adoption. The classic &amp;ldquo;install everything manually&amp;rdquo; approach could only work in small production or lab environments and the ever increasing number of project under the &lt;a href=&#34;https://governance.openstack.org/tc/reference/projects/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;Big Tent&amp;rdquo;&lt;/a&gt; made service-by-service installation infeasible. This led to the rise of automated installers that over time evolved from a simple collection of scripts to container management systems.&lt;/p&gt;

&lt;h1 id=&#34;evolution-of-automated-openstack-installers&#34;&gt;Evolution of automated OpenStack installers&lt;/h1&gt;

&lt;p&gt;The first generation of automated installers were simple utilities that tied together a collection of Puppet/Chef/Ansible scripts. Some of these tools could do baremetal server provisioning through Cobbler or Ironic (Fuel, Compass) and some relied on server operating system to be pre-installed (Devstack, Packstack). In either case the packages were pulled from the Internet or local repository every time the installer ran.&lt;/p&gt;

&lt;p&gt;The biggest problem with the above approach is the time it takes to re-deploy, upgrade or scale the existing environment. Even for relatively small environments it could be hours before all packages are downloaded, installed and configured. One of the ways to tackle this is to pre-build an operating system with all the necessary packages and only use Puppet/Chef/Ansible to change configuration files and turn services on and off. Redhat&amp;rsquo;s TripleO is one example of this approach. It uses a &amp;ldquo;golden image&amp;rdquo; with pre-installed OpenStack packages, which is dd-written bit-by-bit onto the baremetal server&amp;rsquo;s disk. The undercloud then decides which services to turn on based on the overcloud server&amp;rsquo;s role.&lt;/p&gt;

&lt;p&gt;Another big problem with most of the existing deployment methods was that, despite their microservices architecture, all OpenStack services were deployed as static packages on top of a shared operating system. This made the ongoing operations, troubleshooting and ugprades really difficult. The obvious thing to do would be to have all OpenStack services (e.g. Neutron, Keyston, Nova) deployed as containers and managed by a container management system. The first company to implement that, as far as I know, was Canonical. The deployment process is quite complicated, however the end result is a highly flexible OpenStack cloud deployed using LXC containers, managed and orchestrated by Juju controller.&lt;/p&gt;

&lt;p&gt;Today (September 2017) deploying OpenStack services as containers is becoming mainstream and in this post I&amp;rsquo;ll show how to use Kolla to build container images and Kolla-Ansible to deploy them on a pair of &amp;ldquo;baremetal&amp;rdquo; VMs.&lt;/p&gt;

&lt;h1 id=&#34;lab-overview&#34;&gt;Lab overview&lt;/h1&gt;

&lt;p&gt;My lab consists of a single controller and a single compute VM. The goal was to make them as small as possible so they could run on a laptop with limited resources. Both VMs are connected to three VM bridged networks - provisioning, management and external VM access.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/kolla-lab.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve written some bash and Ansible scripts to automate the deployment of VMs on top of any Fedora derivative (e.g. Centos7). These scripts should be run directly from the hypervisor:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/networkop/kolla-odl-bgpvpn.git &amp;amp;&amp;amp; cd kolla-odl-bgpvpn
./1-create.sh do
./2-bootstrap.sh do
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first bash script downloads the VM OS (Centos7), creates two blank VMs and sets up a local Docker registry. The second script installs all the dependencies, including Docker and Ansible.&lt;/p&gt;

&lt;h1 id=&#34;building-openstack-docker-containers-with-kolla&#34;&gt;Building OpenStack docker containers with Kolla&lt;/h1&gt;

&lt;p&gt;The first step in Kolla deployment workflow is deciding where to get the Docker images. Kolla maintains a &lt;a href=&#34;https://hub.docker.com/u/kolla/&#34; target=&#34;_blank&#34;&gt;Docker Hub registry&lt;/a&gt; with container images built for every major OpenStack release. The easiest way to get them would be to pull the images from Docker hub either directly or via a &lt;a href=&#34;https://docs.docker.com/registry/recipes/mirror/&#34; target=&#34;_blank&#34;&gt;pull-through caching registry&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In my case I needed to build the latest version of OpenStack packages, not just the latest major release. I also wanted to build a few additional, non-OpenStack images (Opendaylight and Quagga). Because of that I had to build all Docker images locally and push them into a local docker registry. The procedure to build container images is very well documented in the official &lt;a href=&#34;https://docs.openstack.org/kolla/latest/image-building.html&#34; target=&#34;_blank&#34;&gt;Kolla image building guide&lt;/a&gt;. I&amp;rsquo;ve modified it slightly to include the Quagga Dockerfile and automated it so that the whole process can be run with a single command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./3-build.sh do
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This step can take quite a long time (anything from 1 to 4 hours depending on the network and disk I/O speed), however, once it&amp;rsquo;s been done these container images can be used to deploy as many OpenStack instances as necessary.&lt;/p&gt;

&lt;h1 id=&#34;deploying-openstack-with-kolla-ansible&#34;&gt;Deploying OpenStack with Kolla-Ansible&lt;/h1&gt;

&lt;p&gt;The next step in OpenStack deployment workflow is to deploy Docker images on target hosts. &lt;a href=&#34;https://docs.openstack.org/kolla-ansible/latest/quickstart.html&#34; target=&#34;_blank&#34;&gt;Kolla-Ansible&lt;/a&gt; is a highly customizable OpenStack deployment tool that is also extemely easy to use, at least for people familiar with Ansible. There are two main sources of information for Kolla-Ansible:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Global configuration file (/etc/kolla/globals.yaml), which contains some of the most common customization options&lt;/li&gt;
&lt;li&gt;Ansible inventory file (/usr/share/kolla-ansible/ansible/inventory/*), which maps OpenStack packages to target deployment hosts&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To get started with Kolla-Ansible all what it takes is a few modifications to the global configuration file to make sure that network settings match the underlying OS interface configuration and an update to the inventory file to point it to the correct deployment hosts. In my case I&amp;rsquo;m making additional changes to enable SFC, Skydive and Tacker and adding files for Quagga container, all of which can be done with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./4-deploy.sh do
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The best thing about this method of deployment is that it takes (in my case) under 5 minutes to get the full OpenStack cloud from scratch. That means if I break something or want to redeploy with some major changes (add/remove Opendaylight), all what I have to do is destroy the existing deployment (approx. 1 minute), modify global configuration file and re-deploy OpenStack. This makes Kolla-Ansible an ideal choice for my lab environment.&lt;/p&gt;

&lt;h1 id=&#34;overview-of-containerized-openstack&#34;&gt;Overview of containerized OpenStack&lt;/h1&gt;

&lt;p&gt;Once the deployment has been completed, we should be able to see a number of running Docker containers - one for each OpenStack process.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@compute-1# docker ps
CONTAINER ID        IMAGE                                                                 COMMAND             CREATED             STATUS              PORTS               NAMES
0bb8a8eeb1a9        172.26.0.1:5000/kolla/centos-source-skydive-agent:5.0.0               &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               skydive_agent
63b5b643dfae        172.26.0.1:5000/kolla/centos-source-neutron-openvswitch-agent:5.0.0   &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               neutron_openvswitch_agent
f6f74c5982cb        172.26.0.1:5000/kolla/centos-source-openvswitch-vswitchd:5.0.0        &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               openvswitch_vswitchd
3078421a3892        172.26.0.1:5000/kolla/centos-source-openvswitch-db-server:5.0.0       &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               openvswitch_db
9146c16d561b        172.26.0.1:5000/kolla/centos-source-nova-compute:5.0.0                &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               nova_compute
8079f840627f        172.26.0.1:5000/kolla/centos-source-nova-libvirt:5.0.0                &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               nova_libvirt
220d617d31a5        172.26.0.1:5000/kolla/centos-source-nova-ssh:5.0.0                    &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               nova_ssh
743ce602d485        172.26.0.1:5000/kolla/centos-source-cron:5.0.0                        &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               cron
8b71f08d2781        172.26.0.1:5000/kolla/centos-source-kolla-toolbox:5.0.0               &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               kolla_toolbox
f76d0a7fcf2a        172.26.0.1:5000/kolla/centos-source-fluentd:5.0.0                     &amp;quot;kolla_start&amp;quot;       3 days ago          Up 3 days                               fluentd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All the standard docker tools are available to interact with those containers. For example, this is how we can see what processes are running inside a container:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@compute-1# docker exec nova_compute ps -www aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
nova         1  0.0  0.0    188     4 pts/3    Ss+  Sep04   0:00 /usr/local/bin/dumb-init /bin/bash /usr/local/bin/kolla_start
nova         7  0.7  1.3 2292560 134896 ?      Ssl  Sep04  35:33 /var/lib/kolla/venv/bin/python /var/lib/kolla/venv/bin/nova-compute
root        86  0.0  0.3 179816 32900 ?        S    Sep05   0:00 /var/lib/kolla/venv/bin/python /var/lib/kolla/venv/bin/privsep-helper --config-file /etc/nova/nova.conf --privsep_context vif_plug_ovs.privsep.vif_plug --privsep_sock_path /tmp/tmpFvP0GS/privsep.sock
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some of you may have noticed that none of the containers expose any ports. So how do they communicate? The answer is very simple - all containers run in a &lt;strong&gt;host&lt;/strong&gt; networking mode, effectively disabling any network isolation and giving all contaners access to TCP/IP stacks of their Docker hosts. This is a simple way to avoid having to deal with Docker networking complexities, while at the same time preserving the immutability and portability of Docker containers.&lt;/p&gt;

&lt;p&gt;All containers are configured to restart in case of a failure, however there&amp;rsquo;s no &lt;a href=&#34;abbr:Container Management System&#34; target=&#34;_blank&#34;&gt;CMS&lt;/a&gt; to provide full lifecycle management and advanced scheduling. If upgrade of scale-in/out is needed, Kolla-Ansible will have to be re-run with updated configuration options. There is sibling project called &lt;a href=&#34;https://github.com/openstack/kolla-kubernetes&#34; target=&#34;_blank&#34;&gt;Kolla-Kubernetes&lt;/a&gt; (still under developement), that&amp;rsquo;s designed to address some of the mentioned shortcomings.&lt;/p&gt;

&lt;h1 id=&#34;coming-up&#34;&gt;Coming up&lt;/h1&gt;

&lt;p&gt;Now that the lab is up we can start exploring the new OpenStack SDN features. In the next post I&amp;rsquo;ll have a close look at Neutron&amp;rsquo;s &lt;a href=&#34;abbr: Service Function Chainng&#34; target=&#34;_blank&#34;&gt;SFC&lt;/a&gt; feature, how to configure it and how it&amp;rsquo;s been implemented in OVS forwarding pipeline.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux SSH Session Management for Network Engineers</title>
      <link>https://networkop.co.uk/blog/2017/05/12/linux-ssh/</link>
      <pubDate>Fri, 12 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/05/12/linux-ssh/</guid>
      <description>

&lt;p&gt;A few weeks ago I bought myself a new Dell XPS-13 and decided for the n-th time to go all-in Linux, that is to have Linux as the main and only laptop OS. Since most of my Linux experience is with Fedora-family distros, I quickly installed Fedora-25 and embarked on a long and painful journey of getting out of my Windows comfort zone and re-establishing it in Linux. One of the most important aspects for me, as a network engineer, is to have a streamlined process of accessing network devices. In Windows I was using MTPutty and it helped define my expectations of an ideal SSH session manager:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I want a multi-tab terminal with the ability to switch between tabs quickly - default (GNOME) terminal does that out-of-the box with no extra modifications&lt;/li&gt;
&lt;li&gt;I want to login the device without having to enter a password - Not available by default but is possible with some dirty &lt;code&gt;expect&lt;/code&gt; hacks.&lt;/li&gt;
&lt;li&gt;I want my SSH sessions to be organised in a hierarchical manner with groups representing various administrative domains - customer A, local VMs, lab.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although GNOME terminal looked like a very good option, it didn&amp;rsquo;t meet all of my requirements. I briefly looked and PAC Manager and GNOME Connection Manager but quickly dismissed them due to their ugliness and clunkiness. Ideally I wanted to keep using GNOME terminal as the main terminal emulator, without having to configure and rely on other 3rd party apps. I also didn&amp;rsquo;t want to wrap my SSH session in &lt;code&gt;expect&lt;/code&gt; as I didn&amp;rsquo;t want my password to be pasted in my screen every time I &lt;em&gt;cat&lt;/em&gt; a file containing the trigger keyword &lt;em&gt;Password:&lt;/em&gt;. I&amp;rsquo;ve finally managed to make everything work inside the native GNOME terminal and this post is a documentation of my approach.&lt;/p&gt;

&lt;h1 id=&#34;1-install-ssh-copy-net&#34;&gt;1. Install ssh-copy-net&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;ve written a little &lt;a href=&#34;https://github.com/networkop/ssh-copy-net&#34; target=&#34;_blank&#34;&gt;tool&lt;/a&gt; that uses &lt;a href=&#34;https://github.com/ktbyers/netmiko&#34; target=&#34;_blank&#34;&gt;Netmiko&lt;/a&gt; to install (and remove) public SSH keys onto network devices. Assuming &lt;code&gt;python-pip&lt;/code&gt; is already installed here&amp;rsquo;s what&amp;rsquo;s required to download and install &lt;code&gt;ssh-copy-net&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pip install git+https://github.com/networkop/ssh-copy-net.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Its functionality mimics the one of &lt;code&gt;ssh-copy-id&lt;/code&gt;, so the next step is always to upload the public key to the device:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh-copy-net 10.6.142.1 juniper
Username: admin
Password:
All Done!
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;2-define-ssh-config-for-network-devices&#34;&gt;2. Define SSH config for network devices&lt;/h1&gt;

&lt;p&gt;OpenSSH client &lt;a href=&#34;https://linux.die.net/man/5/ssh_config&#34; target=&#34;_blank&#34;&gt;config file&lt;/a&gt; provides a nice way of managing user&amp;rsquo;s SSH sessions. Configuration file allows you to define per-host SSH settings including username, port forwarding options, key checking flags etc. In my case all what I had to do was define IP addresses of my network devices:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Host srx
  HostName 10.6.142.1

Host arista
  HostName 10.6.142.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now I am able to login the device by simply typing its name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh arista
Last login: Sun May  7 10:57:30 2017 from 10.1.2.3
arista-1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;3-define-zsh-aliases&#34;&gt;3. Define zsh aliases&lt;/h1&gt;

&lt;p&gt;The final step is session organisations. For that I&amp;rsquo;ve decided to use zsh aliases and have device groups encoded in the alias name, separated by dashes. For example, if my SRX device was in the &lt;strong&gt;lab&lt;/strong&gt; and Arista was in &lt;strong&gt;Site-51&lt;/strong&gt; of &lt;strong&gt;Customer-A&lt;/strong&gt; this is how I would write my aliases:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;alias lab-srx=&#39;ssh srx&#39;
alias customer-a-site-51-arista=&#39;ssh arista&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;4-multi-pane-sessions-with-tmux&#34;&gt;4. Multi-pane sessions with tmux&lt;/h1&gt;

&lt;p&gt;As a network engineer, I often find myself troubleshooting issues spanning multiple devices, which is why I need multiple tabs inside a single terminal window. Simply pressing Ctrl+T in GNOME terminal opens a new tab and I can switch between tabs using Alt+[1-9]. However what would be really nice is to have a couple of tabs opened side by side so that I can see the logs and compare output on a number of devices at the same time. This is where tmux comes in. It can do much more than this, but I simply use it to have multiple panes inside the same terminal tab:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/tmux.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an example of my tmux configuration file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Automatically set window title
set-window-option -g automatic-rename on
set-option -g set-titles on

# Use Alt-arrow keys without prefix key to switch panes
bind -n M-Left select-pane -L
bind -n M-Right select-pane -R
bind -n M-Up select-pane -U
bind -n M-Down select-pane -D

# Pane splitting keys
bind-key v split-window -h
bind-key s split-window -v

# New key-binding to reset hung SSH sessions
bind-key k respawn-pane -k

# Easy fix for arrow keys inside ssh
set -g default-terminal &amp;quot;xterm&amp;quot;

# Enable mouse mode (tmux 2.1 and above)
set -g mouse on

# Reload tmux config
bind r source-file ~/.tmux.conf

# No delay for escape key press
set -sg escape-time 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;demo&#34;&gt;Demo&lt;/h1&gt;

&lt;p&gt;Now having all the above defined and with the help of zsh command autocompletion, I can login the device with just a few keypresses (shown in square brackets below):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ lab  [TAB]
$ lab-  [TAB]
lab-srx
$ lab-  [s][TAB]
$ lab-srx  [ENTER]
--- JUNOS 12.3X48-D30.7 built 2016-04-28 22:37:34 UTC
{primary:node0}
null@srx&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press Ctrl+B v to split the terminal window vertically:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ customer [TAB]
$ customer- [TAB]
customer-a-site-51-arista
$ customer- [a][TAB]
$ customer-a-arista [ENTER]
Last login: Thu May 11 15:28:03 2017 from 10.1.2.3
arista-1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An so on and so forth&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/tmux.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using YANG Models in Ansible to Configure and Verify State of IOS-XE and JUNOS Devices</title>
      <link>https://networkop.co.uk/blog/2017/04/04/ansible-yang/</link>
      <pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/04/04/ansible-yang/</guid>
      <description>

&lt;p&gt;The idea of using Ansible for &lt;a href=&#34;http://networkop.co.uk/blog/2015/08/26/automating-network-build-p1/&#34; target=&#34;_blank&#34;&gt;configuration changes&lt;/a&gt; and &lt;a href=&#34;https://github.com/networktocode/ntc-ansible&#34; target=&#34;_blank&#34;&gt;state verification&lt;/a&gt; is not new. However the approach I&amp;rsquo;m going to demonstrate in this post, using YANG and NETCONF, will have a few notable differences:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I will not use any templates and absolutely no XML/JSON for device config generation&lt;/li&gt;
&lt;li&gt;All changes will be pushed through a single, vendor and model-independent Ansible module&lt;/li&gt;
&lt;li&gt;State verification will be done with no pattern-matching or screen-scraping&lt;/li&gt;
&lt;li&gt;All configuration and operational state will be based on a couple of YAML files&lt;/li&gt;
&lt;li&gt;To demonstrate the model-agnostic behaviour I will use a mixture of vendor&amp;rsquo;s native, IETF and OpenConfig YANG models&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I hope this promise is exciting enough so without further ado, let&amp;rsquo;s get cracking.&lt;/p&gt;

&lt;h1 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h1&gt;

&lt;p&gt;The test environment will consist of a single instance of CSR1000v running IOS-XE version 16.4.1 and a single instance of vMX running JUNOS version 17.1R1.8. The VMs containing the two devices are deployed within a single hypervisor and connected with one interface to the management network and back-to-back with the second  pair of interfaces for BGP peering.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ansible-yang.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each device contains some basic initial configuration to allow it be reachable from the Ansible server.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;interface GigabitEthernet1
ip address 192.168.145.51 255.255.255.0
!
netconf-yang
netconf-yang cisco-odm polling enable
netconf-yang cisco-odm actions parse Interfaces
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vMX configuration is quite similar. Static MAC address is &lt;a href=&#34;http://noshut.ru/2015/09/how-to-run-juniper-vmx-in-unetlab/&#34; target=&#34;_blank&#34;&gt;required&lt;/a&gt; in order for &lt;code&gt;ge&lt;/code&gt; interfaces to work.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set system login user admin class super password admin123
set system services netconf
set interface fxp0 unit 0 family inet address 192.168.145.53/24
set interface ge-0/0/0 mac 00:0c:29:fc:1a:b7
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;ansible-playbook-configuration&#34;&gt;Ansible playbook configuration&lt;/h1&gt;

&lt;p&gt;My &lt;a href=&#34;https://github.com/networkop/yang/tree/master/ansible-101&#34; target=&#34;_blank&#34;&gt;Ansible-101&lt;/a&gt; repository contains two plays - one for configuration and one for state verification. The local inventory file contains details about the two devices along with the login credentials. All the work will be performed by a custom Ansible module stored in the &lt;code&gt;./library&lt;/code&gt; directory. This module is a wrapper for a &lt;code&gt;ydk_yaml&lt;/code&gt; module described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/03/13/yaml-yang/&#34;&gt;previous post&lt;/a&gt;. I had to heavily modify the original &lt;code&gt;ydk_yaml&lt;/code&gt; module to work around some Ansible limitations, like the lack of support for &lt;strong&gt;set&lt;/strong&gt; data structures.&lt;br /&gt;
This custom Ansible module also relies on a number of &lt;a href=&#34;https://networkop.co.uk/blog/2017/02/22/odl-ydk/&#34;&gt;YDK&lt;/a&gt; Python bindings to be pre-installed. Refer to my &lt;a href=&#34;https://github.com/networkop/yang/tree/master/yaml-101&#34; target=&#34;_blank&#34;&gt;YAML&lt;/a&gt;, &lt;a href=&#34;https://github.com/networkop/yang/tree/master/oper-101&#34; target=&#34;_blank&#34;&gt;Operational&lt;/a&gt; and &lt;a href=&#34;https://github.com/networkop/yang/tree/master/junos-101&#34; target=&#34;_blank&#34;&gt;JUNOS&lt;/a&gt; repositories for the instructions on how to install those modules.&lt;br /&gt;
The desired configuration and expected operational state are documented inside a couple of device-specific host variable files. For each device there is a configuration file &lt;code&gt;config.yaml&lt;/code&gt;, describing the desired configuration state. For IOS-XE there is an additional file &lt;code&gt;verify.yaml&lt;/code&gt;, describing the expected operational state using the IETF interface YANG model (I couldn&amp;rsquo;t find how to get the IETF or OpenConfig state models to work on Juniper).&lt;br /&gt;
All of these files follow the same structure:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Root container can be either &lt;code&gt;config&lt;/code&gt; or &lt;code&gt;verify&lt;/code&gt; and defines how the enclosed data is supposed to be used&lt;/li&gt;
&lt;li&gt;First nested container has to match the top-most container of a YANG model. For example it could be &lt;strong&gt;bgp-state&lt;/strong&gt; for &lt;a href=&#34;https://github.com/YangModels/yang/blob/master/vendor/cisco/xe/1641/cisco-bgp-state.yang&#34; target=&#34;_blank&#34;&gt;cisco-bgp-state.yang&lt;/a&gt; or &lt;strong&gt;openconfig-bgp&lt;/strong&gt; for &lt;a href=&#34;https://github.com/openconfig/public/blob/master/release/models/bgp/openconfig-bgp.yang&#34; target=&#34;_blank&#34;&gt;openconfig-bgp.yang&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;The remaining nested data has to follow the structure of the original YANG model as described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/03/13/yaml-yang/&#34;&gt;previous post&lt;/a&gt;.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here&amp;rsquo;s how IOS-XE will be configured, using IETF interfaca YANG models (to unshut the interface) and Cisco&amp;rsquo;s native YANG model for interface IP and BGP settings:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;---
config:
  interfaces:
    interface:
      - name: GigabitEthernet3
        enabled: true
  native:
    interface:
      gigabitethernet:
        - name: &#39;3&#39;
          description: P2P link
          ip:
            address:
              primary:
                address: 12.12.12.1
                mask: 255.255.255.0
      loopback:
        - name: 0
          description: ROUTER ID
          ip:
            address:
              primary:
                address: 1.1.1.1
                mask: 255.255.255.255
    router:
      bgp:
        - id: 65111
          bgp:
            router_id: 1.1.1.1
          neighbor:
            - id: 12.12.12.2
              remote_as: 65222
          redistribute:
            connected:
              empty: empty
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For JUNOS configuration, instead of the default humongous native model, I&amp;rsquo;ll use a set of much more light-weight OpenConfig YANG models to configure interfaces, BGP and redistribution policies:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;---
config:
  openconfig-interfaces:
    interface:
      - name: ge-0/0/0
        subinterfaces:
          subinterface:
            - index: 0
              ipv4:
                addresses:
                  address:
                    - ip: 12.12.12.2/24
                      config:
                        ip: 12.12.2.2
                        prefix_length: 24
      - name: lo0
        subinterfaces:
          subinterface:
            - index: 0
              ipv4:
                addresses:
                  address:
                    - ip: 2.2.2.2/32
                      config:
                        ip: 2.2.2.2
                        prefix_length: 32
  openconfig-policy:
    policy_definitions:
      policy_definition:
        - name: CONNECTED-&amp;gt;BGP
          statements:
            statement:
              - name: Loopback0
                conditions:
                  match_interface:
                    config:
                      interface: lo0
                      subinterface: 0
                actions:
                  config:
                    accept_route: empty
  openconfig-bgp:
    global_:
      config:
        as_: 65222
    neighbors:
      neighbor:
        - neighbor_address: 12.12.12.1
          config:
            peer_group: YANG
            peer_as: 65111
    peer_groups:
      peer_group:
        - peer_group_name: YANG
          config:
            peer_as: 65111
          apply_policy:
            config:
              export_policy:
                - CONNECTED-&amp;gt;BGP
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;configuration&#34;&gt;Configuration&lt;/h1&gt;

&lt;p&gt;Both devices now can be configured with just a single command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook config.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Behind the scenes, Ansible calls my custom &lt;code&gt;ydk_module&lt;/code&gt; and passes to it the full configuration state and device credentials. This module then constructs an empty YDK binding based on the name of a YANG model and &lt;a href=&#34;https://networkop.co.uk/blog/2017/03/13/yaml-yang/&#34;&gt;populates it recursively&lt;/a&gt; with the data from the &lt;code&gt;config&lt;/code&gt; container. Finally, it pushes the data to the device with the help of YDK NETCONF service provider.&lt;/p&gt;

&lt;h1 id=&#34;verification&#34;&gt;Verification&lt;/h1&gt;

&lt;p&gt;There&amp;rsquo;s one side to YANG which I have carefully avoided until now and it&amp;rsquo;s operational state models. These YANG models are built similarly to configuration models, but with a different goal - to extract the running state from a device. The reason why I&amp;rsquo;ve avoided them is that, unlike the configuration models, the current support for state models is limited and somewhat brittle.&lt;br /&gt;
For example, JUNOS natively only supports state models as RPCs, where each RPC represents a certain &lt;code&gt;show&lt;/code&gt; command which, I assume, when passed to the devices gets evaluated, its output parsed and result returned back to the client. With IOX-XE things are a little better with a few of the operational models available in the current 16.4 release. You can check out my &lt;a href=&#34;https://github.com/networkop/yang/tree/master/oper-101&#34; target=&#34;_blank&#34;&gt;Github repo&lt;/a&gt; for some examples of how to check the interface and BGP neighbor state between the two IOS-XE devices. However, most of the models are still missing (I&amp;rsquo;m not counting the MIB-mapped YANG models) in the current release. The next few releases, though, are promised to come with an improved state model support, including some OpenConfig models, which is going to be super cool.&lt;br /&gt;
So in this post, since I couldn&amp;rsquo;t get JUNOS OpenConfig models report any state and my IOS-XE BGP state model wouldn&amp;rsquo;t return any output unless the BGP peering was with another Cisco device or in the &lt;strong&gt;Idle&lt;/strong&gt; state, I&amp;rsquo;m going to have to resort to simply checking the state of physical interfaces. This is how a sample operational state file would look like (question marks are YAML&amp;rsquo;s special notation for sets which is how I decided to encode Enum data type):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
verify:
  interfaces-state:
    interface:
      - name: GigabitEthernet3
        oper_status:
          ? up
      - name: Loopback0
        oper_status:
          ? up
      - name: GigabitEthernet2
        oper_status:
          ? down
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once again, all expected state can be verified with a single command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook verify.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the state defined in that YAML file matches the data returned by the IOS-XE device, the playbook completes successfully. You can check that it works by shutting down one of the &lt;code&gt;GigabitEthernet3&lt;/code&gt; or &lt;code&gt;Loopback0&lt;/code&gt; interfaces and observing how Ansible module returns an error.&lt;/p&gt;

&lt;h1 id=&#34;outro&#34;&gt;Outro&lt;/h1&gt;

&lt;p&gt;Now that I&amp;rsquo;ve come to the end of my YANG series of posts I feel like I need to provide some concise and critical summary of everything I&amp;rsquo;ve been through. However, if there&amp;rsquo;s one thing I&amp;rsquo;ve learned in the last couple of months about YANG, it&amp;rsquo;s that things are changing very rapidly. Both Cisco and Juniper are working hard introducing new models and improving support for the existing ones. So one thing to keep in mind, if you&amp;rsquo;re reading this post a few months after it was published (April 2017), is that some or most of the above limitations may not exist and it&amp;rsquo;s always worth checking what the latest software release has to offer.&lt;/p&gt;

&lt;p&gt;Finally, I wanted to say that I&amp;rsquo;m a strong believer that YANG models are the way forward for network device configuration and state verification, despite the timid scepticism of the networking industry. I think that there are two things that may improve the industry&amp;rsquo;s perception of YANG and help increase its adoption:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Support from networking vendors - we&amp;rsquo;ve already seen Cisco changing by introducing YANG support on IOS-XE instead of producing another dubious One-PK clone. So big thanks to them and I hope that other vendors will follow suit.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tools - this part, IMHO, is the most crucial. In order for people to start using YANG models we have to have the right tools that would be versatile enough to allow network engineers to be limited only by their imagination and at the same time be as robust as the CLI. So I wanted to give a big shout out to all the people contributing to open-source projects like &lt;strong&gt;pyang&lt;/strong&gt;, &lt;strong&gt;YDK&lt;/strong&gt; and many others that I have missed or don&amp;rsquo;t know about. You&amp;rsquo;re doing a great job guys, don&amp;rsquo;s stop.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>YANG &amp; Ansible</title>
      <link>https://networkop.co.uk/tags/ansible-yang/</link>
      <pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/tags/ansible-yang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Configuring Cisco IOS XE With YANG-based YAML Files</title>
      <link>https://networkop.co.uk/blog/2017/03/13/yaml-yang/</link>
      <pubDate>Mon, 13 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/03/13/yaml-yang/</guid>
      <description>

&lt;p&gt;XML, just like many more structured data formats, was not designed to be human-friendly. That&amp;rsquo;s why many network engineers lose interest in YANG as soon as the conversation gets to the XML part. JSON is a much more human-readable alternative, however very few devices support RESTCONF, and the ones that do may have &lt;a href=&#34;https://github.com/CiscoDevNet/openconfig-getting-started/issues/4&#34; target=&#34;_blank&#34;&gt;buggy implementations&lt;/a&gt;. At the same time, a lot of network engineers have happily embraced Ansible, which extensively uses YAML. That&amp;rsquo;s why I&amp;rsquo;ve decided to write a Python module that would program network devices using YANG and NETCONF according to configuration data described in a YAML format.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&#34;blog/2017/02/22/odl-ydk/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; I have introduced a new open-source tool called YDK, designed to create API bindings for YANG models and interact with network devices using NETCONF or RESTCONF protocols. I have also mentioned that I would still prefer to use &lt;a href=&#34;https://github.com/robshakir/pyangbind&#34; target=&#34;_blank&#34;&gt;pyangbind&lt;/a&gt; along with other open-source tools to achieve the same functionality. Now, two weeks later, I must admin I have been converted. Initially, I was planning to write a simple REST API client to interact with RESTCONF interface of IOS XE, create an API binding with &lt;strong&gt;pyangbind&lt;/strong&gt;, use it to produce the JSON output, convert it to XML and send it to the device, similar to what I&amp;rsquo;ve described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;netconf&lt;/a&gt; and &lt;a href=&#34;blog/2017/02/15/restconf-yang/&#34; target=&#34;_blank&#34;&gt;restconf&lt;/a&gt; posts. However, I&amp;rsquo;ve realised that YDK can already do all what I need with just a few function calls. All what I&amp;rsquo;ve got left to do is create a wrapper module to consume the YAML data and use it to automatically populate YDK bindings.&lt;/p&gt;

&lt;p&gt;This post will be mostly about the internal structure of this wrapper module I call &lt;code&gt;ydk_yaml.py&lt;/code&gt;, which will serve as a base library for a YANG Ansible module, which I will describe in my next post. This post will be very programming-oriented, I&amp;rsquo;ll start with a quick overview of some of the programming concepts being used by the module and then move on to the details of module implementation. Those who are not interested in technical details can jump straight to the &lt;strong&gt;examples&lt;/strong&gt; sections at the end of this post for a quick demonstration of how it works.&lt;/p&gt;

&lt;h1 id=&#34;recursion&#34;&gt;Recursion&lt;/h1&gt;

&lt;p&gt;One of the main tasks of &lt;code&gt;ydk_yaml.py&lt;/code&gt; module is to be able parse a YAML data structure. This data structure, when loaded into Python, is stored as a collection of Python objects like dictionaries, lists and primitive data types like strings, integers and booleans. One key property of YAML data structures is that they can be represented as trees and parsing trees is a very well-known programming problem.&lt;/p&gt;

&lt;p&gt;After having completed &lt;a href=&#34;https://www.coursera.org/learn/programming-languages&#34; target=&#34;_blank&#34;&gt;this programming course&lt;/a&gt; I fell in love with functional programming and recursions. Every problem I see, I try to solve with a recursive function. Recursions are very interesting in a way that they are very difficult to understand but relatively easy to write. Any recursive function will consist of a number of &lt;code&gt;if/then/else&lt;/code&gt; conditional statements. The first one (or few) &lt;code&gt;if&lt;/code&gt; statements are called the base of a recursion - this is where recursion stops and the value is returned to the outer function. The remaining few &lt;code&gt;if&lt;/code&gt; statements will implement the recursion by calling the same function with a &lt;strong&gt;reduced input&lt;/strong&gt;. You can find a much better explanation of recursive functions &lt;a href=&#34;http://composingprograms.com/pages/17-recursive-functions.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. For now, let&amp;rsquo;s consider the problem of parsing the following tree-like data structure:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{ &#39;parent&#39;: {
    &#39;child_1&#39;: {
      &#39;leaf_1&#39;: &#39;value_1&#39;
    },
    &#39;child_1&#39;: &#39;value_2&#39;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Recursive function to parse this data structure written in a pseudo-language will look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def recursion(input_key, input_value):
  if input_value is String:
    return process(input_value)
  elif input_value is Dictonary:
    for key, value in input_value.keys_and_values():
      return recursion(key, value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The beauty of recursive functions is that they are capable parsing data structures of arbitrary complexity. That means if we had 1000 randomly nested child elements in the parent data structure, they all could have been parsed by the same 6-line function.&lt;/p&gt;

&lt;h1 id=&#34;introspection&#34;&gt;Introspection&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://zetcode.com/lang/python/introspection/&#34; target=&#34;_blank&#34;&gt;Introspection&lt;/a&gt; refers to the ability of Python to examine objects at runtime. It can be useful when dealing with object of arbitrary structure, e.g. a YAML document. Introspection is used whenever there is a need for a function to behave differently based on the runtime data. In the above pseudo-language example, the two conditional statements are the examples of introspection. Whenever we need to determine the type of an object in Python we can either use a built-in function &lt;code&gt;type(obj)&lt;/code&gt; which returns the type of an object or &lt;code&gt;isinstance(obj, type)&lt;/code&gt; which checks if the &lt;strong&gt;object&lt;/strong&gt; is an &lt;em&gt;instance&lt;/em&gt; or a &lt;em&gt;descendant&lt;/em&gt; of a particular &lt;strong&gt;type&lt;/strong&gt;. This is how we can re-write the above two conditional statements using real Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if isinstance(input_value, str):
  print(&#39;input value is a string&#39;)
elif isinstance(input_value, dict):
  print(&#39;intput value is a dictionary&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;metaprogramming&#34;&gt;Metaprogramming&lt;/h1&gt;

&lt;p&gt;Another programming concept used in my Python module is &lt;a href=&#34;http://chase-seibert.github.io/blog/2013/04/12/getattr-setattr.html&#34; target=&#34;_blank&#34;&gt;metaprogramming&lt;/a&gt;. Metaprogramming, in general, refers to an ability of programs to write themselves. This is what compilers normally do when they read the program written in a higher-level language and translate it to a lower-level language, like assembler. What I&amp;rsquo;ve used in my module is the simplest version of metaprogramming - dynamic getting and setting of object attributes. For example, this is how we would configure BGP using YDK Python binding, as described in my &lt;a href=&#34;blog/2017/02/22/odl-ydk/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bgp.id = 100
n = bgp.Neighbor()
n.id = &#39;2.2.2.2&#39;
n.remote_as = 65100
bgp.neighbor.append(n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The same code could be re-written using the &lt;code&gt;getattr&lt;/code&gt; and &lt;code&gt;setattr&lt;/code&gt; method calls:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;setattr(bgp, &#39;id&#39;, 100)
n = getattr(bgp, &#39;Neighbor&#39;)()
setattr(n, &#39;id&#39;, &#39;2.2.2.2&#39;)
setattr(n, &#39;remote_as&#39;, 65100)
getattr(bgp, &#39;neighbor&#39;).append(n)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is also very useful when working with arbitrary data structures and objects. In my case the goal was to write a module that would be completely independent of the structure of a particular YANG model, which means that I can &lt;strong&gt;not know&lt;/strong&gt; the structure of the Python binding generated by YDK. However, I can &amp;ldquo;guess&amp;rdquo; the name of the attributes if I assume that my YAML document is structured &lt;strong&gt;exactly&lt;/strong&gt; like the YANG model. This simple assumption allows me to implement YAML mapping for &lt;strong&gt;all&lt;/strong&gt; possible YANG models with just a single function.&lt;/p&gt;

&lt;h1 id=&#34;yang-mapping-to-yaml&#34;&gt;YANG mapping to YAML&lt;/h1&gt;

&lt;p&gt;As I&amp;rsquo;ve mentioned in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;previous post&lt;/a&gt;, YANG is simply a way to define the structure of an XML document. At the same time, it is known that YANG-based XML can be mapped to JSON as described in &lt;a href=&#34;https://tools.ietf.org/html/draft-ietf-netmod-yang-json-10&#34; target=&#34;_blank&#34;&gt;this RFC&lt;/a&gt;. Since YAML is a superset of JSON, it&amp;rsquo;s easy to come up with a similar XML-to-YAML mapping convention. The following table contains the mapping between some of the most common YAML and YANG data structures and types:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;YANG data&lt;/th&gt;
&lt;th&gt;YAML representation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;container&lt;/td&gt;
&lt;td&gt;dictionary&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;container name&lt;/td&gt;
&lt;td&gt;dictionary key&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;leaf name&lt;/td&gt;
&lt;td&gt;dictionary key&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;leaf&lt;/td&gt;
&lt;td&gt;dictionary value&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;string, bool, integer&lt;/td&gt;
&lt;td&gt;string, bool, integer&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;empty&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Using this table, it&amp;rsquo;s easy to map the YANG data model to a YAML document. Let me demonstrate it on IOS XE&amp;rsquo;s native OSPF data model. First, I&amp;rsquo;ve generated a tree representation of an OSPF data model using &lt;strong&gt;pyang&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pyang -f tree --tree-path &amp;quot;/native/router/ospf&amp;quot; ~/ydk-gen/gen-api/.cache/models/cisco_ios_xe@0.1.0/ned.yang -o ospf.tree
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, I&amp;rsquo;ve trimmed it down to only contain the options that I would like to set and created a YAML document based on the model&amp;rsquo;s tree structure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/yang-yaml.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With the right knowledge of &lt;a href=&#34;blog/2017/02/15/restconf-yang/&#34; target=&#34;_blank&#34;&gt;YANG model&amp;rsquo;s structure&lt;/a&gt;, it&amp;rsquo;s fairly easy to generate similar YAML configuration files for other configuration objects, like &lt;a href=&#34;https://github.com/networkop/yang/blob/master/yaml-101/interface.yaml&#34; target=&#34;_blank&#34;&gt;interface&lt;/a&gt; and &lt;a href=&#34;https://github.com/networkop/yang/blob/master/yaml-101/bgp.yaml&#34; target=&#34;_blank&#34;&gt;BGP&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;yang-instantiating-function&#34;&gt;YANG instantiating function&lt;/h1&gt;

&lt;p&gt;At the heart of the &lt;code&gt;ydk_yaml&lt;/code&gt; module is a single recursive function that traverses the input YAML data structure and uses it to instantiate the YDK-generated Python binding. Here is a simple, abridged version of the function that demonstrates the main logic.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def instantiate(binding, model_key, model_value):
    if any(isinstance(model_value, x) for x in [str, bool, int]):
        setattr(binding, model_key, model_value)
    elif isinstance(model_value, list):
        for el in model_value:
            getattr(binding, model_key).append(instantiate(binding, model_key, el))
    elif isinstance(model_value, dict):
        container_instance = getattr(binding, model_key)()
        for k, v in model_value.iteritems():
            instantiate(container_instance, k, v)
        setattr(binding, model_key, container_instance)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Most of it should already make sense based on what I&amp;rsquo;ve covered above. The first conditional statement is the base of the recursion and performs the action of setting the value of a YANG Leaf element. The second conditional statement takes care of a YANG List by traversing all its elements, instantiating them recursively, and appends the result to a YDK binding. The last &lt;code&gt;elif&lt;/code&gt; statement creates a class instance for a YANG container, recursively populates its values and saves the final result inside a YDK binding.&lt;/p&gt;

&lt;p&gt;The full version of this function covers a few extra corner cases and can be found &lt;a href=&#34;https://github.com/networkop/yang/blob/master/yaml-101/ydk_yaml.py&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;the-ydk-module-wrapper&#34;&gt;The YDK module wrapper&lt;/h1&gt;

&lt;p&gt;The final step is to write a wrapper class that would consume the YDK model binding along with the YAML data, and both instantiate and push the configuration down to the network device.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class YdkModel:

    def __init__(self, model, data):
        self.model = model
        self.data = data
        from ydk.models.cisco_ios_xe.ned import Native
        self.binding = Native()
        for k,v in self.data.iteritems():
            instantiate(self.binding, k, v)

    def action(self, crud_action, device):
        from ydk.services import CRUDService
        from ydk.providers import NetconfServiceProvider
        provider = NetconfServiceProvider(address=device[&#39;hostname&#39;],
                                          port=device[&#39;port&#39;],
                                          username=device[&#39;username&#39;],
                                          password=device[&#39;password&#39;],
                                          protocol=&#39;ssh&#39;)
        crud = CRUDService()
        crud_instance = getattr(crud, crud_action)
        crud_instance(provider, self.binding)
        provider.close()
        return
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The structure of this class is pretty simple. The constructor instantiates a YDK native data model and calls the recursive instantiation function to populate the binding. The &lt;strong&gt;action&lt;/strong&gt; method implements standard CRUD actions using the YDK&amp;rsquo;s NETCONF provider. The full version of this Python module can be found &lt;a href=&#34;https://github.com/networkop/yang/blob/master/yaml-101/ydk_yaml.py&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;configuration-examples&#34;&gt;Configuration examples&lt;/h1&gt;

&lt;p&gt;In my &lt;a href=&#34;https://github.com/networkop/yang/tree/master/yaml-101&#34; target=&#34;_blank&#34;&gt;Github repo&lt;/a&gt;, I&amp;rsquo;ve included a few examples of how to configure Interface, OSPF and BGP settings of IOS XE device. A helper Python script &lt;code&gt;1_send_yaml.py&lt;/code&gt; accepts the YANG model name and the name of the YAML configuration file as the input. It then instantiates the &lt;code&gt;YdkModel&lt;/code&gt; class and calls the &lt;code&gt;create&lt;/code&gt; action to push the configuration to the device. Let&amp;rsquo;s assume that we have the following YAML configuration data saved in a &lt;code&gt;bgp.yaml&lt;/code&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+++
router:
  bgp:
    - id: 100
      bgp:
        router_id: 1.1.1.1
        fast_external_fallover: null
        update_delay: 15
      neighbor:
        - id: 2.2.2.2
          remote_as: 200
        - id: 3.3.3.3
          remote_as: 300
      redistribute:
        connected: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To push this BGP configuration to the device all what I need to do is run the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./1_send_yaml.py bgp bgp.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The resulting configuration on IOS XE device would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;router bgp 100
 bgp router-id 1.1.1.1
 bgp log-neighbor-changes
 bgp update-delay 15
 redistribute connected
 neighbor 2.2.2.2 remote-as 200
 neighbor 3.3.3.3 remote-as 300
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To see more example, follow &lt;a href=&#34;https://github.com/networkop/yang/tree/master/yaml-101&#34; target=&#34;_blank&#34;&gt;this link&lt;/a&gt; to my Github repo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Configuring Cisco IOS XE With YDK and OpenDaylight</title>
      <link>https://networkop.co.uk/blog/2017/02/22/odl-ydk/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/02/22/odl-ydk/</guid>
      <description>

&lt;p&gt;In the previous posts about &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;NETCONF&lt;/a&gt; and &lt;a href=&#34;https://networkop.co.uk/blog/2017/02/15/restconf-yang/&#34;&gt;RESTCONF&lt;/a&gt; I&amp;rsquo;ve demonstrated how to interact with Cisco IOS XE device directly from the Linux shell of my development VM. This approach works fine in some cases, e.g. whenever I setup a new DC fabric, I would make calls directly to the devices I&amp;rsquo;m configuring. However, it becomes impractical in the Ops world where change is constant and involves a large number of devices. This is where centralised service orchestrators come to the fore. The prime examples of such platforms are Network Services Orchestrator from Tail-f/Cisco and open-source project OpenDaylight. In this post we&amp;rsquo;ll concentrate on ODL and how to make it work with Cisco IOS XE. Additionally, I&amp;rsquo;ll show how to use an open-source tool &lt;a href=&#34;https://developer.cisco.com/site/ydk/&#34; target=&#34;_blank&#34;&gt;YDK&lt;/a&gt; to generate Python bindings for native YANG models and how it compares with &lt;strong&gt;pyangbind&lt;/strong&gt;.&lt;/p&gt;

&lt;h1 id=&#34;opendaylight-primer&#34;&gt;OpenDaylight primer&lt;/h1&gt;

&lt;p&gt;OpenDaylight is a swiss army knife of SDN controllers. At the moment it is comprised of dozens of projects implementing all possible sorts of SDN functionality starting from Openflow controller all the way up to L3VPN orchestrator. ODL speaks most of the modern Southbound protocols like Openflow, SNMP, NETCONF and BGP. The brain of the controller is in the Service Abstraction Layer, a framework to model all network-related characteristics and properties. All logic inside SAL is modelled in YANG which is why I called it the godfather of YANG models. Towards the end users ODL exposes Java function calls for applications running on the same host and REST API for application running remotely.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/odl-sal.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;OpenDaylight has several commercial offerings from companies involved in its development. Most notable ones are from Brocade and Cisco. Here I will allow myself a bit of a rant, feel free to skip it to go straight to the technical stuff.&lt;/p&gt;

&lt;p&gt;One thing I find interesting is that Cisco are being so secretive about their Open SDN Controller, perhaps due to the earlier market pressure to come up with a single SDN story, but still have a very large number of contributors to this open-source project. It could be the case of having an egg in each basket, but the number of Cisco&amp;rsquo;s employees involved in ODL development is substantial. I wonder if, now that the use cases for ACI and ODL have finally formed and ACI still not showing the uptake originally expected, Cisco will change their strategy and start promoting ODL more aggressively, or at least stop hiding it deep in the bowels of &lt;a href=&#34;cisco.com&#34; target=&#34;_blank&#34;&gt;cisco.com&lt;/a&gt;. Or, perhaps, it will always stay in the shade of Tail-f&amp;rsquo;s NSC and Insieme&amp;rsquo;s ACI and will be used only for customer with unique requirements, e.g. to have both OpenStack and network devices managed through the same controller.&lt;/p&gt;

&lt;h1 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;ll use the same environment we&amp;rsquo;ve setup in the &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;previous posts&lt;/a&gt;, consisting of a CSR1K and a Linux VM connected to the same network inside my hypervisor. IOS XE device needs to have &lt;code&gt;netconf-yang&lt;/code&gt; configured in order to enable the northbound NETCONF interface.&lt;/p&gt;

&lt;p&gt;On the same Linux VM, I&amp;rsquo;ve downloaded and launched the latest version of ODL (Boron-SR2), and enabled NETCONF and RESTCONF plugins.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;unzip distribution-karaf-0.5.2-Boron-SR2.zip
mv distribution-karaf-0.5.2-Boron-SR2 odl-0.5.2
cd odl-0.5.2/
./bin/karaf
opendaylight-user@root&amp;gt;feature:install odl-netconf-connector-all
opendaylight-user@root&amp;gt;feature:install odl-restconf-all
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ll use NETCONF to connect to Cisco IOS XE device and RESTCONF to interact with ODL from a Linux shell.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/odl-ydk.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It might be useful to turn on logging in karaf console to catch any errors we might encounter later:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;opendaylight-user@root&amp;gt;log:tail
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;connecting-ios-xe-to-odl&#34;&gt;Connecting IOS XE to ODL&lt;/h1&gt;

&lt;p&gt;According to ODL &lt;a href=&#34;http://docs.opendaylight.org/en/stable-boron/user-guide/netconf-user-guide.html&#34; target=&#34;_blank&#34;&gt;NETCONF&lt;/a&gt; user guide, in order to connect a new device to the controller, we need to create an XML document which will include the IP, port and user credentials of the IOS XE device. Here&amp;rsquo;s the excerpt from the &lt;a href=&#34;https://github.com/networkop/yang/blob/master/odl-101/new_device.xml.1&#34; target=&#34;_blank&#34;&gt;full XML document&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;module xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:config&amp;quot;&amp;gt;
  &amp;lt;type xmlns:prefix=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;prefix:sal-netconf-connector&amp;lt;/type&amp;gt;
  &amp;lt;name&amp;gt;CSR1K&amp;lt;/name&amp;gt;
  &amp;lt;address xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;192.168.145.51&amp;lt;/address&amp;gt;
  &amp;lt;port xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;830&amp;lt;/port&amp;gt;
  &amp;lt;username xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;admin&amp;lt;/username&amp;gt;
  &amp;lt;password xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;admin&amp;lt;/password&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming this XML is saved in a file called &lt;a href=&#34;https://github.com/networkop/yang/blob/master/odl-101/new_device.xml.1&#34; target=&#34;_blank&#34;&gt;new_device.xml.1&lt;/a&gt;, we can use &lt;code&gt;curl&lt;/code&gt; to send it to ODL&amp;rsquo;s netconf-connector plugin:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k -u admin:admin -H &amp;quot;Content-Type: application/xml&amp;quot; -X POST \
 http://localhost:8181/restconf/config/network-topology:network-topology\
 /topology/topology-netconf/node/controller-config/yang-ext:mount/config:modules\
  -d @new_device.xml.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the controller gets this information it will try to connect to the device via NETCONF and do the following three things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Discover device capabilities advertised in the Hello message&lt;/li&gt;
&lt;li&gt;Download all YANG models advertised by the device into the &lt;code&gt;./cache/schema&lt;/code&gt; directory&lt;/li&gt;
&lt;li&gt;Go through all of the imports in each model and verify that they can be satisfied&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After ODL downloads all of the 260 available models (can take up to 20 minutes) we will see the following errors in the karaf console:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Netconf device does not provide all yang models reported in hello message capabilities
Unable to build schema context, unsatisfied imports
Initialization in sal failed, disconnecting from device
No more sources for schema context
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Due to inconsistencies between the advertised and the available models, ODL fails to build the full device YANG schema context, which ultimately results in inability to connect the device to the controller. However, we won&amp;rsquo;t need all of the 260 models advertised by the device. In fact, most of the configuration can be done through a single Cisco native YANG model, &lt;code&gt;ned&lt;/code&gt;. With ODL it is possible to override the default capabilities advertised in the Hello message and &amp;ldquo;pin&amp;rdquo; only the ones that are going to be used. Assuming that ODL has downloaded most of the models at the previous step, we can simply tell it use the selected few with the following additions to the &lt;a href=&#34;https://github.com/networkop/yang/blob/master/odl-101/new_device.xml.2&#34; target=&#34;_blank&#34;&gt;XML document&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;yang-module-capabilities xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;
    &amp;lt;override&amp;gt;true&amp;lt;/override&amp;gt;
    &amp;lt;capability xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;
      urn:ietf:params:xml:ns:yang:ietf-inet-types?module=ietf-inet-types&amp;amp;amp;revision=2013-07-15
    &amp;lt;/capability&amp;gt;
    &amp;lt;capability xmlns=&amp;quot;urn:opendaylight:params:xml:ns:yang:controller:md:sal:connector:netconf&amp;quot;&amp;gt;
      http://cisco.com/ns/yang/ned/ios?module=ned&amp;amp;amp;revision=2016-10-24
    &amp;lt;/capability&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming the updated XML is saved in &lt;a href=&#34;https://github.com/networkop/yang/blob/master/odl-101/new_device.xml.2&#34; target=&#34;_blank&#34;&gt;new_device.xml.2&lt;/a&gt; file, the following command will update the current configuration of &lt;strong&gt;CSR1K&lt;/strong&gt; device:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k -u admin:admin -H &amp;quot;Content-Type: application/xml&amp;quot; -X PUT \
http://localhost:8181/restconf/config/network-topology:network-topology\
/topology/topology-netconf/node/controller-config\
/yang-ext:mount/config:modules/module\
/odl-sal-netconf-connector-cfg:sal-netconf-connector\
/CSR1K -d @new_device.xml.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then verify that the device has been successfully mounted to the controller:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k -u admin:admin http://localhost:8181/restconf/operational\
/network-topology:network-topology/ | python -m json.tool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output should look similar to the following with the connection-status set to &lt;code&gt;connected&lt;/code&gt; and no detected &lt;code&gt;unavailable-capabilities&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;netconf-node-topology:connection-status&amp;quot;: &amp;quot;connected&amp;quot;,
&amp;quot;netconf-node-topology:host&amp;quot;: &amp;quot;192.168.145.51&amp;quot;,
&amp;quot;netconf-node-topology:port&amp;quot;: 830,
&amp;quot;netconf-node-topology:unavailable-capabilities&amp;quot;: {},
&amp;quot;node-id&amp;quot;: &amp;quot;CSR1K&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point we should be able to interact with IOS XE&amp;rsquo;s native YANG model through ODL&amp;rsquo;s RESTCONF interface using the following URL&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; http://localhost:8181/restconf/config/network-topology:network-topology\
 /topology/topology-netconf/node/CSR1K/yang-ext:mount/ned:native
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only thing that&amp;rsquo;s missing is the actual configuration data. To generate it, I&amp;rsquo;ll use a new open-source tool called YDK.&lt;/p&gt;

&lt;h1 id=&#34;ydk-primer&#34;&gt;YDK primer&lt;/h1&gt;

&lt;p&gt;Yang Development Kit is a suite of tools to work with NETCONF/RESTCONF interfaces of a network device. The way I see it, YDK accomplishes two things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Generates API bindings for programming languages (Python and C++) from YANG models&lt;/li&gt;
&lt;li&gt;Creates an abstraction layer to interact with southbound protocols (NETCONF or RESTCONF) in a uniform way&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There&amp;rsquo;s a lot of overlap between the tools that we&amp;rsquo;ve used &lt;a href=&#34;https://networkop.co.uk/blog/2017/02/15/restconf-yang/&#34;&gt;before&lt;/a&gt; and YDK. Effectively YDK combines in itself the functions of a NETCONF client, a REST client, pyangbind and pyang(the latter is used internally for model verification). Since one of the main functions of YDK is API generation I thought it&amp;rsquo;d be interesting to know how it compares to Rob Shakir&amp;rsquo;s &lt;strong&gt;pyangbind&lt;/strong&gt; plugin. The following information is what I&amp;rsquo;ve managed to find on the Internet and from the comment of Santiago Alvarez below:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Pyangbind&lt;/th&gt;
&lt;th&gt;YDK&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;PL support&lt;/td&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td&gt;Python, C++ with Ruby and Go in the pipeline&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Serialization&lt;/td&gt;
&lt;td&gt;JSON, XML&lt;/td&gt;
&lt;td&gt;only XML &lt;a href=&#34;https://github.com/CiscoDevNet/ydk-gen/blob/master/sdk/python/core/ydk/providers/codec_provider.py#L53&#34; target=&#34;_blank&#34;&gt;at this stage&lt;/a&gt; with JSON coming up in a few weeks&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Southbound interfaces&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;NETCONF, RESTCONF with ODL coming up in a few weeks&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Support&lt;/td&gt;
&lt;td&gt;Cisco&amp;rsquo;s devnet team&lt;/td&gt;
&lt;td&gt;Rob Shakir&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So it looks like YDK is a very promising alternative to &lt;strong&gt;pyangbind&lt;/strong&gt;, however I, personally, would still prefer to use &lt;strong&gt;pyangbind&lt;/strong&gt; due to familiarity, simplicity and the fact that I don&amp;rsquo;t need the above extra features offered by YDK right now. However, given that YDK has been able to achieve so much in just under one year of its existence, I don&amp;rsquo;t discount the possibility that I may switch to YDK as it becomes more mature and feature-rich.&lt;/p&gt;

&lt;h1 id=&#34;python-binding-generation-with-ydk-gen&#34;&gt;Python binding generation with YDK-GEN&lt;/h1&gt;

&lt;p&gt;One of the first things we need to do is install YDK-GEN, the tools responsible for API bindings generation, and it&amp;rsquo;s core Python packages on the local machine. The following few commands are my version of the official &lt;a href=&#34;https://github.com/CiscoDevNet/ydk-gen&#34; target=&#34;_blank&#34;&gt;installation procedure&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/CiscoDevNet/ydk-gen.git ~/ydk-gen
pip install -r ~/ydk-gen/requirements.txt
export YDKGEN_HOME=~/ydk-gen/
~/ydk-gen/generate.py --python --core
pip install ~/ydk-gen/gen-api/python/ydk/dist/ydk*.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;YDK-GEN generates Python bindings based on the so-called &lt;strong&gt;bundle profile&lt;/strong&gt;. This is a simple JSON document which lists all YANG models to include in the output package. In our case we&amp;rsquo;d need to include a &lt;code&gt;ned&lt;/code&gt; model along with all its imports. The sample below shows only the model specification. Refer to my &lt;a href=&#34;https://github.com/networkop/yang/blob/master/odl-101/cisco-ios-xe_0_1_0.json&#34; target=&#34;_blank&#34;&gt;Github repo&lt;/a&gt; for a complete bundle profile for Cisco IOS XE native YANG model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;models&amp;quot;:{&amp;quot;git&amp;quot;:[{&amp;quot;url&amp;quot;:&amp;quot;https://github.com/YangModels/yang.git&amp;quot;,
  &amp;quot;commits&amp;quot;:[{&amp;quot;commitid&amp;quot;:&amp;quot;6f4a025431103f8cbbf3405ce01bdc61d0811b1d&amp;quot;,
    &amp;quot;file&amp;quot;:[&amp;quot;vendor/cisco/xe/1641/ned.yang&amp;quot;,
      &amp;quot;vendor/cisco/xe/1641/tailf-common.yang&amp;quot;,
      &amp;quot;vendor/cisco/xe/1641/tailf-meta-extensions.yang&amp;quot;,
      &amp;quot;vendor/cisco/xe/1641/tailf-cli-extensions.yang&amp;quot;,
      &amp;quot;standard/ietf/RFC/ietf-inet-types.yang&amp;quot;,
      &amp;quot;standard/ietf/RFC/ietf-yang-types.yang&amp;quot;]
      }]}]}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming that the IOS XE bundle profile is saved in a file called &lt;a href=&#34;https://github.com/networkop/yang/blob/master/odl-101/cisco-ios-xe_0_1_0.json&#34; target=&#34;_blank&#34;&gt;cisco-ios-xe_0_1_0.json&lt;/a&gt;, we can use YDK to generate and install the Python binding package:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~/ydk-gen/generate.py --python --bundle cisco-ios-xe_0_1_0.json -v
pip install ~/ydk-gen/gen-api/python/cisco_ios_xe-bundle/dist/ydk*.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;configuring-bgp-with-ydk&#34;&gt;Configuring BGP with YDK&lt;/h1&gt;

&lt;p&gt;Now we can start configuring BGP using our newly generated Python package. First, we need to create an instance of BGP configuration data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ydk.models.cisco_ios_xe.ned import Native
bgp = Native().router.Bgp()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The configuration will follow the pattern defined in the original model, which is why it&amp;rsquo;s important to understand &lt;a href=&#34;https://networkop.co.uk/blog/2017/02/15/restconf-yang/&#34;&gt;the internal structure&lt;/a&gt; of a YANG model. YANG leafs are represented as simple instance attributes. All YANG containers need to be explicitly instantiated, just like the &lt;code&gt;Native&lt;/code&gt; and &lt;code&gt;Bgp&lt;/code&gt; classes in the example above. Presence containers (&lt;code&gt;router&lt;/code&gt; in the above example) will be instantiated at the same time as its parent container, inside the &lt;code&gt;__init__&lt;/code&gt; function of the &lt;code&gt;Native&lt;/code&gt; class. Don&amp;rsquo;t worry if this doesn&amp;rsquo;t make sense, use &lt;strong&gt;iPython&lt;/strong&gt; or any IDE with autocompletion and after a few tries, you&amp;rsquo;ll get the hang of it.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how we can set the local BGP AS number and add a new BGP peer to the neighbor list.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bgp.id = 100
new_neighbor = bgp.Neighbor()
new_neighbor.id = &#39;2.2.2.2&#39;
new_neighbor.remote_as = 65100
bgp.neighbor.append(new_neighbor)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point of time all data is stored inside the instance of a &lt;code&gt;Bgp&lt;/code&gt; class. In order to get an XML representation of it, we need to use YDK&amp;rsquo;s XML provider and encoding service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ydk.providers import CodecServiceProvider
from ydk.services import CodecService
provider = CodecServiceProvider(type=&amp;quot;xml&amp;quot;)
codec = CodecService()
xml_string = codec.encode(provider, bgp)
print xml_string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All what we&amp;rsquo;ve got left now is to send the data to ODL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests
url = (&amp;quot;http://localhost:8181/restconf&amp;quot;
       &amp;quot;/config/network-topology:network-topology&amp;quot;
       &amp;quot;/topology/topology-netconf/node&amp;quot;
       &amp;quot;/CSR1K/yang-ext:mount/ned:native&amp;quot;
       &amp;quot;/router&amp;quot;)
headers = {&#39;Content-Type&#39;: &#39;application/xml&#39;}
result = requests.post(url, auth=(&#39;admin&#39;, &#39;admin&#39;), headers=headers, data=xml_string)
print result.status_code
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The controller should have returned the status code &lt;code&gt;204 No Content&lt;/code&gt;, meaning that configuration has been changed successfully.&lt;/p&gt;

&lt;h1 id=&#34;verification&#34;&gt;Verification&lt;/h1&gt;

&lt;p&gt;Back at the IOS XE CLI we can see the new BGP configuration that has been pushed down from the controller.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TEST#sh run | i router
router bgp 100
 bgp log-neighbor-changes
 neighbor 2.2.2.2 remote-as 65100
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;more-examples&#34;&gt;More examples&lt;/h1&gt;

&lt;p&gt;You can find a shorter version of the above procedure in my &lt;a href=&#34;https://github.com/networkop/yang/tree/master/odl-101&#34; target=&#34;_blank&#34;&gt;ODL 101 repo&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to YANG Programming and RESTCONF on Cisco IOS XE</title>
      <link>https://networkop.co.uk/blog/2017/02/15/restconf-yang/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2017/02/15/restconf-yang/</guid>
      <description>

&lt;p&gt;In the &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;previous post&lt;/a&gt; I have demonstrated how to make changes to interface configuration of Cisco IOS XE device using the standard &lt;strong&gt;IETF&lt;/strong&gt; model. In this post I&amp;rsquo;ll show how to use Cisco&amp;rsquo;s &lt;strong&gt;native&lt;/strong&gt; YANG model to modify static IP routes. To make things even more interesting I&amp;rsquo;ll use RESTCONF, an HTTP-based sibling of NETCONF.&lt;/p&gt;

&lt;h1 id=&#34;restconf-primer&#34;&gt;RESTCONF primer&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://www.rfc-editor.org/rfc/rfc8040.txt&#34; target=&#34;_blank&#34;&gt;RESTCONF&lt;/a&gt; is a very close functional equivalent of NETCONF. Instead of SSH, RESTCONF relies on HTTP to interact with configuration data and operational state of the network device and encodes all exchanged data in either XML or JSON. RESTCONF borrows the idea of Create-Read-Update-Delete operations on resources from &lt;a href=&#34;https://networkop.co.uk/blog/2016/01/01/rest-for-neteng/&#34;&gt;REST&lt;/a&gt; and maps them to YANG models and datastores. There is a direct relationship between NETCONF operations and RESTCONF HTTP verbs:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;HTTP VERB&lt;/th&gt;
&lt;th&gt;NETCONF OPERATION&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;POST&lt;/td&gt;
&lt;td&gt;create&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;PUT&lt;/td&gt;
&lt;td&gt;replace&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;PATCH&lt;/td&gt;
&lt;td&gt;merge&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DELETE&lt;/td&gt;
&lt;td&gt;delete&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;GET&lt;/td&gt;
&lt;td&gt;get/get-config&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Both RESTfullness and the ability to encode data as JSON make RESTCONF a very attractive choice for application developers. In this post, for the sake of simplicity, we&amp;rsquo;ll use Python CLI and &lt;code&gt;curl&lt;/code&gt; to interact with RESTCONF API. In the upcoming posts I&amp;rsquo;ll show how to implement the same functionality inside a simple Python library.&lt;/p&gt;

&lt;h1 id=&#34;environment-setup&#34;&gt;Environment setup&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;ll pick up from where we left our environment in the &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;previous post&lt;/a&gt; right after we&amp;rsquo;ve configured a network interface. The following IOS CLI command enables RESTCONF&amp;rsquo;s root URL at &lt;code&gt;http://192.168.145.51/restconf/api/&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CSR1k(config)#restconf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can start exploring the structure of RESTCONF interface starting at the root URL by specifying resource names separated by &amp;ldquo;/&amp;rdquo;. For example, the following command will return all configuration from Cisco&amp;rsquo;s native datastore.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k admin:admin http://192.168.145.51/restconfi/api/config/native?deep
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to get JSON instead of the default XML output the client should specify JSON media type &lt;code&gt;application/vnd.yang.datastore+json&lt;/code&gt; and pass it in the &lt;code&gt;Accept&lt;/code&gt; header.&lt;/p&gt;

&lt;h1 id=&#34;writing-a-yang-model&#34;&gt;Writing a YANG model&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;Normally&lt;/a&gt;, you would expect to download the YANG model from the device itself. However IOS XE&amp;rsquo;s NETCONF and RESTCONF support is so new that not all of the models are available. Specifically, Cisco&amp;rsquo;s native YANG model for static routing cannot be found in either &lt;a href=&#34;https://github.com/YangModels&#34; target=&#34;_blank&#34;&gt;Yang Github Repo&lt;/a&gt; or the device itself (via &lt;code&gt;get_schema&lt;/code&gt; RPC), which makes it a very good candidate for this post.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Update 13-02-2017&lt;/strong&gt;: As it turned out, the model was right under my nose the whole time. It&amp;rsquo;s called &lt;code&gt;ned&lt;/code&gt; and encapsulates the whole of Cisco&amp;rsquo;s native datastore. So think of everything that&amp;rsquo;s to follow as a simple learning exercise, however the point I raise in the closing paragraph still stands.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The first thing we need to do is get an understanding of the structure and naming convention of the YANG model. The simplest way to do that would be to make a change on the CLI and observe the result via RESTCONF.&lt;/p&gt;

&lt;h2 id=&#34;retrieving-running-configuration-data&#34;&gt;Retrieving running configuration data&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start by adding the following static route to the IOS XE device:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip route 2.2.2.2 255.255.255.255 GigabitEthernet2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can view the configured static route via RESTCONF:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k -u admin:admin -H &amp;quot;Accept: application/vnd.yang.data+json&amp;quot; \
 http://192.168.145.51/restconf/api/config/native/ip/route?deep
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The returned output should look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{ &amp;quot;ned:route&amp;quot;: {
    &amp;quot;ip-route-interface-forwarding-list&amp;quot;: [
      { &amp;quot;prefix&amp;quot;: &amp;quot;2.2.2.2&amp;quot;,
        &amp;quot;mask&amp;quot;: &amp;quot;255.255.255.255&amp;quot;,
        &amp;quot;fwd-list&amp;quot;: [ { &amp;quot;fwd&amp;quot;: &amp;quot;GigabitEthernet2&amp;quot; } ]
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This JSON object gives us a good understanding of how the YANG model should look like. The root element &lt;code&gt;route&lt;/code&gt; contains a list of IP prefixes, called &lt;code&gt;ip-route-interface-forwarding-list&lt;/code&gt;. Each element of this list contains values for IP network and mask as well as the list of next-hops called &lt;code&gt;fwd-list&lt;/code&gt;. Let&amp;rsquo;s see how we can map this to YANG model concepts.&lt;/p&gt;

&lt;h2 id=&#34;building-a-simple-yang-model&#34;&gt;Building a simple YANG model&lt;/h2&gt;

&lt;p&gt;YANG &lt;a href=&#34;https://tools.ietf.org/html/rfc6020&#34; target=&#34;_blank&#34;&gt;RFC&lt;/a&gt; defines a number of data structures to model an XML tree. Let&amp;rsquo;s first concentrate on the three most fundamental data structures that constitute the biggest part of any YANG model:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Container&lt;/strong&gt; is a node of a tree with a unique name which encloses a set of child elements. In JSON it is mapped to a name/object pair &lt;code&gt;&#39;name&#39;: {...}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leaf&lt;/strong&gt; is a node which contains a value and does not contain any child elements. In JSON leaf is mapped to a single key/value pair &lt;code&gt;&#39;name&#39;: &#39;value&#39;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;List&lt;/strong&gt; can be thought of as a table that contains a set rows (list entries). Each list entry can contain Leafs, Containers and other elements and can be uniquely identified by at least one Leaf element called a &lt;code&gt;key&lt;/code&gt;. In JSON lists are encoded as name/arrays pairs containing JSON objects &lt;code&gt;&#39;name&#39;: [{...}, {...}]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now let&amp;rsquo;s see how we can describe the received data in terms of the above data structures:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The value of the topmost &lt;code&gt;route&lt;/code&gt; element is a JSON object, therefore it can only be mapped to a YANG container.&lt;/li&gt;
&lt;li&gt;The value of &lt;code&gt;ip-route-interface-forwarding-list&lt;/code&gt; is an array of JSON objects, therefore it must be a list.&lt;/li&gt;
&lt;li&gt;The only entry of this list contains &lt;code&gt;prefix&lt;/code&gt; and &lt;code&gt;mask&lt;/code&gt; key/value pairs. Since they don&amp;rsquo;t contain any child elements and their values are strings they can only be mapped to YANG leafs.&lt;/li&gt;
&lt;li&gt;The third element, &lt;code&gt;fwd-list&lt;/code&gt;, is another YANG list and so far contains a single next-hop value inside a YANG leaf called &lt;code&gt;fwd&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Finally, since &lt;code&gt;fwd&lt;/code&gt; is the only leaf in the &lt;code&gt;fwd-list&lt;/code&gt; list, it must be that lists&amp;rsquo; key. The &lt;code&gt;ip-route-interface-forwarding-list&lt;/code&gt; list will have both &lt;code&gt;prefix&lt;/code&gt; and &lt;code&gt;mask&lt;/code&gt; as its key values since their combination represents a unique IP destination.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With all that in mind, this is how a skeleton of our YANG model will look like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;module cisco-route-static {
  namespace &amp;quot;http://cisco.com/ns/yang/ned/ios&amp;quot;;
  prefix ned;
  container route {
    list ip-route-interface-forwarding-list {
      key &amp;quot;prefix mask&amp;quot;;
      leaf prefix { type string; }
      leaf mask { type string; }
      list fwd-list {
        key &amp;quot;fwd&amp;quot;;
        leaf fwd { type string; }
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;YANG&amp;rsquo;s syntax is pretty light-weight and looks very similar to JSON. The topmost &lt;code&gt;module&lt;/code&gt; defines the model&amp;rsquo;s name and encloses all other elements. The first two statements are used to define XML namespace and prefix that I&amp;rsquo;ve described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;refactoring-a-yang-model&#34;&gt;Refactoring a YANG model&lt;/h2&gt;

&lt;p&gt;At this stage the model can already be instantiated by &lt;strong&gt;pyang&lt;/strong&gt; and &lt;strong&gt;pyangbind&lt;/strong&gt;, however there&amp;rsquo;s a couple of very important changes and additions that I wanted to make to demonstrate some of the other features of YANG.&lt;/p&gt;

&lt;p&gt;The first of them is common IETF data types. So far in our model we&amp;rsquo;ve assumed that prefix and mask can take &lt;strong&gt;any&lt;/strong&gt; value in string format. But what if we wanted to check that the values we use are, in fact, the correctly-formatted IPv4 addresses and netmasks before sending them to the device? That is where IETF common data types come to the rescue. All what we need to do is add an import statement to define which model to use and we can start referencing them in our type definitions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;...
import ietf-yang-types { prefix &amp;quot;yang&amp;quot;; }
import ietf-inet-types { prefix &amp;quot;inet&amp;quot;; }
...
leaf prefix { type inet:ipv4-address; }
leaf mask { type yang:dotted-quad; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This solves the problem for the prefix part of a static route but how about its next-hop? Next-hops can be defined as either strings (representing an interface name) or IPv4 addresses. To make sure we can use either of these two types in the &lt;code&gt;fwd&lt;/code&gt; leaf node we can define its type as a &lt;code&gt;union&lt;/code&gt;. This built-in type is literally a union, a logical OR, of all its member elements. This is how we can change the &lt;code&gt;fwd&lt;/code&gt; leaf definition:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;...
typedef ip-next-hop {
  type union {
    type inet:ipv4-address;
    type string;
  }
}
...
leaf fwd { type ip-next-hop; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So far we&amp;rsquo;ve been concentrating on the simplest form of a static route, which doesn&amp;rsquo;t include any of the optional arguments. Let&amp;rsquo;s add the leaf nodes for name, AD, tag, track and permanent options of the static route:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;...
leaf metric { type uint8; }
leaf name { type string; }
leaf tag { type uint8; }
leaf track { type uint8; }
leaf permanent { type empty; }
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since &lt;strong&gt;track&lt;/strong&gt; and &lt;strong&gt;permanent&lt;/strong&gt; options are mutually exclusive they should not appear in the configuration at the same time. To model that we can use the &lt;code&gt;choice&lt;/code&gt; YANG statement. Let&amp;rsquo;s remove the &lt;strong&gt;track&lt;/strong&gt; and &lt;strong&gt;permanent&lt;/strong&gt; leafs from the model and replace them with this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;choice track-or-perm {
  leaf track { type uint8; }
  leaf permanent { type empty; }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally, we need to add an options for VRF. When VRF is defined the whole &lt;code&gt;ip-route-interface-forwarding-list&lt;/code&gt; gets encapsulated inside a list called &lt;code&gt;vrf&lt;/code&gt;. This list has just one more leaf element &lt;code&gt;name&lt;/code&gt; which plays the role of this lists&amp;rsquo; key. In order to model this we can use another oft-used YANG concept called &lt;code&gt;grouping&lt;/code&gt;. I like to think of it as a Python function, a reusable part of code that can be referenced multiple times by its name. Here are the final changes to our model to include the VRF support:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;grouping ip-route-list {
  list ip-route-interface-forwarding-list {
      ...
  }
}
grouping vrf-grouping {
  list vrf {
    key &amp;quot;name&amp;quot;;
    leaf name { type string; }
    uses ip-route-list;
  }
}
container route {
  uses vrf-grouping;
  uses ip-route-list;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each element in a YANG model is optional by default, which means that the &lt;code&gt;route&lt;/code&gt; container can include any number of VRF and non-VRF routes. The full YANG model can be found &lt;a href=&#34;https://github.com/networkop/yang/blob/master/yang-101/cisco-route-static.yang&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;modifying-static-route-configuration&#34;&gt;Modifying static route configuration&lt;/h1&gt;

&lt;p&gt;Now let me demonstrate how to use our newly built YANG model to change the next-hop of an existing static route. Using &lt;a href=&#34;https://github.com/mbj4668/pyang&#34; target=&#34;_blank&#34;&gt;pyang&lt;/a&gt; we need to generate a Python module based on the YANG model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pyang --plugindir $PYBINDPLUGIN -f pybind -o binding.py cisco-route-static.yang
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From a Python shell, download the current static IP route configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests
url = &amp;quot;http://{h}:{p}/restconf/api/config/native/ip/route?deep&amp;quot;.format(h=&#39;192.168.145.51&#39;, p=&#39;80&#39;)
headers = {&#39;accept&#39;: &#39;application/vnd.yang.data+json&#39;}
result = requests.get(url, auth=(&#39;admin&#39;, &#39;admin&#39;), headers=headers)
current_json = result.text
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Import the downloaded JSON into a YANG model instance:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import binding
import pyangbind.lib.pybindJSON as pybindJSON
model = pybindJSON.loads_ietf(current_json, binding, &amp;quot;cisco_route_static&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Delete the old next-hop and replace it with &lt;strong&gt;12.12.12.2&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;route = model.route.ip_route_interface_forwarding_list[&amp;quot;2.2.2.2 255.255.255.255&amp;quot;]
route.fwd_list.delete(&amp;quot;GigabitEthernet2&amp;quot;)
route.fwd_list.add(&amp;quot;12.12.12.2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save the updated model in a JSON file with the help of a &lt;a href=&#34;https://github.com/networkop/yang/blob/master/yang-101/helpers.py&#34; target=&#34;_blank&#34;&gt;write_file&lt;/a&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;json_data = pybindJSON.dumps(model, mode=&#39;ietf&#39;)
write_file(&#39;new_conf.json&#39;, json_data)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;updating-running-configuration&#34;&gt;Updating running configuration&lt;/h1&gt;

&lt;p&gt;If we tried sending the &lt;code&gt;new_conf.json&lt;/code&gt; file now, the device would have responded with an error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;missing element: prefix in /ios:native/ios:ip/ios:route/ios:ip-route-interface-forwarding-list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In our JSON file the order of elements inside a JSON object can be different from what was defined in the YANG model. This is expected since one of the fundamental principles of JSON is that an object is an &lt;strong&gt;unordered&lt;/strong&gt; collection of name/value pairs. However it looks like behind the scenes IOS XE converts JSON to XML before processing and expects all elements to come in a strict, predefined order. Fortunately, this &lt;a href=&#34;https://github.com/CiscoDevNet/openconfig-getting-started/issues/4&#34; target=&#34;_blank&#34;&gt;bug&lt;/a&gt; is already known and we can hope that Cisco will implement the fix for IOS XE soon. In the meantime, we&amp;rsquo;re gonna have to resort to sending XML.&lt;/p&gt;

&lt;p&gt;Following the procedure described in my &lt;a href=&#34;https://networkop.co.uk/blog/2017/01/25/netconf-intro/&#34;&gt;previous post&lt;/a&gt;, we can use &lt;strong&gt;json2xml&lt;/strong&gt; tool to convert our instance into an XML document. Here we hit another issue. Since &lt;strong&gt;json2xml&lt;/strong&gt; was designed to produce a NETCONF-compliant XML, it wraps the payload inside a &lt;strong&gt;data&lt;/strong&gt; or a &lt;strong&gt;config&lt;/strong&gt; element. Thankfully, &lt;strong&gt;json2xml&lt;/strong&gt; is a Python script and can be easily patched to produce a RESTCONF-compliant XML. The following is a diff between the original and the patched files&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;408c409
&amp;lt;     if args.target not in [&amp;quot;data&amp;quot;, &amp;quot;config&amp;quot;]:
+++
&amp;gt;     if args.target not in [&amp;quot;data&amp;quot;, &amp;quot;config&amp;quot;, &amp;quot;restconf&amp;quot;]:
437c438,442
&amp;lt;     ET.ElementTree(root_el).write(outfile, encoding=&amp;quot;utf-8&amp;quot;, xml_declaration=True)
+++
&amp;gt;     if args.target != &#39;restconf&#39;:
&amp;gt;         ET.ElementTree(root_el).write(outfile, encoding=&amp;quot;utf-8&amp;quot;, xml_declaration=True)
&amp;gt;     else:
&amp;gt;         ET.ElementTree(list(root_el)[0]).write(outfile, encoding=&amp;quot;utf-8&amp;quot;, xml_declaration=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead of patching the original file, I&amp;rsquo;ve applied the above changes to a local copy of the file. Once patched, the following commands should produce the needed XML.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pyang -f jtox -o static-route.jtox cisco-route-static.yang
./json2xml -t restconf -o new_conf.xml static-route.jtox new_conf.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final step would be to send the generated XML to the IOS XE device. Since we are replacing the old static IP route configuration we&amp;rsquo;re gonna have to use HTTP PUT to overwrite the old data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -k -u admin:admin -H &amp;quot;Content-Type: application/vnd.yang.data+xml&amp;quot; \
 -X PUT http://192.168.145.51/restconf/api/config/native/ip/route/ -d @new_conf.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;verification&#34;&gt;Verification&lt;/h1&gt;

&lt;p&gt;Back at the IOS XE CLI we can see the new static IP route installed.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TEST#sh run | i ip route
ip route 2.2.2.2 255.255.255.255 12.12.12.2
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;more-examples&#34;&gt;More examples&lt;/h1&gt;

&lt;p&gt;As always there are more examples available in my &lt;a href=&#34;https://github.com/networkop/yang/tree/master/yang-101&#34; target=&#34;_blank&#34;&gt;YANG 101 repo&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The exercise we&amp;rsquo;ve done in this post, though useful from a learning perspective, can come in very handy when dealing with vendors who forget or simply don&amp;rsquo;t want to share their YANG models with their customers (I know of at least one vendor that would only publish tree representations of their YANG models). In the upcoming posts I&amp;rsquo;ll show how to create a simple Python library to program static routes via RESTCONF and finally how to build an Ansible module to do that.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
