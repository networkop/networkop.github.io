<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>networkop on networkop</title>
    <link>https://networkop.co.uk/</link>
    <description>Recent content in networkop on networkop</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Michael Kashin 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Self-hosted external DNS resolver for Kubernetes</title>
      <link>https://networkop.co.uk/post/2020-08-k8s-gateway/</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2020-08-k8s-gateway/</guid>
      <description>

&lt;p&gt;There comes a time in the life of every Kubernetes cluster when internal resources (pods, deployments) need to be exposed to the outside world. Doing so from a pure IP connectivity perspective is relatively easy as most of the constructs come baked-in (e.g. NodePort-type Services) or can be enabled with an off-the-shelf add-on (e.g. Ingress and LoadBalancer controllers). In this post, we&amp;rsquo;ll focus on one crucial piece of network connectivity which glues together the dynamically-allocated external IP with a static customer-defined hostname — a DNS. We&amp;rsquo;ll examine the pros and cons of various ways of implementing external DNS in Kubernetes and introduce a new CoreDNS plugin that can be used for dynamic discovery and resolution of multiple types of external Kubernetes resources.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/d11.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;external-kubernetes-resources&#34;&gt;External Kubernetes Resources&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start by reviewing various types of &amp;ldquo;external&amp;rdquo; Kubernetes resources and the level of networking abstraction they provide starting from the lowest all the way to the highest level.&lt;/p&gt;

&lt;p&gt;One of the most fundamental building block of all things external in Kubernetes is the &lt;strong&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/#nodeport&#34; target=&#34;_blank&#34;&gt;NodePort&lt;/a&gt;&lt;/strong&gt; service. It works by allocating a unique external port for every service instance and setting up kube-proxy to deliver incoming packets from that port to the one of the healthy backend pods. This service is rarely used on its own and was designed to be a building block for other higher-level resources.&lt;/p&gt;

&lt;p&gt;Next level up is the &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;LoadBalancer&lt;/strong&gt;&lt;/a&gt; service which is one of the most common ways of exposing services externally. This service type requires an extra controller that will be responsible for IP address allocation and delivering traffic to the Kubernetes nodes. This function can be implemented by cloud load-balancers, in case the cluster is deployed one of the public clouds, a physical appliance or a cluster add-on like &lt;a href=&#34;https://github.com/metallb/metallb&#34; target=&#34;_blank&#34;&gt;MetalLB&lt;/a&gt; or &lt;a href=&#34;https://github.com/kubesphere/porter&#34; target=&#34;_blank&#34;&gt;Porter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;At the highest level of abstraction is the &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Ingress&lt;/strong&gt;&lt;/a&gt; resource. It, too, requires a dedicated controller which spins up and configures a number of proxy servers that can act as a L7 load-balancer, API gateway or, in some cases, a L4 (TCP/UDP) proxy. Similarly to the LoadBalancer, Ingress may be implemented by one of the public cloud L7 load-balancers or could be self-hosted by the cluster using any one of the &lt;a href=&#34;https://docs.google.com/spreadsheets/d/16bxRgpO1H_Bn-5xVZ1WrR_I-0A-GOI6egmhvqqLMOmg/edit#gid=1612037324&#34; target=&#34;_blank&#34;&gt;open-source ingress controllers&lt;/a&gt;. Amongst other things, Ingress controllers can perform &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/#tls&#34; target=&#34;_blank&#34;&gt;TLS offloading&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/#name-based-virtual-hostinghttps://kubernetes.io/docs/concepts/services-networking/ingress/#name-based-virtual-hosting&#34; target=&#34;_blank&#34;&gt;named-based routing&lt;/a&gt; which rely heavily on external DNS infrastructure that can dynamically discover Ingress resources as they get added/removed from the cluster.&lt;/p&gt;

&lt;p&gt;There are other external-ish resources like &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34; target=&#34;_blank&#34;&gt;ExternalName&lt;/a&gt; services and even ClusterIP in &lt;a href=&#34;https://docs.projectcalico.org/networking/advertise-service-ips&#34; target=&#34;_blank&#34;&gt;certain cases&lt;/a&gt;. They represent a very small subset of corner case scenarios and are considered outside of the scope of this article. Instead, we&amp;rsquo;ll focus on the two most widely used external resources—LoadBalancers and Ingresses, and see how they can be integrated into the public DNS infrastructure.&lt;/p&gt;

&lt;h2 id=&#34;externaldns&#34;&gt;ExternalDNS&lt;/h2&gt;

&lt;p&gt;The most popular solution today is the &lt;a href=&#34;https://github.com/kubernetes-sigs/external-dns&#34; target=&#34;_blank&#34;&gt;ExternalDNS controller&lt;/a&gt;. It works by integrating with one of the public DNS providers and populates a pre-configured DNS zone with entries extracted from the monitored objects, e.g. Ingress&amp;rsquo;s &lt;code&gt;spec.rules[*].host&lt;/code&gt; or Service&amp;rsquo;s &lt;code&gt;external-dns.alpha.kubernetes.io/hostname&lt;/code&gt; annotations. In addition, it natively supports non-standard resources like Istio&amp;rsquo;s Gateway or Contour&amp;rsquo;s IngressRoute which, together with the support for over 15 cloud DNS providers, makes it a default choice for anyone approaching this problem for the first time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/d12.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ExternalDNS is an ideal solution for Kubernetes clusters under a single administrative domain, however, it does have a number of trade-offs that start to manifest themselves when a cluster is shared among multiple tenants:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Multiple DNZ zones require a dedicated ExternalDNS instance per zone.&lt;/li&gt;
&lt;li&gt;Each new zone requires cloud-specific IAM rules to be set up to allow ExternalDNS to make the required changes.&lt;/li&gt;
&lt;li&gt;Unless managing a local cloud DNS, API credentials will need to be stored as a secret inside the cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to the above, ExternalDNS represents another layer of abstraction and complexity outside of the cluster that needs to be considered during maintenance and troubleshooting. Every time the controller fails, there&amp;rsquo;s a possibility of some stale state to be left, accumulating over time and polluting the hosted DNS zone.&lt;/p&gt;

&lt;h2 id=&#34;coredns-s-k8s-external-plugin&#34;&gt;CoreDNS&amp;rsquo;s &lt;code&gt;k8s_external&lt;/code&gt; plugin&lt;/h2&gt;

&lt;p&gt;An alternative approach is to make internal Kubernetes DNS add-on respond to external DNS queries. The prime example of this is the CoreDNS &lt;a href=&#34;https://coredns.io/plugins/k8s_external/&#34; target=&#34;_blank&#34;&gt;k8s_external&lt;/a&gt; plugin. It works by configuring CoreDNS to respond to external queries matching a number of pre-configured domains. For example, the following configuration will allow it to resolve queries for &lt;code&gt;svc2.ns.mydomain.com&lt;/code&gt;, as shown in the diagram above, as well as the &lt;code&gt;svc2.ns.example.com&lt;/code&gt; domain:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;k8s_external mydomain.com example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both queries will return the same set of IP addresses extracted from the &lt;code&gt;.status.loadBalancer&lt;/code&gt; field of the &lt;code&gt;svc2&lt;/code&gt; object.&lt;/p&gt;

&lt;p&gt;These domains will still need to be delegated, which means you will need to expose CoreDNS externally with service type LoadBalancer and update NS records with the provisioned IP address.&lt;/p&gt;

&lt;p&gt;Under the hood, &lt;code&gt;k8s_external&lt;/code&gt; relies on the main &lt;a href=&#34;https://coredns.io/plugins/kubernetes/&#34; target=&#34;_blank&#34;&gt;kubernetes&lt;/a&gt; plugin and simply re-uses information already collected by it. This presents a problem when trying to add extra resources (e.g. Ingresses, Gateways) as these changes will increase the amount of information the main plugin needs to process and will inevitably affect its performance. This is why there&amp;rsquo;s a new plugin now that&amp;rsquo;s designed to absorb and extend the functionality of the &lt;code&gt;k8s_external&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-new-k8s-gateway-coredns-plugin&#34;&gt;The new &lt;code&gt;k8s_gateway&lt;/code&gt; CoreDNS plugin&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ori-edge/k8s_gateway&#34; target=&#34;_blank&#34;&gt;This out-of-tree plugin&lt;/a&gt; is loosely based on the &lt;code&gt;k8s_external&lt;/code&gt; and maintains a similar configuration syntax, however it does contain a few notable differences:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It doesn&amp;rsquo;t rely on any other plugin and uses its own mechanism of Kubernetes object discovery.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s designed to be used alongside (and not replace) an existing internal DNS plugin, be it kube-dns or CoreDNS.&lt;/li&gt;
&lt;li&gt;It doesn&amp;rsquo;t collect or expose any internal cluster IP addresses.&lt;/li&gt;
&lt;li&gt;It supports both LoadBalancer services and Ingresses with an eye on the service API&amp;rsquo;s &lt;a href=&#34;https://github.com/kubernetes-sigs/service-apis/blob/master/examples/basic-http.yaml#L29&#34; target=&#34;_blank&#34;&gt;HTTPRoute&lt;/a&gt; when it becomes available.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/d13.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The way it&amp;rsquo;s designed to be used can be summarised as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The scope of the plugin is controlled by a set of RBAC rules and by default is limited to List/Watch operations on Ingress and Service resources.&lt;/li&gt;
&lt;li&gt;The plugin is &lt;a href=&#34;https://github.com/ori-edge/k8s_gateway#build&#34; target=&#34;_blank&#34;&gt;built&lt;/a&gt; as a CoreDNS binary and run as a deployment.&lt;/li&gt;
&lt;li&gt;This deployment is exposed externally and the required domains are delegated to the address of the external load-balancer.&lt;/li&gt;
&lt;li&gt;Any DNS query that reaches the &lt;code&gt;k8s_gateway&lt;/code&gt; plugin will go through the following stages:

&lt;ul&gt;
&lt;li&gt;First, it will be matched against one of the zones configured for this plugin in the Corefile.&lt;/li&gt;
&lt;li&gt;If there&amp;rsquo;s a hit, the next step is to match it against any of the existing Ingress resources. The lookup is performed against FQDNs configured in &lt;code&gt;spec.rules[*].host&lt;/code&gt; fields of the Ingress.&lt;/li&gt;
&lt;li&gt;At this stage, the result can be returned to the user with IPs collected from the &lt;code&gt;.status.loadBalancer.ingress&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If no matching Ingress was found, the search continues with the Services objects. Since services don&amp;rsquo;t really have domain names, the lookup is performed using the &lt;code&gt;serviceName.namespace&lt;/code&gt; as the key.&lt;/li&gt;
&lt;li&gt;If there&amp;rsquo;s a match, it is returned to the end-user in a similar way, alternatively the plugin responds with &lt;code&gt;NXDOMAIN&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The design of the &lt;code&gt;k8s_gateway&lt;/code&gt; plugin attempts to address some of the issues of other solutions described above, but also brings a number of extra advantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All external DNS entries and associated state are contained within the Kubernetes cluster while the hosted zone only contains a single NS record.&lt;/li&gt;
&lt;li&gt;You get the power and flexibility of the full suite of CoreDNS&amp;rsquo;s &lt;a href=&#34;https://coredns.io/plugins/&#34; target=&#34;_blank&#34;&gt;internal&lt;/a&gt; and &lt;a href=&#34;https://coredns.io/explugins/&#34; target=&#34;_blank&#34;&gt;external&lt;/a&gt; plugins, e.g. you can use ACL to control which source IPs are (not)allowed to make queries.&lt;/li&gt;
&lt;li&gt;Provisioning that doesn&amp;rsquo;t rely on annotations makes it easier to maintain Kubernetes manifests.&lt;/li&gt;
&lt;li&gt;Separate deployment means that internal DNS resolution is not affected in case external DNS becomes overloaded.&lt;/li&gt;
&lt;li&gt;Since API keys are &lt;strong&gt;not&lt;/strong&gt; stored in the cluster, it makes it easier and safer for new tenants to bring their own domain.&lt;/li&gt;
&lt;li&gt;Federated Kubernetes cluster deployments (e.g. using &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-api&#34; target=&#34;_blank&#34;&gt;Cluster API&lt;/a&gt;) become easier as there&amp;rsquo;s only a single entrypoint via the management cluster and each workload cluster can get its own self-hosted subdomain.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/d14.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;k8s_gateway&lt;/code&gt; is developed as an out-of-tree plugin under an open-source license. Community contributions in the form of issues, pull requests and documentation are always &lt;a href=&#34;https://github.com/ori-edge/k8s_gateway&#34; target=&#34;_blank&#34;&gt;welcomed&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Anatomy of the &#34;kubernetes.default&#34;</title>
      <link>https://networkop.co.uk/post/2020-06-kubernetes-default/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2020-06-kubernetes-default/</guid>
      <description>

&lt;p&gt;Every Kubernetes cluster is provisioned with a special service that provides a way for internal applications to talk to the API server. However, unlike the rest of the components that get spun up by default, you won&amp;rsquo;t find the definition of this service in any of the static manifests and this is just one of the many things that make this service unique.&lt;/p&gt;

&lt;h2 id=&#34;the-special-one&#34;&gt;The Special One&lt;/h2&gt;

&lt;p&gt;To make sure we&amp;rsquo;re on the same page, I&amp;rsquo;m talking about this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubect get svc kubernetes -n default
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &amp;lt;none&amp;gt;        443/TCP   161m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This service is unique in many ways. First, as you may have noticed, it always occupies the first available IP in the Cluster CIDR, a.k.a. &lt;code&gt;--service-cluster-ip-range&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Second, this service is invincible, i.e. it will always get re-created, even when it&amp;rsquo;s manually removed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &amp;lt;none&amp;gt;        443/TCP   118s
$ kubectl delete svc kubernetes
service &amp;quot;kubernetes&amp;quot; deleted
$ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &amp;lt;none&amp;gt;        443/TCP   0s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may notice that it comes up with the same ClusterIP, regardless of how many services may already exist in the cluster.&lt;/p&gt;

&lt;p&gt;Third, this service does not have any matching pods, however it does have a fully populated &lt;code&gt;Endpoints&lt;/code&gt; object:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pod --selector component=apiserver --all-namespaces
No resources found
$ kubectl get endpoints kubernetes
NAME         ENDPOINTS                                         AGE
kubernetes   172.18.0.2:6443,172.18.0.3:6443,172.18.0.4:6443   4m16s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This last bit is perhaps the most curious one. How can a service have a list of endpoints when there are no pods that match this service&amp;rsquo;s label selector? This goes against how services controller &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service&#34; target=&#34;_blank&#34;&gt;works&lt;/a&gt;.  Note that this behaviour is true even for managed kubernetes clusters, where the API server is run by the provider (e.g. GKE).&lt;/p&gt;

&lt;p&gt;Finally, the IP and Port of this service get injected into every pod as environment variables:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These values can later be used by k8s controllers to &lt;a href=&#34;https://github.com/kubernetes/client-go/blob/master/tools/clientcmd/client_config.go#L561&#34; target=&#34;_blank&#34;&gt;configure&lt;/a&gt; the client-go&amp;rsquo;s rest interface that is used to establish connectivity to the API server:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func InClusterConfig() (*Config, error) {

  host := os.Getenv(&amp;quot;KUBERNETES_SERVICE_HOST&amp;quot;), 
  port := os.Getenv(&amp;quot;KUBERNETES_SERVICE_PORT&amp;quot;)

  return &amp;amp;Config{
		Host: &amp;quot;https://&amp;quot; + net.JoinHostPort(host, port),
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;controller-of-controllers&#34;&gt;Controller of controllers&lt;/h2&gt;

&lt;p&gt;To find out who&amp;rsquo;s behind this magical service, we need to look at the code for the k/k&amp;rsquo;s &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/master/controller.go&#34; target=&#34;_blank&#34;&gt;master controller&lt;/a&gt;, that is described as the &amp;ldquo;controller manager for the core bootstrap Kubernetes controller loops&amp;rdquo;, meaning it&amp;rsquo;s one of the first controllers that gets spun up by the API server binary. Let&amp;rsquo;s break it down into smaller pieces and see what&amp;rsquo;s going on inside it.&lt;/p&gt;

&lt;p&gt;When the controller is started, it spins up a runner, which is a group of functions that run forever until they receive a stop signal via a channel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Start begins the core controller loops that must exist for bootstrapping
// a cluster.
func (c *Controller) Start() {
  
	c.runner = async.NewRunner(c.RunKubernetesNamespaces, c.RunKubernetesService, repairClusterIPs.RunUntil, repairNodePorts.RunUntil)
	c.runner.Start()
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The most interesting is the second function - &lt;code&gt;RunKubernetesService()&lt;/code&gt;, which is a control loop that constantly updates the default kubernetes service.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// RunKubernetesService periodically updates the kubernetes service
func (c *Controller) RunKubernetesService(ch chan struct{}) {

	if err := c.UpdateKubernetesService(false); err != nil {
		runtime.HandleError(fmt.Errorf(&amp;quot;unable to sync kubernetes service: %v&amp;quot;, err))
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Most of the work is done by the &lt;code&gt;UpdateKubernetesService()&lt;/code&gt;. This function does three things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Creates the &amp;ldquo;default&amp;rdquo; namespace whose name is defined in the &lt;code&gt;metav1.NamespaceDefault&lt;/code&gt; variable.&lt;/li&gt;
&lt;li&gt;Creates/Updates the default kuberentes service.&lt;/li&gt;
&lt;li&gt;Creates/Updates the endpoints resource for this service.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// UpdateKubernetesService attempts to update the default Kube service.
func (c *Controller) UpdateKubernetesService(reconcile bool) error {

	if err := createNamespaceIfNeeded(c.NamespaceClient, metav1.NamespaceDefault); err != nil {
		return err
   }

	if err := c.CreateOrUpdateMasterServiceIfNeeded(kubernetesServiceName, c.ServiceIP, servicePorts, serviceType, reconcile); err != nil {
		return err
	}

	if err := c.EndpointReconciler.ReconcileEndpoints(kubernetesServiceName, c.PublicIP, endpointPorts, reconcile); err != nil {
		return err
	}

	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, the &lt;code&gt;CreateOrUpdateMasterServiceIfNeeded()&lt;/code&gt; function is where the default service is being built. You can see the skeleton of this service&amp;rsquo;s object in the below snippet:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;const kubernetesServiceName = &amp;quot;kubernetes&amp;quot;

// CreateOrUpdateMasterServiceIfNeeded will create the specified service if it
// doesn&#39;t already exist.
func (c *Controller) CreateOrUpdateMasterServiceIfNeeded(serviceName string, serviceIP net.IP, servicePorts []corev1.ServicePort, serviceType corev1.ServiceType, reconcile bool) error {

	svc := &amp;amp;corev1.Service{
		ObjectMeta: metav1.ObjectMeta{
			Name:      serviceName,
			Namespace: metav1.NamespaceDefault,
			Labels:    map[string]string{&amp;quot;provider&amp;quot;: &amp;quot;kubernetes&amp;quot;, &amp;quot;component&amp;quot;: &amp;quot;apiserver&amp;quot;},
		},
		Spec: corev1.ServiceSpec{
			Ports: servicePorts,
			// maintained by this code, not by the pod selector
			Selector:        nil,
			ClusterIP:       serviceIP.String(),
			SessionAffinity: corev1.ServiceAffinityNone,
			Type:            serviceType,
		},
	}

	_, err := c.ServiceClient.Services(metav1.NamespaceDefault).Create(context.TODO(), svc, metav1.CreateOptions{})

	return err
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code above explains why this service can never be completely removed from the cluster - the master controller loop will always recreate it if it&amp;rsquo;s missing, along with its endpoints object. However, this still doesn&amp;rsquo;t explain how the IP for this service is selected nor where the endpoint IPs are coming from. In order to do this, we need to get a deeper look at how the API server builds its runtime configuration.&lt;/p&gt;

&lt;h2 id=&#34;always-the-first&#34;&gt;Always the first&lt;/h2&gt;

&lt;p&gt;One of the interesting qualities of the ClusterIP of the &lt;code&gt;kubernetes.default&lt;/code&gt; is that it always (unless manually overridden) occupies the first IP in the Cluster CIDR. The answer is hidden in the &lt;code&gt;ServiceIPRange()&lt;/code&gt; function of the master controller&amp;rsquo;s &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/master/services.go&#34; target=&#34;_blank&#34;&gt;service.go&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;
func ServiceIPRange(passedServiceClusterIPRange net.IPNet) (net.IPNet, net.IP, error) {

	size := integer.Int64Min(utilnet.RangeSize(&amp;amp;serviceClusterIPRange), 1&amp;lt;&amp;lt;16)
	if size &amp;lt; 8 {
		return net.IPNet{}, net.IP{}, fmt.Errorf(&amp;quot;the service cluster IP range must be at least %d IP addresses&amp;quot;, 8)
	}

	// Select the first valid IP from ServiceClusterIPRange to use as the GenericAPIServer service IP.
	apiServerServiceIP, err := utilnet.GetIndexedIP(&amp;amp;serviceClusterIPRange, 1)
	if err != nil {
		return net.IPNet{}, net.IP{}, err
	}

	return serviceClusterIPRange, apiServerServiceIP, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function gets &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/master/master.go#L292&#34; target=&#34;_blank&#34;&gt;called&lt;/a&gt; when the master controller is started and hard-codes the service IP for the default service to the first IP of the range. Another interesting fact in this function is that it always checks that the Cluster IP range is at least /29, which fits 6 usable addresses in the worst case. The latter can probably be explained by the fact that the next size down is /30, which doesn&amp;rsquo;t leave much room for user-defined clusterIPs after the &lt;code&gt;kubernetes.default&lt;/code&gt; and &lt;code&gt;kube-dns.kube-system&lt;/code&gt; are configured, so in the smallest possible cluster you can at least configure a few non-default services before you run out of IPs.&lt;/p&gt;

&lt;h2 id=&#34;endpoint-ips&#34;&gt;Endpoint IPs&lt;/h2&gt;

&lt;p&gt;The way endpoint addresses are populated is different between managed (GKE, AKS, EKS) and non-managed clusters. Let&amp;rsquo;s first have a look at a highly-available kind cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe svc kubernetes | grep Endpoints
Endpoints:         172.18.0.3:6443,172.18.0.4:6443,172.18.0.7:6443
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bearing in mind that by default kind would use &lt;code&gt;10.244.0.0/16&lt;/code&gt; as the pod IP range and &lt;code&gt;10.96.0.0/12&lt;/code&gt; as the cluster IP range, these IPs don&amp;rsquo;t make a lot of sense. However, since kind uses kubeadm under the hood, which spins up control plane components as static pods, we can find API server pods in the &lt;code&gt;kube-system&lt;/code&gt; namespace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl -n kube-system get pod -l tier=control-plane -o wide | grep api
kube-apiserver-kind-control-plane             1/1     Running   172.18.0.3
kube-apiserver-kind-control-plane2            1/1     Running   172.18.0.4
kube-apiserver-kind-control-plane3            1/1     Running   172.18.0.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we check the manifest of any of the above pods, we&amp;rsquo;ll see that they are run with &lt;code&gt;hostNetwork: true&lt;/code&gt; and those IP come from the underlying containers that kind uses as nodes. As a part of the &lt;code&gt;UpdateKubernetesService()&lt;/code&gt; mentioned above, each API server in the cluster goes and &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/master/controller.go#L243&#34; target=&#34;_blank&#34;&gt;updates&lt;/a&gt; the &lt;code&gt;endpoints&lt;/code&gt; object with its own IP and Port as defined in the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/master/reconcilers/mastercount.go#L62&#34; target=&#34;_blank&#34;&gt;mastercount.go&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (r *masterCountEndpointReconciler) ReconcileEndpoints(serviceName string, ip net.IP, endpointPorts []corev1.EndpointPort, reconcilePorts bool) error {

	e.Subsets = []corev1.EndpointSubset{{
		Addresses: []corev1.EndpointAddress{{IP: ip.String()}},
		Ports:     endpointPorts,
	}}
	klog.Warningf(&amp;quot;Resetting endpoints for master service %q to %#v&amp;quot;, serviceName, e)
	_, err = r.epAdapter.Update(metav1.NamespaceDefault, e)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;With managed Kubernetes clusters, control-plane nodes are not accessible by end users, so it&amp;rsquo;s harder to say exactly how endpoints are getting populated. However, it&amp;rsquo;s fairly easy to imagine that a cloud provider spins up a 3-node control-plane with a load-balancer and configures all three API servers with this LB&amp;rsquo;s IP as the &lt;code&gt;advertise-address&lt;/code&gt;. This would results in a single endpoint that represents that managed control-plane load-balancer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get ep kubernetes
NAME         ENDPOINTS          AGE
kubernetes   172.16.0.2:443   40d
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Solving the Ingress Mystery Puzzle</title>
      <link>https://networkop.co.uk/post/2020-06-ingress-puzzle/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2020-06-ingress-puzzle/</guid>
      <description>

&lt;p&gt;Last week I posted a &lt;a href=&#34;https://twitter.com/networkop1/status/1269651463690760193&#34; target=&#34;_blank&#34;&gt;tweet&lt;/a&gt; about a Kubernetes networking puzzle. In this post, we&amp;rsquo;ll go over the details of this puzzle and uncover the true cause and motive of the misbehaving ingress.&lt;/p&gt;

&lt;h2 id=&#34;puzzle-recap&#34;&gt;Puzzle recap&lt;/h2&gt;

&lt;p&gt;Imagine you have a Kubernetes cluster with three namespaces, each with its own namespace-scoped ingress controller. You&amp;rsquo;ve created an ingress in each namespace that exposes a simple web application. You&amp;rsquo;ve checked one of them, made sure it works and moved on to other things. However some time later, you get reports that the web app is unavailable. You go to check it again and indeed, the page is not responding, although nothing has changed in the cluster. In fact, you realise that the problem is intermittent - one minute you can access the page, and on the next refresh it&amp;rsquo;s gone. To make things worse, you realise that similar issues affect the other two ingresses.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ingress-puzzle.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you feel like you&amp;rsquo;re capable of solving it on your own, feel free to follow the steps in the &lt;a href=&#34;https://github.com/networkop/ingress-puzzle&#34; target=&#34;_blank&#34;&gt;walkthrough&lt;/a&gt;, otherwise, continue on reading. In either case, make sure you&amp;rsquo;ve setup a local test environment so that it&amp;rsquo;s easier to follow along:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Clone the ingress-puzzle repo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/networkop/ingress-puzzle &amp;amp;&amp;amp; cd ingress-puzzle
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Build a local test cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make cluster
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create three namespaces:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create an in-cluster load-balancer (MetalLB) that will allocate IPs from a &lt;code&gt;100.64.0.0/16&lt;/code&gt; range:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make load-balancer
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In each namespace, install a namespace-scoped ingress controller:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make controllers
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create three test deployments and expose them via ingresses:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make ingresses
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;ingress-controller-expected-behaviour&#34;&gt;Ingress controller expected behaviour&lt;/h2&gt;

&lt;p&gt;In order to solve this puzzle we need to understand how ingress controllers perform their duties, so let&amp;rsquo;s see how a typical ingress controller does that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;An ingress controller consists of &lt;strong&gt;two components&lt;/strong&gt; - control plane and data plane, which can be run separately or be a part of the same pod/deployment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Control plane&lt;/strong&gt; is a k8s controller that uses its pod&amp;rsquo;s service account to talk to the API server and establishes &amp;ldquo;watches&amp;rdquo; on &lt;code&gt;Ingress&lt;/code&gt;-type resources.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data plane&lt;/strong&gt; is a reverse proxy (e.g. nginx, envoy) that receives traffic from end users and forwards it upstream to one of the backend k8s services.&lt;/li&gt;
&lt;li&gt;In order to steer the traffic to the data plane, an external &lt;strong&gt;load-balancer&lt;/strong&gt; service is required, whose address (IP or hostname) is reflected in ingress&amp;rsquo;s status field.&lt;/li&gt;
&lt;li&gt;As &lt;code&gt;Ingress&lt;/code&gt; resources get created/deleted, controller updates configuration of its data plane to match the desired state described in those resources.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This sounds simple enough, but as always, the devil is in the details, so let&amp;rsquo;s start by focusing on one of the namespaces and observe the behaviour of its ingress.&lt;/p&gt;

&lt;h2 id=&#34;exhibit-1-namespace-one&#34;&gt;Exhibit #1 - namespace one&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s looks at the ingress in namespace &lt;code&gt;one&lt;/code&gt;. It seems like a healthy-looking output, the address is set to the &lt;code&gt;100.64.0.0&lt;/code&gt; which is part of the MetalLB range.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubens one
$ kubectl get ingress
NAME   CLASS    HOSTS   ADDRESS      PORTS   AGE
test   &amp;lt;none&amp;gt;   *       100.64.0.0   80      141m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to test connectivity to the backend deployment, you can add the MetalLB public IP range to the docker bridge like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip=$(kubectl get nodes -o jsonpath=&#39;{.items[0].status.addresses[0].address}&#39;)
device=$(ip -j route get $ip | jq &#39;.[0].dev&#39;)
sudo ip addr add 100.64.0.100/16 dev $device
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you should be able to hit the test nginx deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -s 100.64.0.0 | grep Welcome
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nothing unusual so far, and nothing to indicate intermittent connectivity either. Let&amp;rsquo;s move on.&lt;/p&gt;

&lt;h2 id=&#34;exhibit-2-namespaces-two-three&#34;&gt;Exhibit #2 - namespaces two &amp;amp; three&lt;/h2&gt;

&lt;p&gt;This output looks a bit weird, the IP in the address field is definitely not a part of the MetalLB range:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubens two
$ kubectl get ingress
NAME   CLASS    HOSTS   ADDRESS      PORTS   AGE
test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      155m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A similar situation can be observed in the other namespace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubens three
$ kubectl get ingress
NAME   CLASS    HOSTS   ADDRESS      PORTS   AGE
test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      155m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point, these outputs don&amp;rsquo;t make a lot of sense. How can two different ingresses, controlled by two distinct controllers have the same address? And why do they get allocated with a private IP, which is not managed by MetalLB? If we check services across all existing namespaces, there won&amp;rsquo;t be a single service with IPs from &lt;code&gt;172.16.0.0/12&lt;/code&gt; range.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get svc -A | grep 172
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;exhibit-4-flapping-addresses&#34;&gt;Exhibit #4 - flapping addresses&lt;/h2&gt;

&lt;p&gt;Another one of the reported issues was the intermittent connectivity to some of the ingresses. If we keep watching the ingress in namespace &lt;code&gt;one&lt;/code&gt;, we should see something interesting:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubens one
kubectl get ingress --watch
NAME   CLASS    HOSTS   ADDRESS      PORTS   AGE
test   &amp;lt;none&amp;gt;   *       100.64.0.0   80      141m
test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      141m
test   &amp;lt;none&amp;gt;   *       100.64.0.0   80      142m
test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      142m
test   &amp;lt;none&amp;gt;   *       100.64.0.0   80      143m
test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      143m
test   &amp;lt;none&amp;gt;   *       100.64.0.0   80      144m
test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      144m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like the ingress address is flapping between our &amp;ldquo;good&amp;rdquo; MetalLB IP and the same exact IP that the other two ingresses have. Now let&amp;rsquo;s zoom out a bit and have a look at all three ingresses at the same time:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get ingress --watch -A
NAMESPACE   NAME   CLASS    HOSTS   ADDRESS      PORTS   AGE
one         test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      150m
three       test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      150m
two         test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      150m
one         test   &amp;lt;none&amp;gt;   *       100.64.0.0   80      150m
three       test   &amp;lt;none&amp;gt;   *       100.64.0.2   80      151m
three       test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      151m
one         test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      151m
one         test   &amp;lt;none&amp;gt;   *       100.64.0.0   80      151m
three       test   &amp;lt;none&amp;gt;   *       100.64.0.2   80      152m
one         test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      152m
three       test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      152m
one         test   &amp;lt;none&amp;gt;   *       100.64.0.0   80      152m

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This looks even more puzzling - it seems that all ingresses have addresses that flap continuously. This would definitely explain the intermittent connectivity, however the most important question now is &amp;ldquo;why&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;exhibit-5-controller-logs&#34;&gt;Exhibit #5 - controller logs&lt;/h2&gt;

&lt;p&gt;The most obvious suspect at this stage is the ingress controller, since it&amp;rsquo;s the one that updates the status of its managed ingress resources. Let stay in the same namespace and look at its logs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl logs deploy/ingress-ingress-nginx-controller -f

event.go:278] Event(v1.ObjectReference{Kind:&amp;quot;Ingress&amp;quot;, Namespace:&amp;quot;one&amp;quot;, Name:&amp;quot;test&amp;quot;, UID:&amp;quot;7d1e4069-d285-4cf8-ba28-437d0a8fd04d&amp;quot;, APIVersion:&amp;quot;networking.k8s.io/v1beta1&amp;quot;, ResourceVersion:&amp;quot;55860&amp;quot;, FieldPath:&amp;quot;&amp;quot;}): type: &#39;Normal&#39; reason: &#39;UPDATE&#39; Ingress one/test

status.go:275] updating Ingress one/test status from [{172.18.0.2 }] to [{100.64.0.0 }]

event.go:278] Event(v1.ObjectReference{Kind:&amp;quot;Ingress&amp;quot;, Namespace:&amp;quot;one&amp;quot;, Name:&amp;quot;test&amp;quot;, UID:&amp;quot;7d1e4069-d285-4cf8-ba28-437d0a8fd04d&amp;quot;, APIVersion:&amp;quot;networking.k8s.io/v1beta1&amp;quot;, ResourceVersion:&amp;quot;55870&amp;quot;, FieldPath:&amp;quot;&amp;quot;}): type: &#39;Normal&#39; reason: &#39;UPDATE&#39; Ingress one/test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This doesn&amp;rsquo;t make a lot of sense - the ingress controller clearly updates the status with the right IP, but why does it get overwritten? and by whom?&lt;/p&gt;

&lt;h2 id=&#34;exhibit-5-cluster-wide-logs&#34;&gt;Exhibit #5 - cluster-wide logs&lt;/h2&gt;

&lt;p&gt;At this point, we can allow ourselves a little bit of cheating. Since it&amp;rsquo;s a test cluster and we&amp;rsquo;ve only got a few ingresses configured, we can tail logs from all ingress controllers and watch all ingresses at the same time. Don&amp;rsquo;t forget to install &lt;a href=&#34;https://github.com/wercker/stern&#34; target=&#34;_blank&#34;&gt;stern&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get ingress -A -w &amp;amp;
stern --all-namespaces -l app.kubernetes.io/name=ingress-nginx &amp;amp;
three ingress-ingress-nginx-controller-58b79c576b-94v8d controller status.go:275] updating Ingress three/test status from [{172.18.0.2 }] to [{100.64.0.2 }]

three       test   &amp;lt;none&amp;gt;   *       100.64.0.2   80      174m

twothree  ingress-ingress-nginx-controller-5db5984d7d-vljth ingress-ingress-nginx-controller-58b79c576b-94v8d controller event.go:278] Event(v1.ObjectReference{Kind:&amp;quot;Ingress&amp;quot;, Namespace:&amp;quot;three&amp;quot;, Name:&amp;quot;test&amp;quot;, UID:&amp;quot;176f0f8e-d3d5-4476-9b51-2d02c7eb47e2&amp;quot;, APIVersion:&amp;quot;networking.k8s.io/v1beta1&amp;quot;, ResourceVersion:&amp;quot;57195&amp;quot;, FieldPath:&amp;quot;&amp;quot;}): type: &#39;Normal&#39; reason: &#39;UPDATE&#39; Ingress three/test
event.go:278] Event(v1.ObjectReference{Kind:&amp;quot;Ingress&amp;quot;, Namespace:&amp;quot;three&amp;quot;, Name:&amp;quot;test&amp;quot;, UID:&amp;quot;176f0f8e-d3d5-4476-9b51-2d02c7eb47e2&amp;quot;, APIVersion:&amp;quot;networking.k8s.io/v1beta1&amp;quot;, ResourceVersion:&amp;quot;57195&amp;quot;, FieldPath:&amp;quot;&amp;quot;}): type: &#39;Normal&#39; reason: &#39;UPDATE&#39; Ingress three/test

two ingress-ingress-nginx-controller-5db5984d7d-vljth controller status.go:275] updating Ingress one/test status from [{100.64.0.0 }] to [{172.18.0.2 }]
two ingress-ingress-nginx-controller-5db5984d7d-vljth controller status.go:275] updating Ingress three/test status from [{100.64.0.2 }] to [{172.18.0.2 }]

three       test   &amp;lt;none&amp;gt;   *       172.18.0.2   80      174m
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;whodunit&#34;&gt;Whodunit&lt;/h2&gt;

&lt;p&gt;So, it looks like the culprit is the ingress controller in namespace &lt;code&gt;two&lt;/code&gt; and it tries to change status fields of all three ingresses. Now it&amp;rsquo;s safe to look at exactly how it was installed, and this is the helm values file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;controller:
  publishService:
    enabled: false
    pathOverride: &amp;quot;two/svc&amp;quot;
  scope:
    enabled: false
  admissionWebhooks:
    enabled: false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like the scope variable is set incorrectly so the ingress controller defaults to trying to manage ingresses across all namespaces. This should be an easy fix - just change the scope to &lt;code&gt;true&lt;/code&gt; and upgrade the chart.&lt;/p&gt;

&lt;p&gt;However, this still doesn&amp;rsquo;t explain the private IP address or its origin. Let&amp;rsquo;s try the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get nodes -o wide
NAME                           STATUS   ROLES    AGE    VERSION   INTERNAL-IP
ingress-puzzle-control-plane   Ready    master   5h3m   v1.18.2   172.18.0.2 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So this is where it comes from - it&amp;rsquo;s the IP of the k8s node we&amp;rsquo;ve been running our tests on. But why would it get allocated to an ingress? To understand that we need to have a look at
 the nginx-ingress controller source code, specifically this function from &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx/blob/master/internal/ingress/status/status.go#L174&#34; target=&#34;_blank&#34;&gt;status.go&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (s *statusSync) runningAddresses() ([]string, error) {
	if s.PublishStatusAddress != &amp;quot;&amp;quot; {
		return []string{s.PublishStatusAddress}, nil
	}

	if s.PublishService != &amp;quot;&amp;quot; {
		return statusAddressFromService(s.PublishService, s.Client)
	}

	// get information about all the pods running the ingress controller
	pods, err := s.Client.CoreV1().Pods(s.pod.Namespace).List(context.TODO(), metav1.ListOptions{
		LabelSelector: labels.SelectorFromSet(s.pod.Labels).String(),
	})
	if err != nil {
		return nil, err
	}

	addrs := make([]string, 0)
	for _, pod := range pods.Items {
		// only Running pods are valid
		if pod.Status.Phase != apiv1.PodRunning {
			continue
		}

		name := k8s.GetNodeIPOrName(s.Client, pod.Spec.NodeName, s.UseNodeInternalIP)
		if !sliceutils.StringInSlice(name, addrs) {
			addrs = append(addrs, name)
		}
	}

	return addrs, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is how the nginx-ingress controller determines the address to report in the ingress status:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Check if the address is statically set with the &lt;code&gt;--publish-status-address&lt;/code&gt; flag.&lt;/li&gt;
&lt;li&gt;Try to collect addresses from a published service (load-balancer).&lt;/li&gt;
&lt;li&gt;If both of the above have failed, get the list of pods and return the IPs of the nodes they are running on.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This last bit is why we had that private IP in the status field. If you look at the above values YAML again, you&amp;rsquo;ll see that the &lt;code&gt;publishService&lt;/code&gt; value is overridden with a static service called &lt;code&gt;svc&lt;/code&gt;. However, because this service doesn&amp;rsquo;t exist and was never created, the ingress controller will fail to collect the right IP and will fall through to step 3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ingress-puzzle-solved.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The logic described above is quite common and is also implemented by &lt;a href=&#34;https://github.com/Kong/kubernetes-ingress-controller/blob/master/internal/ingress/status/status.go&#34; target=&#34;_blank&#34;&gt;Kong&lt;/a&gt; ingress controller. The idea is that if your k8s nodes are running in a cluster with public IPs, this should still make the ingress accessible, even without a load-balancer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting Started with Cluster API using Docker</title>
      <link>https://networkop.co.uk/post/2020-05-cluster-api-intro/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2020-05-cluster-api-intro/</guid>
      <description>

&lt;p&gt;Cluster API (CAPI) is a relatively new project aimed at deploying Kubernetes clusters using a declarative API (think YAML). The official documentation (a.k.a. the Cluster API book), does a very good job explaining the main &lt;a href=&#34;https://cluster-api.sigs.k8s.io/user/concepts.html&#34; target=&#34;_blank&#34;&gt;concepts&lt;/a&gt; and &lt;a href=&#34;https://cluster-api.sigs.k8s.io/introduction.html&#34; target=&#34;_blank&#34;&gt;goals&lt;/a&gt; of the project. I always find that one of the best ways to explore new technology is to see how it works locally, on my laptop, and Cluster API has a special &amp;ldquo;Docker&amp;rdquo; infrastructure provider (CAPD) specifically for that. However, the official documentation for how to setup a docker managed cluster is very poor and fractured. In this post, I&amp;rsquo;ll try to demonstrate the complete journey to deploy a single CAPI-managed k8s cluster and provide some explanation of what happens behind the scene so that its easier to troubleshoot when things go wrong.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;Two things must be pre-installed before we can start building our test clusters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://kind.sigs.k8s.io/docs/user/quick-start/&#34; target=&#34;_blank&#34;&gt;kind&lt;/a&gt;&lt;/strong&gt; - a tool to setup k8s clusters in docker containers, it will be used as a management (a.k.a. bootstrap) cluster.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl&#34; target=&#34;_blank&#34;&gt;clusterctl&lt;/a&gt;&lt;/strong&gt; - a command line tool to interact with the management cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;rsquo;re gonna need run a few scripts from the Cluster API Github repo, so let&amp;rsquo;s get a copy of it locally:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone --depth=1 git@github.com:kubernetes-sigs/cluster-api.git &amp;amp;&amp;amp; cd cluster-api
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When building a management cluster with kind, it&amp;rsquo;s a good idea to mount the &lt;code&gt;docker.sock&lt;/code&gt; file from your host OS into the kind cluster, as it is mentioned in &lt;a href=&#34;https://cluster-api.sigs.k8s.io/clusterctl/developers.html#additional-steps-in-order-to-use-the-docker-provider&#34; target=&#34;_blank&#34;&gt;the book&lt;/a&gt;. This will allow you to see the CAPD-managed nodes directly in your hostOS as regular docker containers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; kind-cluster-with-extramounts.yaml &amp;lt;&amp;lt;EOF
kind: Cluster
apiVersion: kind.sigs.k8s.io/v1alpha3
nodes:
  - role: control-plane
    extraMounts:
      - hostPath: /var/run/docker.sock
        containerPath: /var/run/docker.sock
EOF
kind create cluster --config ./kind-cluster-with-extramounts.yaml --name clusterapi
kubectl cluster-info --context kind-clusterapi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this stage you should have your kubectl pointed at the new kind cluster, which can be verified like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get nodes -o wide
NAME                       STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION          CONTAINER-RUNTIME
clusterapi-control-plane   Ready    master   66s   v1.17.0   172.17.0.2    &amp;lt;none&amp;gt;        Ubuntu 19.10   5.6.6-200.fc31.x86_64   containerd://1.3.2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;preparing-a-capd-controller&#34;&gt;Preparing a CAPD controller&lt;/h2&gt;

&lt;p&gt;The docker image for the CAPD controller is not available in the public registry, so we need to build it locally. The following two commands will build the image and update the installation manifests to use that image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make -C test/infrastructure/docker docker-build REGISTRY=gcr.io/k8s-staging-capi-docker
make -C test/infrastructure/docker generate-manifests REGISTRY=gcr.io/k8s-staging-capi-docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we need side-load this image into a kind cluster to make it available to the future CAPD deployment&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind load docker-image --name clusterapi gcr.io/k8s-staging-capi-docker/capd-manager-amd64:dev
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;setting-up-a-docker-provider&#34;&gt;Setting up a Docker provider&lt;/h2&gt;

&lt;p&gt;Once again, following &lt;a href=&#34;https://cluster-api.sigs.k8s.io/clusterctl/developers.html#additional-steps-in-order-to-use-the-docker-provider&#34; target=&#34;_blank&#34;&gt;the book&lt;/a&gt;, we need to run a local override script to generate a set of manifests for Docker provider:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; clusterctl-settings.json &amp;lt;&amp;lt;EOF
{
  &amp;quot;providers&amp;quot;: [&amp;quot;cluster-api&amp;quot;,&amp;quot;bootstrap-kubeadm&amp;quot;,&amp;quot;control-plane-kubeadm&amp;quot;, &amp;quot;infrastructure-docker&amp;quot;]
}
EOF
cmd/clusterctl/hack/local-overrides.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should be able to see the generated manifests at &lt;code&gt;~/..cluster-api/overrides/infrastructure-docker/latest/infrastructure-components.yaml&lt;/code&gt;, the only last thing we need to do is let clusterctl know where to find them:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; ~/.cluster-api/clusterctl.yaml &amp;lt;&amp;lt;EOF
providers:
  - name: docker
    url: $HOME/.cluster-api/overrides/infrastructure-docker/latest/infrastructure-components.yaml
    type: InfrastructureProvider
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we can use the  &lt;code&gt;clusterctl init&lt;/code&gt; command printed by the &lt;code&gt;local-verrides.py&lt;/code&gt; script to create all CAPI and CAPD components inside our kind cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;clusterctl init --core cluster-api:v0.3.0 --bootstrap kubeadm:v0.3.0 --control-plane kubeadm:v0.3.0 --infrastructure docker:v0.3.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this stage, we should see the following deployments created and ready (1/1).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;k get deploy -A | grep cap
capd-system                         capd-controller-manager                         1/1
capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager       1/1
capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager   1/1
capi-system                         capi-controller-manager                         1/1
capi-webhook-system                 capi-controller-manager                         1/1 
capi-webhook-system                 capi-kubeadm-bootstrap-controller-manager       1/1
capi-webhook-system                 capi-kubeadm-control-plane-controller-manager   1/1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If &lt;code&gt;capd-system&lt;/code&gt; deployment is not READY and stuck trying to pull the image, make sure that the &lt;code&gt;capd-controller-manager&lt;/code&gt; deployment is using the image that was generated in the previous section.&lt;/p&gt;

&lt;h2 id=&#34;generating-a-capd-managed-cluster-manifest&#34;&gt;Generating a CAPD-managed cluster manifest&lt;/h2&gt;

&lt;p&gt;All the instructions provided so far can also be found in the official documentation. However, at this stage, the book started having big gaps that were not trivial to figure out. TLDR: you can just run the below command to generate a sample capd cluster manifest and move on to the next section. However if you ever need to modify this command, check out my notes below it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DOCKER_POD_CIDRS=&amp;quot;192.168.0.0/16&amp;quot; \
DOCKER_SERVICE_CIDRS=&amp;quot;10.128.0.0/12&amp;quot; \
DOCKER_SERVICE_DOMAIN=&amp;quot;cluster.local&amp;quot; \
clusterctl config cluster capd --kubernetes-version v1.17.5 \
--from ./test/e2e/data/infrastructure-docker/cluster-template.yaml \
--target-namespace default \
--control-plane-machine-count=1 \
--worker-machine-count=1 \
&amp;gt; capd.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At the time of writing, CAPD used &lt;code&gt;kindest/node&lt;/code&gt; docker images (see &lt;code&gt;defaultImageName&lt;/code&gt; in test/infrastructure/docker/docker/machines.go) and combined it with a tag provided in the &lt;code&gt;--kubernetes-version&lt;/code&gt; argument. Be sure to always check if there&amp;rsquo;s a matching tag on &lt;a href=&#34;https://hub.docker.com/r/kindest/node/tags&#34; target=&#34;_blank&#34;&gt;dockerhub&lt;/a&gt;. If it is missing (e.g. v1.17.3), Machine controller will fail to create a docker container for your kubernetes cluster and you&amp;rsquo;ll only see the load-balancer container being created.&lt;/p&gt;

&lt;p&gt;Another issue is the clusterctl may not find the &lt;code&gt;cluster-template.yaml&lt;/code&gt; where it expects, so it would have to be provided with the &lt;code&gt;--from&lt;/code&gt; argument. This template would require additional variables (all that start with &lt;code&gt;DOCKER_&lt;/code&gt;) that have to be provided for it to be rendered. These can be modified as long as you understand what they do.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: never set the POD CIDR equal to the Service CIDR unless you want to spend your time troubleshooting networking and DNS issues.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finally, you should also make sure that the target namespace is specified explicitly, otherwise the generated manifest will contain an incorrect combination of namespaces and will get rejected by the validating webhook.&lt;/p&gt;

&lt;h2 id=&#34;creating-a-capd-managed-cluster&#34;&gt;Creating a CAPD-managed cluster&lt;/h2&gt;

&lt;p&gt;The final step is to apply the generated manifest and let the k8s controllers do their job.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f capd.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s worth spending a bit of time understanding what&amp;rsquo;s some of these controllers do. The &lt;code&gt;DockerCluster&lt;/code&gt; controller is responsible for the creation of a load-balancing container (capd-lb). A load-balancer is needed to provide a single API endpoint in front of multiple controller nodes. It&amp;rsquo;s built on top of the HAProxy image (kindest/haproxy:2.1.1-alpine), and does the healthchecking and load-balancing across all cluster controller nodes. It&amp;rsquo;s worth noting that the &lt;code&gt;DockerCluster&lt;/code&gt; resource is marked as &lt;code&gt;READY&lt;/code&gt; as soon as the controller can read the IP assigned to the &lt;code&gt;capd-lb&lt;/code&gt; container, which doesn&amp;rsquo;t necessarily reflect that the cluster itself is built.&lt;/p&gt;

&lt;p&gt;Typically, all nodes in a CAPI-managed clusters are bootstrapped with cloud-init that is generated by the bootstrap controller. However Docker doesn&amp;rsquo;t have a cloud-init equivalent, so the &lt;code&gt;DockerMachine&lt;/code&gt; controller simply executes each line of the bootstrap script using the &lt;code&gt;docker exec&lt;/code&gt; commands. It&amp;rsquo;s also worth noting that containers themselves are also managed using docker CLI and not API.&lt;/p&gt;

&lt;h2 id=&#34;installing-cni-and-metallb&#34;&gt;Installing CNI and MetalLB&lt;/h2&gt;

&lt;p&gt;As a bonus, I&amp;rsquo;ll show how to install CNI and MetalLB to build a completely functional k8s cluster. First, we need to extract the kubeconfig file and save it locally:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get secret/capd-kubeconfig -o jsonpath={.data.value} \
  | base64 --decode  &amp;gt; ./capd.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can apply the CNI config, as suggested in the book.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KUBECONFIG=./capd.kubeconfig kubectl \
  apply -f https://docs.projectcalico.org/v3.12/manifests/calico.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A minute later, both nodes should transition to &lt;code&gt;Ready&lt;/code&gt; state:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KUBECONFIG=./capd.kubeconfig kubectl get nodes
NAME                              STATUS   ROLES    AGE   VERSION
capd-capd-control-plane-hn724     Ready    master   30m   v1.17.5
capd-capd-md-0-84df67c74b-lzm6z   Ready    &amp;lt;none&amp;gt;   29m   v1.17.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to be able to create load-balancer type services, we can install MetalLB in L2 mode. Thanks to the &lt;code&gt;docker.sock&lt;/code&gt; mounting we&amp;rsquo;ve done above, our test cluster is now attached to the same docker bridge as the rest of the containers in host OS. We can easily determine what subnet is being used by it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DOCKER_BRIDGE_SUBNET=$(docker network inspect bridge | jq -r &#39;.[0].IPAM.Config[0].Subnet&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, using the &lt;a href=&#34;http://jodies.de/ipcalc-archive/ipcalc-0.41/ipcalc&#34; target=&#34;_blank&#34;&gt;ipcalc&lt;/a&gt; tool, we can pick a small range from the high end of that subnet:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DOCKER_HIGHEND_RANGE=$(ipcalc -s 6 ${DOCKER_BRIDGE_SUBNET}  | grep 29 | tail -n 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can create the configuration for MetalLB:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; metallb_cm.yaml &amp;lt;&amp;lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: my-ip-space
      protocol: layer2
      addresses:
      - $DOCKER_HIGHEND_RANGE   
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, all we have to do is install it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KUBECONFIG=./capd.kubeconfig kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml
KUBECONFIG=./capd.kubeconfig kubectl apply -f metallb_cm.yaml
KUBECONFIG=./capd.kubeconfig kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml
KUBECONFIG=./capd.kubeconfig kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=&amp;quot;$(openssl rand -base64 128)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To test it, we can deploy a test application and expose it with a service of type LoadBalancer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KUBECONFIG=./capd.kubeconfig kubectl create deployment test --image=nginx
KUBECONFIG=./capd.kubeconfig kubectl expose deployment test --name=lb --port=80 --target-port=80 --type=LoadBalancer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we should be able to access the application running inside the cluster by hitting the external load-balancer IP:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MetalLB_IP=$(KUBECONFIG=./capd.kubeconfig kubectl get svc lb -o jsonpath=&#39;{.status.loadBalancer.ingress[0].ip}&#39;)
curl -s $MetalLB_IP | grep &amp;quot;Thank you&amp;quot;
&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/capd.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Network Simulations with Network Service Mesh</title>
      <link>https://networkop.co.uk/post/2020-01-nsm-topo/</link>
      <pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2020-01-nsm-topo/</guid>
      <description>

&lt;p&gt;In September 2019 I had the honour to &lt;a href=&#34;https://onseu19.sched.com/event/SYsb/large-scale-network-simulations-in-kubernetes-michael-kashin-arista-networks&#34; target=&#34;_blank&#34;&gt;present&lt;/a&gt; at Open Networking Summit in Antwerp. My talk was about &lt;a href=&#34;https://github.com/networkop/meshnet-cni&#34; target=&#34;_blank&#34;&gt;meshnet&lt;/a&gt; CNI plugin, &lt;a href=&#34;https://github.com/networkop/k8s-topo&#34; target=&#34;_blank&#34;&gt;k8s-topo&lt;/a&gt; orchestrator and how to use them for large-scale network simulations in Kubernetes. During the same conference, I attended a talk about Network Service Mesh and its new &lt;a href=&#34;https://onseu19.sched.com/event/SYum/kernel-based-forwarding-plane-for-network-service-mesh-radoslav-dimitrov-vmware&#34; target=&#34;_blank&#34;&gt;kernel-based forwarding dataplane&lt;/a&gt; which had a lot of similarities with the work that I&amp;rsquo;ve done for meshnet. Having had a chat with the presenters, we&amp;rsquo;ve decided that it would be interesting to try and implement a meshnet-like functionality with NSM. In this post, I&amp;rsquo;ll try to document some of the findings and results of my research.&lt;/p&gt;

&lt;h1 id=&#34;network-service-mesh-introduction&#34;&gt;Network Service Mesh Introduction&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://networkservicemesh.io/&#34; target=&#34;_blank&#34;&gt;NSM&lt;/a&gt; is a CNCF project aimed at providing service mesh-like capabilities for L2/L3 traffic. In the context of Kubernetes, NSM&amp;rsquo;s role is to interconnect pods and setup the underlying forwarding, which involves creating new interfaces, allocating IPs and configuring pod&amp;rsquo;s routing table. The main use cases are cloud-native network functions (e.g. 5G), service function chaining and any containerised applications that may need to talk over non-standard protocols. Similar to traditional service meshes, the intended functionality is achieved by injecting sidecar containers that communicate with a distributed control plane of network service managers, deployed as a &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&#34; target=&#34;_blank&#34;&gt;daemonset&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll try to avoid repeating NSM&amp;rsquo;s theory here and instead refer my readers to the official &lt;a href=&#34;https://networkservicemesh.io/docs/concepts/what-is-nsm&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt; and a very good introductory &lt;a href=&#34;https://docs.google.com/presentation/d/1IC2kLnQGDz1hbeO0rD7Y82O_4NwzgIoGgm0oOXyaQ9Y/edit#slide=id.p&#34; target=&#34;_blank&#34;&gt;slide deck&lt;/a&gt;. There are a few concepts, however, that are critical to the understanding of this blogpost, that I&amp;rsquo;ll mention here briefly:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Network Services&lt;/strong&gt; are built around a client-server model - a client receives a service from an endpoint (server).&lt;/li&gt;
&lt;li&gt;Both client and endpoint are implemented as &lt;strong&gt;containers&lt;/strong&gt; and interact with &lt;strong&gt;local control plane agents&lt;/strong&gt; over a gRPC-based API.&lt;/li&gt;
&lt;li&gt;Typically, a &lt;strong&gt;client&lt;/strong&gt; would request a service with &lt;code&gt;ns.networkservicemesh.io&lt;/code&gt; &lt;strong&gt;annotation&lt;/strong&gt;, which gets matched by a mutating webhook responsible for injecting an init container.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Endpoints&lt;/strong&gt;, being designed specifically to provide network services, have endpoint container statically defined as a &lt;strong&gt;sidecar&lt;/strong&gt; (unless they natively implement NSM&amp;rsquo;s SDK).&lt;/li&gt;
&lt;li&gt;One important distinction between client and endpoint sidecars is that the former is an &lt;strong&gt;init&lt;/strong&gt; container (runs to completion at pod create time) and the latter is a normal &lt;strong&gt;sidecar&lt;/strong&gt; which allows service reconfiguration at runtime.&lt;/li&gt;
&lt;li&gt;All client and endpoint configurations get passed as &lt;strong&gt;environment variables&lt;/strong&gt; to the respective containers either dynamically (client) or statically (endpoint).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given all of the above, this is how you&amp;rsquo;d use NSM to create a point-to-point link between any two pods.&lt;/p&gt;

&lt;h1 id=&#34;using-nsm-to-create-links-between-pods&#34;&gt;Using NSM to create links between pods&lt;/h1&gt;

&lt;p&gt;First, we need to decide which side of the link will be a client and which will be an endpoint. This is where we&amp;rsquo;ll abuse NSM&amp;rsquo;s concepts for the first time as it really doesn&amp;rsquo;t matter how this allocation takes place. For a normal network service, it&amp;rsquo;s fairly easy to identify and map client/server roles, however, for topology simulations they can be assigned arbitrarily as both sides of the link are virtually equivalent.&lt;/p&gt;

&lt;p&gt;The next thing we need to do is statically add sidecar containers not only to the endpoint side of the link but to the client as well. This is another abuse of NSM&amp;rsquo;s intended mode of operation, where a client init container gets injected automatically by the webhook. The reason for that is that the init container will block until its network service request gets accepted, which may create a circular dependency if client/endpoint roles are assigned arbitrarily, as discussed above.&lt;/p&gt;

&lt;p&gt;The resulting &amp;ldquo;endpoint&amp;rdquo; side of the link will have the following pod manifest. The NSE sidecar container will read the environment variables and use NSM&amp;rsquo;s &lt;a href=&#34;https://github.com/networkservicemesh/networkservicemesh/tree/master/sdk&#34; target=&#34;_blank&#34;&gt;SDK&lt;/a&gt; to register itself with a &lt;code&gt;p2p&lt;/code&gt; network service with a &lt;code&gt;device=device-2&lt;/code&gt; label.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: device-2
spec:
  containers:
  - image: alpine:latest
    command: [&amp;quot;tail&amp;quot;, &amp;quot;-f&amp;quot;, &amp;quot;/dev/null&amp;quot;]
    name: alpine
  - name: nse-sidecar
    image: networkservicemesh/topology-sidecar-nse:master
    env:
    - name: ENDPOINT_NETWORK_SERVICE
      value: &amp;quot;p2p&amp;quot;
    - name: ENDPOINT_LABELS
      value: &amp;quot;device=device-2&amp;quot;
    - name: IP_ADDRESS
      value: &amp;quot;10.60.1.0/24&amp;quot;
    resources:
      limits:
        networkservicemesh.io/socket: &amp;quot;1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When a local control plane agent receives the above registration request, it will create a new k8s &lt;code&gt;NetworkServiceEndpoint&lt;/code&gt; resource, effectively letting all the other agents know where to find this particular service endpoint (in this case it&amp;rsquo;s the k8s node called &lt;code&gt;nsm-control-plane&lt;/code&gt;). Note that the below resource is managed by NSM&amp;rsquo;s control plane and should not be created by the user:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: networkservicemesh.io/v1alpha1
kind: NetworkServiceEndpoint
metadata:
  generateName: p2p
  labels:
    device: device-2
    networkservicename: p2p
  name: p2ppdp2d
spec:
  networkservicename: p2p
  nsmname: nsm-control-plane
  payload: IP
status:
  state: RUNNING
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next bit is the manifest of the network service itself. Its goal is to establish a relationship between multiple clients and endpoints of a service by matching their network service labels.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: networkservicemesh.io/v1alpha1
kind: NetworkService
metadata:
  name: p2p
spec:
  matches:
  - match: 
    sourceSelector:
      link: net-0
    route:
    - destination: 
      destinationSelector:
        device: device-2
  payload: IP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final bit is the &amp;ldquo;client&amp;rdquo; side of the link which will have the following pod manifest. Note that the format of &lt;code&gt;NS_NETWORKSERVICEMESH_IO&lt;/code&gt; variable is the same as the one used in &lt;a href=&#34;https://github.com/networkservicemesh/networkservicemesh/blob/master/docs/spec/admission.md#what-to-trigger-on&#34; target=&#34;_blank&#34;&gt;annotations&lt;/a&gt; and can be read as &amp;ldquo;client requesting a &lt;code&gt;p2p&lt;/code&gt; service with two labels (&lt;code&gt;link=net-0&lt;/code&gt; and &lt;code&gt;peerif=eth21&lt;/code&gt;) and wants to connect to it over a local interface called &lt;code&gt;eth12&lt;/code&gt;&amp;rdquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: device-1
spec:
  containers:
  - image: alpine:latest
    command: [&amp;quot;tail&amp;quot;, &amp;quot;-f&amp;quot;, &amp;quot;/dev/null&amp;quot;]
    name: alpine
  - name: nsc-sidecar
    image: networkservicemesh/topology-sidecar-nsc:master
    env:
    - name: NS_NETWORKSERVICEMESH_IO
      value: p2p/eth12?link=net-0&amp;amp;peerif=eth21
    resources:
      limits:
        networkservicemesh.io/socket: &amp;quot;1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The client&amp;rsquo;s sidecar will read the above environment variable and send a connection request to the local control plane agent which will perform the following sequence of steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Locate a network service called &lt;code&gt;p2p&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Find a match based on client-provided labels (&lt;code&gt;link=net-0&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Try to find a matching network service endpoint (&lt;code&gt;device=device-2&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Contact the remote agent hosting a matching endpoint (found in NSE CRDs) and relay the connection request.&lt;/li&gt;
&lt;li&gt;If the request gets accepted by the endpoint, instruct the local forwarding agent to set up pod&amp;rsquo;s networking.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;topology-orchestration-with-k8s-topo&#34;&gt;Topology orchestration with k8s-topo&lt;/h1&gt;

&lt;p&gt;Looking at the above manifests, it&amp;rsquo;s clear that writing them manually, even for smaller topologies, can be a serious burden. That&amp;rsquo;s why I&amp;rsquo;ve adapted the &lt;a href=&#34;https://github.com/networkop/k8s-topo&#34; target=&#34;_blank&#34;&gt;k8s-topo&lt;/a&gt; tool that I&amp;rsquo;ve written originally for &lt;a href=&#34;https://github.com/networkop/meshnet-cni&#34; target=&#34;_blank&#34;&gt;meshnet-cni&lt;/a&gt; to produce and instantiate NSM-compliant manifest based on a single light-weight topology YAML file. The only thing that&amp;rsquo;s needed to make it work with NSM is to add a &lt;code&gt;nsm: true&lt;/code&gt; to the top of the file, e.g.:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;nsm: true
links:
  - endpoints: [&amp;quot;device-1:eth12&amp;quot;, &amp;quot;device-2:eth21&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Behind the scenes, k8s-topo will create the required network service manifest and configure all pods with correct sidecars and variables. As an added bonus, it will still attempt to inject startup configs and expose ports as described &lt;a href=&#34;https://github.com/networkop/k8s-topo&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/k8s-nsm.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;nsm-vs-meshnet-for-network-simulations&#34;&gt;NSM vs Meshnet for network simulations&lt;/h1&gt;

&lt;p&gt;In the context of virtual network simulations, both NSM and meshnet-cni can perform similar functions, however, their implementation and modes of operation are rather different. Here are the main distinctions of a CNI plugin approach:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All networking is setup BEFORE the pod is started.&lt;/li&gt;
&lt;li&gt;CNI plugin does all the work so there&amp;rsquo;s no need for sidecar containers.&lt;/li&gt;
&lt;li&gt;A very thin code base for a very specific use case.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And here are some of the distinctions of an NSM-based approach:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All networking is setup AFTER the pod is started.&lt;/li&gt;
&lt;li&gt;This does come with a requirement for a sidecar container, but potentially allows for runtime reconfiguration.&lt;/li&gt;
&lt;li&gt;No requirement for a CNI plugin at all.&lt;/li&gt;
&lt;li&gt;More generic use cases are possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the end, none of the options limit the currently available featureset of k8s-topo and the choice can be done based on the characteristics of an individual environment, e.g. if it&amp;rsquo;s a managed k8s from GCP (GKE) or Azure (AKS) then most likely you&amp;rsquo;ll be running &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#kubenet&#34; target=&#34;_blank&#34;&gt;kubenet&lt;/a&gt; and won&amp;rsquo;t have an option to install any CNI plugin at all, in which case NSM can be the only available solution.&lt;/p&gt;

&lt;h1 id=&#34;demo&#34;&gt;Demo&lt;/h1&gt;

&lt;p&gt;Now it&amp;rsquo;s demo time and I&amp;rsquo;ll show how to use k8s-topo together with NSM to build a 10-node virtual router topology. We start by spinning up a local &lt;a href=&#34;https://github.com/kubernetes-sigs/kind&#34; target=&#34;_blank&#34;&gt;kind&lt;/a&gt; kubernetes cluster and installing NSM on it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/networkservicemesh/networkservicemesh
cd networkservicemesh
make helm-init
SPIRE_ENABLED=false INSECURE=true FORWARDING_PLANE=kernel make helm-install-nsm 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we install the k8s-topo deployment and connect to the pod running it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f https://raw.githubusercontent.com/networkop/k8s-topo/master/manifest.yml
kubectl exec -it deploy/k8s-topo -- sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For demonstration purposes I&amp;rsquo;ll use a random 10-node tree topology generated using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Loop-erased_random_walk&#34; target=&#34;_blank&#34;&gt;loop-erased random walk&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./examples/builder/builder 10 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only thing needed to make it work with NSM is set the &lt;code&gt;nsm&lt;/code&gt; flag to &lt;code&gt;true&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -i &#39;$ a\nsm: true&#39; ./examples/builder/random.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now everything&amp;rsquo;s ready for us to instantiate the topology inside k8s:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;k8s-topo --create ./examples/builder/random.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once all the pods are up, we can issue a ping from one of the routers to every other router in the topology and confirm the connectivity between their loopback IPs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i in `seq 0 9`; do (kubectl exec qrtr-192-0-2-0 -c router -- ping -c 1 192.0.2.$i|grep loss); done

1 packets transmitted, 1 packets received, 0% packet loss
1 packets transmitted, 1 packets received, 0% packet loss
1 packets transmitted, 1 packets received, 0% packet loss
1 packets transmitted, 1 packets received, 0% packet loss
1 packets transmitted, 1 packets received, 0% packet loss
1 packets transmitted, 1 packets received, 0% packet loss
1 packets transmitted, 1 packets received, 0% packet loss
1 packets transmitted, 1 packets received, 0% packet loss
1 packets transmitted, 1 packets received, 0% packet loss
1 packets transmitted, 1 packets received, 0% packet loss
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to have a look at your topology, it&amp;rsquo;s possible to make k8s-topo generate a D3 graph of all pods and their connections and view it in the browser:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;k8s-topo --graph ./examples/builder/random.yml
INFO:__main__:D3 graph created
INFO:__main__:URL: http://172.17.0.3:30000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/k8s-nsm-topo.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Network-as-a-Service Part 3 - Authentication and Admission control</title>
      <link>https://networkop.co.uk/post/2019-06-naas-p3/</link>
      <pubDate>Thu, 27 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2019-06-naas-p3/</guid>
      <description>

&lt;p&gt;In the previous two posts, we&amp;rsquo;ve seen how to &lt;a href=&#34;https://networkop.co.uk/post/2019-06-naas-p2/&#34;&gt;build&lt;/a&gt; a custom network API with Kubernetes CRDs and &lt;a href=&#34;https://networkop.co.uk/post/2019-06-naas-p1/&#34;&gt;push&lt;/a&gt; the resulting configuration to network devices. In this post, we&amp;rsquo;ll apply the final touches by enabling oAuth2 authentication and enforcing separation between different tenants. All of these things are done while the API server processes incoming requests, so it would make sense to have a closer look at how it does that first.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-request-admission-pipeline&#34;&gt;Kubernetes request admission pipeline&lt;/h2&gt;

&lt;p&gt;Every incoming request has to go through several stages before it can get accepted and persisted by the API server. Some of these stages are mandatory (e.g. authentication), while some can be added through webhooks. The following diagram comes from another &lt;a href=&#34;https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/&#34; target=&#34;_blank&#34;&gt;blogpost&lt;/a&gt; that covers each one of these stages in detail:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/admission-controller-phases.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Specifically for NaaS platform, this is how we&amp;rsquo;ll use the above stages:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;All users will authenticate with Google and get mapped to individual namespace/tenant based on their google alias.&lt;/li&gt;
&lt;li&gt;Mutating webhook will be used to inject default values into each request and allow users to define ranges as well as individual ports.&lt;/li&gt;
&lt;li&gt;Object schema validation will do the syntactic validation of each request.&lt;/li&gt;
&lt;li&gt;Validating webhook will perform the semantic validation to make sure users cannot change ports assigned to a different tenant.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The following sections will cover these stages individually.&lt;/p&gt;

&lt;h2 id=&#34;authenticating-with-google&#34;&gt;Authenticating with Google&lt;/h2&gt;

&lt;p&gt;Typically, external users are authenticated using X.509 certificates, however, lack of CRL or  OCSP support in Kubernetes creates a problem since lost or exposed certs cannot be revoked. One of the alternatives is to use &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens&#34; target=&#34;_blank&#34;&gt;OpenID Connect&lt;/a&gt; which works on top of the OAuth 2.0 protocol and is supported by a few very big identity providers like Google, Microsoft and Salesforce. Although OIDC has its own shortcomings (read &lt;a href=&#34;https://blog.gini.net/frictionless-kubernetes-openid-connect-integration-f1c356140937&#34; target=&#34;_blank&#34;&gt;this blogpost&lt;/a&gt; for details), it is still often preferred over X.509.&lt;/p&gt;

&lt;p&gt;In order to authenticate users with OIDC, we need to do three things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Configure the API server to bind different user aliases to their respective tenants.&lt;/li&gt;
&lt;li&gt;Authenticate with the identity provider and get a signed token.&lt;/li&gt;
&lt;li&gt;Update local credentials to use this token.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first step is pretty straightforward and can be done with a simple RBAC &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-3/oidc/manifest.yaml&#34; target=&#34;_blank&#34;&gt;manifest&lt;/a&gt;. The latter two steps can either be done manually or automatically with the help of &lt;a href=&#34;https://github.com/gini/dexter&#34; target=&#34;_blank&#34;&gt;dexter&lt;/a&gt;. NaaS Github repo contains a sample two-liner &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-3/dexter-auth-public.sh&#34; target=&#34;_blank&#34;&gt;bash script&lt;/a&gt; that uses dexter to authenticate with Google and save the token in the local &lt;code&gt;~/.kube/config&lt;/code&gt; file.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;All that&amp;rsquo;s required from a NaaS administrator is to maintain an up-to-date tenant role bindings and users can authenticate and maintain their tokens independently.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;mutating-incoming-requests&#34;&gt;Mutating incoming requests&lt;/h2&gt;

&lt;p&gt;Mutating webhooks are commonly used to inject additional information (a sidecar proxy for service meshes) or defaults values (default CPU/memory) into incoming requests. Both mutating and validating webhooks get triggered based on a set of &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-3/webhooks/template-webhook.yaml&#34; target=&#34;_blank&#34;&gt;rules&lt;/a&gt; that match the API group and type of the incoming request. If there&amp;rsquo;s a match, a webhook gets called by the API server with an HTTP POST request containing the full body of the original request. The NaaS mutating &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-3/webhooks/mutate.py&#34; target=&#34;_blank&#34;&gt;webhook&lt;/a&gt; is written in Python/Flask and the first thing it does is extract the payload and its type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;request_info = request.json
modified_spec = copy.deepcopy(request_info)
workload_type = modified_spec[&amp;quot;request&amp;quot;][&amp;quot;kind&amp;quot;][&amp;quot;kind&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we set the default values and normalize ports:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if workload_type == &amp;quot;Interface&amp;quot;:
    defaults = get_defaults()
    set_intf_defaults(modified_spec[&amp;quot;request&amp;quot;][&amp;quot;object&amp;quot;][&amp;quot;spec&amp;quot;], defaults)
    normalize_ports(modified_spec[&amp;quot;request&amp;quot;][&amp;quot;object&amp;quot;][&amp;quot;spec&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last function expands interface ranges, i.e. translates &lt;code&gt;1-5&lt;/code&gt; into &lt;code&gt;1,2,3,4,5&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for port in ports:
    if not &amp;quot;-&amp;quot; in port:
        result.append(str(port))
    else:
        start, end = port.split(&amp;quot;-&amp;quot;)
        for num in range(int(start), int(end) + 1):
            result.append(str(num))  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we generate a json patch from the diff between the original and the mutated request, build a response and send it back to the API server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;patch = jsonpatch.JsonPatch.from_diff(
    request_info[&amp;quot;request&amp;quot;][&amp;quot;object&amp;quot;], modified_spec[&amp;quot;request&amp;quot;][&amp;quot;object&amp;quot;]
)
admission_response = {
    &amp;quot;allowed&amp;quot;: True,
    &amp;quot;uid&amp;quot;: request_info[&amp;quot;request&amp;quot;][&amp;quot;uid&amp;quot;],
    &amp;quot;patch&amp;quot;: base64.b64encode(str(patch).encode()).decode(),
    &amp;quot;patchtype&amp;quot;: &amp;quot;JSONPatch&amp;quot;,
}
return jsonify(admissionReview = {&amp;quot;response&amp;quot;: admission_response})
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;The &lt;a href=&#34;https://kubernetes.io/blog/2019/06/19/kubernetes-1-15-release-announcement/&#34; target=&#34;_blank&#34;&gt;latest&lt;/a&gt; (v1.15) release of Kubernetes has added support for default values to be defined inside the OpenAPI validation schema, making the job of writing mutating webhooks a lot easier.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;validating-incoming-requests&#34;&gt;Validating incoming requests&lt;/h2&gt;

&lt;p&gt;As we&amp;rsquo;ve seen in the &lt;a href=&#34;https://networkop.co.uk/post/2019-06-naas-p2/&#34;&gt;previous post&lt;/a&gt;, it&amp;rsquo;s possible to use OpenAPI schema to perform syntactic validation of incoming requests, i.e. check the structure and the values of payload variables. This function is very similar to what you can &lt;a href=&#34;http://plajjan.github.io/validating-data-with-YANG/&#34; target=&#34;_blank&#34;&gt;accomplish&lt;/a&gt; with a YANG model and, in theory, OpenAPI schema can be converted to YANG and &lt;a href=&#34;http://ipengineer.net/2018/10/yang-openapi-swagger-code-generation/&#34; target=&#34;_blank&#34;&gt;vice versa&lt;/a&gt;. However useful, such validation only takes into account a single input and cannot cross-correlate this data with other sources. In our case, the main goal is to protect one tenant&amp;rsquo;s data from being overwritten by request coming from another tenant. In Kubernetes, semantic validation is commonly done using &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook&#34; target=&#34;_blank&#34;&gt;validating&lt;/a&gt; admission webhooks and one of the most interesting tools in this landscape is &lt;a href=&#34;https://www.openpolicyagent.org/docs/v0.10.7/kubernetes-admission-control/&#34; target=&#34;_blank&#34;&gt;Open Policy Agent&lt;/a&gt; and its policy language called Rego.&lt;/p&gt;

&lt;h4 id=&#34;using-opa-s-policy-language&#34;&gt;Using OPA&amp;rsquo;s policy language&lt;/h4&gt;

&lt;p&gt;Rego is a special-purpose DSL with &amp;ldquo;rich support for traversing nested documents&amp;rdquo;. What this means is that it can iterate over dictionaries and lists without using traditional for loops. When it encounters an iterable data structure, it will automatically expand it to include all of its possible values. I&amp;rsquo;m not going to try to explain how &lt;a href=&#34;https://www.openpolicyagent.org/docs/v0.10.7/how-does-opa-work/&#34; target=&#34;_blank&#34;&gt;opa works&lt;/a&gt; in this post, instead I&amp;rsquo;ll show how to use it to solve our particular problem. Assuming that an incoming request is stored in the &lt;code&gt;input&lt;/code&gt; variable and &lt;code&gt;devices&lt;/code&gt; contain all custom device resources, this is how a Rego policy would look like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input.request.kind.kind == &amp;quot;Interface&amp;quot;
new_tenant := input.request.namespace
port := input.request.object.spec.services[i].ports[_]
new_device := input.request.object.spec.services[i].devicename
existing_device_data := devices[_][lower(new_device)].spec
other_tenant := existing_device_data[port].annotations.namespace
not new_tenant == other_tenant
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-3/webhooks/validate.rego&#34; target=&#34;_blank&#34;&gt;actual policy&lt;/a&gt; contains more than 7 lines but the most important ones are listed above and perform the following sequence of actions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Verify that the incoming request is of kind &lt;code&gt;Interface&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Extract its namespace and save it in the &lt;code&gt;new_tenant&lt;/code&gt; variable&lt;/li&gt;
&lt;li&gt;Save all ports in the &lt;code&gt;port&lt;/code&gt; variable&lt;/li&gt;
&lt;li&gt;Remember which device those ports belong to in the &lt;code&gt;new_device&lt;/code&gt; variables&lt;/li&gt;
&lt;li&gt;Extract existing port allocation information for each one of the above devices&lt;/li&gt;
&lt;li&gt;If any of the ports from the incoming request is found in the existing data, record its owner&amp;rsquo;s namespace&lt;/li&gt;
&lt;li&gt;Deny the request if the requesting port owner (tenant) is different from the current tenant.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Although Rego may not be that easy to write (or debug), it&amp;rsquo;s very easy to read, compared to an equivalent implemented in, say, Python, which would have taken x3 the number of lines and contain multiple for loops and conditionals. Like any DSL, it strives to strike a balance between readability and flexibility, while abstracting away less important things like web server request parsing and serialising.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The same functionality can be implemented in any standard web server (e.g. Python+Flask), so using OPA is not a requirement&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;

&lt;p&gt;This is a complete end-to-end demo of Network-as-a-Service platform and encompasses all the demos from the previous posts. The code for this demo is available &lt;a href=&#34;https://github.com/networkop/network-as-a-service/archive/part-3.zip&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and can be run on any Linux OS with Docker.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/naas-p3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;0-prepare-for-oidc-authentication&#34;&gt;0. Prepare for OIDC authentication&lt;/h4&gt;

&lt;p&gt;For this demo, I&amp;rsquo;ll only use a single non-admin user. Before you run the rest of the steps, you need to make sure you&amp;rsquo;ve followed &lt;a href=&#34;https://github.com/gini/dexter&#34; target=&#34;_blank&#34;&gt;dexter&lt;/a&gt; to setup google credentials and update OAuth client and user IDs in &lt;code&gt;kind.yaml&lt;/code&gt;, &lt;code&gt;dexter-auth.sh&lt;/code&gt; and &lt;code&gt;oidc/manifest.yaml&lt;/code&gt; files.&lt;/p&gt;

&lt;h4 id=&#34;1-build-the-test-topology&#34;&gt;1. Build the test topology&lt;/h4&gt;

&lt;p&gt;This step assumes you have &lt;a href=&#34;https://github.com/networkop/docker-topo&#34; target=&#34;_blank&#34;&gt;docker-topo&lt;/a&gt; installed and c(vEOS) image &lt;a href=&#34;https://github.com/networkop/docker-topo/tree/master/topo-extra-files/veos&#34; target=&#34;_blank&#34;&gt;built&lt;/a&gt; and available in local docker registry.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make topo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This test topology can be any Arista EOS device reachable from the localhost. If using a different test topology, be sure to update the &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-3/topo/inventory.yaml&#34; target=&#34;_blank&#34;&gt;inventory&lt;/a&gt; file.&lt;/p&gt;

&lt;h4 id=&#34;2-build-the-kubernetes-cluster&#34;&gt;2. Build the Kubernetes cluster&lt;/h4&gt;

&lt;p&gt;The following step will build a docker-based &lt;a href=&#34;https://github.com/kubernetes-sigs/kind&#34; target=&#34;_blank&#34;&gt;kind&lt;/a&gt; cluster with a single control plane and a single worker node.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;3-check-that-the-cluster-is-functional&#34;&gt;3. Check that the cluster is functional&lt;/h4&gt;

&lt;p&gt;The following step will build a base docker image and push it to dockerhub. It is assumed that the user has done &lt;code&gt;docker login&lt;/code&gt; and has his username saved in the &lt;code&gt;DOCKERHUB_USER&lt;/code&gt; environment variable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export KUBECONFIG=&amp;quot;$(kind get kubeconfig-path --name=&amp;quot;naas&amp;quot;)&amp;quot;
make warmup
kubectl get pod test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a 100MB image, so it may take a few minutes for test pod to transition from &lt;code&gt;ContainerCreating&lt;/code&gt; to &lt;code&gt;Running&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-build-the-naas-platform&#34;&gt;4. Build the NaaS platform&lt;/h4&gt;

&lt;p&gt;The next command will install and configure both mutating and validating admission webhooks, the watcher and scheduler services and all of the required CRDs and configmaps.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make build
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;5-authenticate-with-google&#34;&gt;5. Authenticate with Google&lt;/h4&gt;

&lt;p&gt;Assuming all files from step 0 have been updated correctly, the following command will open a web browser and prompt you to select a google account to authenticate with.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make oidc-build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From now on, you should be able to switch to your google-authenticated user like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context mk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And back to the admin user like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context kubernetes-admin@naas
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;6-test&#34;&gt;6. Test&lt;/h4&gt;

&lt;p&gt;To demonstrate how everything works, I&amp;rsquo;m going to issue three API requests. The &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-3/crds/03_cr.yaml&#34; target=&#34;_blank&#34;&gt;first&lt;/a&gt; API request will set up a large range of ports on test switches.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context mk
kubectl apply -f crds/03_cr.yaml                 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-3/crds/04_cr.yaml&#34; target=&#34;_blank&#34;&gt;second&lt;/a&gt; API request will try to re-assign some of these ports to a different tenant and will get denied by the validating controller.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context kubernetes-admin@naas
kubectl apply -f crds/04_cr.yaml        
Error from server (Port 11@deviceA is owned by a different tenant: tenant-a (request request-001), Port 12@deviceA is owned by a different tenant: tenant-a (request request-001),
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-3/crds/05_cr.yaml&#34; target=&#34;_blank&#34;&gt;third&lt;/a&gt; API request will update some of the ports from the original request within the same tenant.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config use-context mk
kubectl apply -f crds/05_cr.yaml                 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following result can be observed on one of the switches:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;devicea#sh run int eth2-3
interface Ethernet2
   description request-002
   shutdown
   switchport trunk allowed vlan 100
   switchport mode trunk
   spanning-tree portfast
interface Ethernet3
   description request-001
   shutdown
   switchport trunk allowed vlan 10
   switchport mode trunk
   spanning-tree portfast
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;outro&#34;&gt;Outro&lt;/h2&gt;

&lt;p&gt;Currently, Network-as-a-Service platform is more of a proof-of-concept of how to expose parts of the device data model for end users to consume in a safe and controllable way. Most of it is built out of standard Kubernetes component and the total amount of Python code is under 1000 lines, while the code itself is pretty linear. I have plans to add more things like an SPA front-end, Git and OpenFaaS integration, however, I don&amp;rsquo;t want to invest too much time until I get some sense of external interest. So if this is something that you like and think you might want to try, ping me via social media and I&amp;rsquo;ll try to help get things off the ground.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Network-as-a-Service Part 2 - Designing a Network API</title>
      <link>https://networkop.co.uk/post/2019-06-naas-p2/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2019-06-naas-p2/</guid>
      <description>

&lt;p&gt;In the &lt;a href=&#34;https://networkop.co.uk/post/2019-06-naas-p1/&#34;&gt;previous post&lt;/a&gt;, we&amp;rsquo;ve examined the foundation of the Network-as-a-Service platform. A couple of services were used to build the configuration from data models and templates and push it to network devices using Nornir and Napalm. In this post, we&amp;rsquo;ll focus on the user-facing part of the platform. I&amp;rsquo;ll show how to expose a part of the device data model via a custom API built on top of Kubernetes and how to tie it together with the rest of the platform components.&lt;/p&gt;

&lt;h2 id=&#34;interacting-with-a-kubernetes-api&#34;&gt;Interacting with a Kubernetes API&lt;/h2&gt;

&lt;p&gt;There are two main ways to interact with a &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/kubernetes-api/&#34; target=&#34;_blank&#34;&gt;Kubernetes API&lt;/a&gt;: one using a &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/client-libraries/&#34; target=&#34;_blank&#34;&gt;client library&lt;/a&gt;, which is how NaaS services communicate with K8s internally, the other way is with a command line tool called &lt;code&gt;kubectl&lt;/code&gt;, which is intended to be used by humans. In either case, each API request is expected to contain at least the following fields:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;apiVersion&lt;/strong&gt; - all API resources are grouped and versioned to allow multiple versions of the same kind to co-exist at the same time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;kind&lt;/strong&gt; - defines the type of object to be created.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;metadata&lt;/strong&gt; - collection of request attributes like name, namespaces, labels etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;spec&lt;/strong&gt; - the actual payload of the request containing the attributes of the requested object.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to describe these fields in a concise and human-readable way, API requests are often written in YAML, which is why you&amp;rsquo;ll see a lot of YAML snippets throughout this post. You can treat each one of those snippets as a separate API call that can be applied to a K8s cluster using a &lt;code&gt;kubectl apply&lt;/code&gt; command.&lt;/p&gt;

&lt;h2 id=&#34;designing-a-network-interface-api&#34;&gt;Designing a Network Interface API&lt;/h2&gt;

&lt;p&gt;The structure and logic behind any user-facing API can be very customer-specific. Although the use-case I&amp;rsquo;m focusing on here is a very simple one, my goal is to demonstrate the idea which, if necessary, can be adapted to other needs and requirements. So let&amp;rsquo;s assume we want to allow end users to change access ports configuration of multiple devices and this is how a sample API request may look like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: network.as.a.service/v1
kind: Interface
metadata:
  name: request-001
  namespace: tenant-a
spec:
  services:
    - devicename: deviceA
      ports: [&amp;quot;1&amp;quot;, &amp;quot;15&amp;quot;]
      vlan: 10
      trunk: yes
    - devicename: deviceB
      ports: [&amp;quot;1&amp;quot;,&amp;quot;10&amp;quot;, &amp;quot;11&amp;quot;]
      vlan: 110
      trunk: no
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a few things to note in the above request:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Every request will have a unique name per namespace (tenant).&lt;/li&gt;
&lt;li&gt;The main payload inside the &lt;code&gt;.spec&lt;/code&gt; property is a list of (VLAN) network services that need to be configured on network devices.&lt;/li&gt;
&lt;li&gt;Each element of the list contains the name of the device, list of ports and a VLAN number to be associated with them.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now let&amp;rsquo;s see what it takes to make Kubernetes &amp;ldquo;understand&amp;rdquo; this API.&lt;/p&gt;

&lt;h2 id=&#34;introducing-kubernetes-crds&#34;&gt;Introducing Kubernetes CRDs&lt;/h2&gt;

&lt;p&gt;API server is the main component of the control plane of a Kubernetes cluster. It receives all incoming requests, validates them, notifies the respective controllers and stores them in a database.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/k8s-api.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Apart from the APIs exposing a set of standard resources, there&amp;rsquo;s an ability to define &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/&#34; target=&#34;_blank&#34;&gt;custom resources&lt;/a&gt; - user-defined data structures that an API server can accept and store. Custom resources are the main building blocks for a lot of platforms built on top of K8s and at the very least they allow users to store and retrieve some arbitrary YAML data.&lt;/p&gt;

&lt;p&gt;In order to be able to create a custom resource, we need to define it with a custom resource definition (CRD) object that would describe the name of the resource, the api group it belongs to and, optionally, the structure and values of the YAML data via OpenAPI v3 &lt;a href=&#34;https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#schemaObject&#34; target=&#34;_blank&#34;&gt;schema&lt;/a&gt;. This is how a CRD for the above Interface API would look like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: interfaces.network.as.a.service
spec:
  group: network.as.a.service
  versions:
  - name: v1
    served: true
    storage: true
  scope: Namespaced
  subresources:
    status: {}
  names:
    plural: interfaces
    singular: interface
    kind: Interface
    shortNames:
    - intf
  validation:
    openAPIV3Schema:
      required: [&amp;quot;spec&amp;quot;]
      properties:
        spec:
          required: [&amp;quot;services&amp;quot;]
          properties:
            services:
              type: array
              items: 
                type: object
                required: [&amp;quot;devicename&amp;quot;, &amp;quot;vlan&amp;quot;, &amp;quot;ports&amp;quot;]
                properties:
                  devicename: 
                    type: string
                  vlan:
                    type: integer
                    minimum: 1
                    maximum: 4094
                  ports:
                    type: array
                    items:
                      type: string
                  trunk:
                    type: boolean
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As soon as we &lt;code&gt;kubectl apply&lt;/code&gt; the above YAML, our API server will expose the &lt;code&gt;Interface&lt;/code&gt; API  for all external users to perform standard CRUD operations on, and store the results alongside other K8s resources in etcd datastore.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-custom-controllers&#34;&gt;Kubernetes custom controllers&lt;/h2&gt;

&lt;p&gt;Custom resources, by themselves, do not provide any way to define a business logic of what to do with their data. This job is normally performed by Kubernetes controllers that &amp;ldquo;watch&amp;rdquo; events that happen to these resources and perform actions based on that. This tandem between custom controllers and CRDs is so common, it led to the creation of an &lt;a href=&#34;https://coreos.com/operators/&#34; target=&#34;_blank&#34;&gt;operator pattern&lt;/a&gt; and a whole &lt;a href=&#34;https://twitter.com/alexellisuk/status/1132755044313522176&#34; target=&#34;_blank&#34;&gt;slew&lt;/a&gt; of operator frameworks with languages ranging from Go to Ansible.&lt;/p&gt;

&lt;p&gt;However, as I&amp;rsquo;ve mentioned in the &lt;a href=&#34;https://networkop.co.uk/post/2019-06-naas-p1/&#34;&gt;previous post&lt;/a&gt;, sometimes using a framework does not give you any benefit and after having looked at some of the most popular ones, I&amp;rsquo;ve decided to settle on my own implementation which turned out to be a lot easier. In essence, all that&amp;rsquo;s required from a custom controller is to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Subscribe to events about a custom resource (via K8s API).&lt;/li&gt;
&lt;li&gt;Once an event is received, perform the necessary business logic.&lt;/li&gt;
&lt;li&gt;Update the resource status if required.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;rsquo;s see how these custom controllers are implemented inside the NaaS platform.&lt;/p&gt;

&lt;h2 id=&#34;naas-controller-architecture&#34;&gt;NaaS controller architecture&lt;/h2&gt;

&lt;p&gt;NaaS platform has a special &lt;strong&gt;watcher&lt;/strong&gt; service that implements all custom controller logic. Its main purpose is to process incoming &lt;code&gt;Interface&lt;/code&gt; API events and generate a device-centric interface data model based on them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/naas-p2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Internally, the watcher service is built out of two distinct controllers:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;interface-watcher&lt;/strong&gt; - listens to &lt;code&gt;Interface&lt;/code&gt; API events and updates a custom &lt;code&gt;Device&lt;/code&gt; resource that stores an aggregated device-centric view of all interface API requests received up to date. Once all the changes have been made, it updates the status of the request and notifies the scheduler about all the devices affected by this event.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;device-watcher&lt;/strong&gt; - listens to &lt;code&gt;Device&lt;/code&gt; API events and generates configmaps containing a device interface data model. These configmaps are then consumed by enforcers to build the access interface part of the total device configuration.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;interface-watcher-architecture&#34;&gt;Interface-watcher architecture&lt;/h2&gt;

&lt;p&gt;The main loop of the &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-2/watcher/interface-watcher.py&#34; target=&#34;_blank&#34;&gt;interface-watcher&lt;/a&gt; receives &lt;code&gt;Interface&lt;/code&gt; API events as they arrive and processes each network service individually:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for network_service in event_object[&amp;quot;spec&amp;quot;][&amp;quot;services&amp;quot;]:
    results.append(
        process_service(event_metadata, network_service, action, defaults)
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For each service, depending on the type of the event, we either add, update or delete ports from the global device-centric model:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;device = get_or_create_device(device_name, defaults)
device_data = device[&amp;quot;spec&amp;quot;]
if action == &amp;quot;ADDED&amp;quot;:
    device_data = add_ports(
        network_service, device_data, resource_name, resource_namespace
    )
elif action == &amp;quot;DELETED&amp;quot;:
    device_data = delete_ports(network_service, device_data, resource_name)
elif action == &amp;quot;MODIFIED&amp;quot;:
    device_data = delete_all_ports(device_data, resource_name)
    device_data = add_ports(
        network_service, device_data, resource_name, resource_namespace
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For each of the added ports, we copy all settings from the original request and annotate it with metadata about its current owner and tenant:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ports = origin.pop(&amp;quot;ports&amp;quot;)
for port in ports:
    destination[port] = dict()
    destination[port] = origin
    destination[port][&amp;quot;annotations&amp;quot;] = annotate(owner, namespace)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This results in the following custom &lt;code&gt;Device&lt;/code&gt; resource being created from the original &lt;code&gt;Interface&lt;/code&gt; API request:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: network.as.a.service/v1
kind: Device
metadata:
  name: devicea
  namespace: default
spec:
  &amp;quot;1&amp;quot;:
    annotations:
      namespace: tenant-a
      owner: request-001
      timestamp: &amp;quot;2019-06-19 22:09:02&amp;quot;
    trunk: true
    vlan: 10
  &amp;quot;15&amp;quot;:
    annotations:
      namespace: tenant-a
      owner: request-001
      timestamp: &amp;quot;2019-06-19 22:09:02&amp;quot;
    trunk: true
    vlan: 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As subsequent requests can add or overwrite port ownership information, metadata allows the controller to be selective about which ports to modify in order to not accidentally delete ports assigned to a different owner:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_destination = copy.deepcopy(destination)
for port in origin[&amp;quot;ports&amp;quot;]:
    if (port in destination) and (
        destination[port].get(&amp;quot;annotations&amp;quot;, {}).get(&amp;quot;owner&amp;quot;, &amp;quot;&amp;quot;) == owner
    ):
        log.debug(f&amp;quot;Removing port {port} from structured config&amp;quot;)
        new_destination.pop(port, None)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the event has been processed, interface-watcher updates the device resource with the new values:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;device[&amp;quot;spec&amp;quot;] = device_data
update_device(device_name, device, defaults)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last command triggers a MODIFIED event on the &lt;code&gt;Device&lt;/code&gt; CR and this is where the next controller kicks in.&lt;/p&gt;

&lt;h2 id=&#34;device-watcher-architecture&#34;&gt;Device-watcher architecture&lt;/h2&gt;

&lt;p&gt;The job of a &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-2/watcher/device-watcher.py&#34; target=&#34;_blank&#34;&gt;device-watcher&lt;/a&gt; is to, first, extract the payload from the above request:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;event_object = event[&amp;quot;object&amp;quot;]
event_metadata = event_object[&amp;quot;metadata&amp;quot;
device_name = event_metadata[&amp;quot;name&amp;quot;]
device_data = event_object[&amp;quot;spec&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The payload is then serialised into a string and saved as a configmap with additional pointers to Jinja template and order/priority number to help the enforcer build the full device configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k8s_api = client.CoreV1Api()
body = {
    &amp;quot;metadata&amp;quot;: {
        &amp;quot;name&amp;quot;: device_name,
        &amp;quot;annotations&amp;quot;: {&amp;quot;order&amp;quot;: &amp;quot;99&amp;quot;, &amp;quot;template&amp;quot;: &amp;quot;interface.j2&amp;quot;},
        &amp;quot;labels&amp;quot;: {&amp;quot;device&amp;quot;: device_name, &amp;quot;app&amp;quot;: &amp;quot;naas&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;model&amp;quot;},
    },
    &amp;quot;data&amp;quot;: {&amp;quot;structured-config&amp;quot;: yaml.safe_dump(device_data)},
}

k8s_api.replace_namespaced_config_map(
    device_name, event_metadata[&amp;quot;namespace&amp;quot;], body
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The remaining part of the workflow is similar to what was described in the previous post. The scheduler receives the request with the list of devices to be re-provisioned, spins up the required number of enforcers who collect all relevant data models, combine them with Jinja templates and push the new config.&lt;/p&gt;

&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;

&lt;p&gt;This demo will pick up from where the previous one has left off. The assumption is that the test topology, K8s cluster and scheduler/enforcer services are already deployed as described in the &lt;a href=&#34;https://networkop.co.uk/post/2019-06-naas-p1/&#34;&gt;previous post&lt;/a&gt;. The code for this demo can be downloaded &lt;a href=&#34;https://github.com/networkop/network-as-a-service/archive/part-2.zip&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;deploy-the-watcher-service&#34;&gt;Deploy the watcher service&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;make watcher-build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above command performs the following actions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Creates &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-2/crds/00_namespace.yaml&#34; target=&#34;_blank&#34;&gt;two namespaces&lt;/a&gt; that will represent different platform tenants&lt;/li&gt;
&lt;li&gt;Creates &lt;code&gt;Interface&lt;/code&gt; and &lt;code&gt;Device&lt;/code&gt; &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-2/crds/01_crd.yaml&#34; target=&#34;_blank&#34;&gt;CRD objects&lt;/a&gt; describing our custom APIs&lt;/li&gt;
&lt;li&gt;Deploys both watcher &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-2/watcher/manifest.yaml&#34; target=&#34;_blank&#34;&gt;custom controllers&lt;/a&gt; along with the necessary RBAC rules&lt;/li&gt;
&lt;li&gt;Uploads the interface &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-2/templates/interface.j2&#34; target=&#34;_blank&#34;&gt;jinja template&lt;/a&gt; to be used by enforcers&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;test&#34;&gt;Test&lt;/h2&gt;

&lt;p&gt;Issue the &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-2/crds/03_cr.yaml&#34; target=&#34;_blank&#34;&gt;first&lt;/a&gt; &lt;code&gt;Interface&lt;/code&gt; API call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -f crds/03_cr.yaml         
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the logs of the interface-watcher to make sure it&amp;rsquo;s picked up the &lt;code&gt;Interface&lt;/code&gt; ADDED event:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl logs deploy/interface-watcher
2019-06-20 08:20:01 INFO interface-watcher - interface_watcher: Watching Interface CRDs
2019-06-20 08:20:09 INFO interface-watcher - process_services: Received ADDED event request-001 of Interface kind
2019-06-20 08:20:09 INFO interface-watcher - process_service: Processing ADDED config for Vlans 10 on device devicea
2019-06-20 08:20:09 INFO interface-watcher - get_device: Reading the devicea device resource
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the logs of the device-watcher to make sure it has detected the &lt;code&gt;Device&lt;/code&gt; API event:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl logs deploy/device-watcher
2019-06-20 08:20:09 INFO device-watcher - update_configmaps: Updating ConfigMap for devicea
2019-06-20 08:20:09 INFO device-watcher - update_configmaps: Creating configmap for devicea
2019-06-20 08:20:09 INFO device-watcher - update_configmaps: Configmap devicea does not exist yet. Creating
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the logs of the scheduler service to see if it has been notified about the change:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl logs deploy/scheduler
2019-06-20 08:20:09 INFO scheduler - webhook: Got incoming request from 10.32.0.4
2019-06-20 08:20:09 INFO scheduler - webhook: Request JSON payload {&#39;devices&#39;: [&#39;devicea&#39;, &#39;deviceb&#39;]}
2019-06-20 08:20:09 INFO scheduler - create_job: Creating job job-6rlwg0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the logs of the enforcer service to see if device configs have been generated and pushed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl logs jobs/job-6rlwg0
2019-06-20 08:20:18 INFO enforcer - push_configs: Downloading Model configmaps
2019-06-20 08:20:18 INFO enforcer - get_configmaps: Retrieving the list of ConfigMaps matching labels {&#39;app&#39;: &#39;naas&#39;, &#39;type&#39;: &#39;model&#39;}
2019-06-20 08:20:18 INFO enforcer - push_configs: Found models: [&#39;devicea&#39;, &#39;deviceb&#39;, &#39;generic-cm&#39;]
2019-06-20 08:20:18 INFO enforcer - push_configs: Downloading Template configmaps
2019-06-20 08:20:18 INFO enforcer - get_configmaps: Retrieving the list of ConfigMaps matching labels {&#39;app&#39;: &#39;naas&#39;, &#39;type&#39;: &#39;template&#39;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we can check the result on the device itself:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;devicea#sh run int eth1
interface Ethernet1
   description request-001
   switchport trunk allowed vlan 10
   switchport mode trunk
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;coming-up&#34;&gt;Coming up&lt;/h2&gt;

&lt;p&gt;What we&amp;rsquo;ve covered so far is enough for end users to be able to modify access port settings on multiple devices via a standard API. However, there&amp;rsquo;s still nothing protecting the configuration created by one user from being overwritten by a request coming from a user in a different tenant. In the next post, I&amp;rsquo;ll show how to validate requests to make sure they do not cross the tenant boundaries. Additionally, I&amp;rsquo;ll show how to mutate incoming requests to be able to accept interface ranges and inject default values. To top it off, we&amp;rsquo;ll integrate NaaS with Google&amp;rsquo;s identity provider via OIDC to allow users to be mapped to different namespaces based on their google alias.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Network-as-a-Service Part 1 - Frameworkless automation</title>
      <link>https://networkop.co.uk/post/2019-06-naas-p1/</link>
      <pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2019-06-naas-p1/</guid>
      <description>

&lt;p&gt;Recently I&amp;rsquo;ve been pondering the idea of cloud-like method of consumption of traditional (physical) networks. My main premise for this was that users of a network don&amp;rsquo;t have to wait hours or days for their services to be provisioned when all that&amp;rsquo;s required is a simple change of an access port. Let me reinforce it by an example. In a typical data center network, the configuration of the core (fabric) is fairly static, while the config at the edge can change constantly as servers get added, moved or reconfigured. Things get even worse when using infrastructure-as-code with CI/CD pipelines to generate and test the configuration since it&amp;rsquo;s hard to expose only a subset of it all to the end users and it certainly wouldn&amp;rsquo;t make sense to trigger a pipeline every time a vlan is changed on an edge port.&lt;/p&gt;

&lt;p&gt;This is where Network-as-a-Service (NaaS) platform fits in. The idea is that it would expose the required subset of configuration to the end user and will take care of applying it to the devices in a fast and safe way. In this series of blogposts I will describe and demonstrate a prototype of such a platform, implemented on top of Kubernetes, using &lt;a href=&#34;https://napalm.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34;&gt;Napalm&lt;/a&gt; as southbound API towards the devices.&lt;/p&gt;

&lt;h2 id=&#34;frameworkless-automation&#34;&gt;Frameworkless automation&lt;/h2&gt;

&lt;p&gt;One thing I&amp;rsquo;ve decided NOT to do is build NaaS around a single automation framework. The tendency to use a single framework to solve all sorts of automation problems can lead to a lot of unnecessary hacking and additional complexity. When you&amp;rsquo;re finding yourself constantly writing custom libraries to perform some logic that can not be done natively within the framework, perhaps it&amp;rsquo;s time to step back and reassess your tools. The benefit of having a single tool, may not be worth the time and effort spent customising it. A much better approach is to split the functionality into multiple services and standardise what information is supposed to be passed between them. Exactly what microservices architecture is all about. You can still use frameworks within each service if it makes sense, but these can be easily swapped when a newer and better alternative comes along without causing a platform-wide impact.&lt;/p&gt;

&lt;p&gt;One problem that needs to be solved, however, is where to run all these microservices. The choice of Kubernetes here may seem like a bit of a stretch to some since it can get quite complicated to troubleshoot and manage. However, in return, I get a number of constructs (e.g. authentication, deployments, ingress) that are an integral part of any platform &amp;ldquo;for free&amp;rdquo;. After all, as Kelsey Hightower said:&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Kubernetes is a platform for building platforms. It&amp;#39;s a better place to start; not the endgame.&lt;/p&gt;&amp;mdash; Kelsey Hightower (@kelseyhightower) &lt;a href=&#34;https://twitter.com/kelseyhightower/status/935252923721793536?ref_src=twsrc%5Etfw&#34;&gt;November 27, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;So here is a list of reasons why I&amp;rsquo;ve decided to build NaaS on top of Kubernetes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I can define arbitrary APIs (via custom resources) with whatever structure I like.&lt;/li&gt;
&lt;li&gt;These resources are stored, versioned and can be exposed externally.&lt;/li&gt;
&lt;li&gt;With &lt;a href=&#34;https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#schemaObject&#34; target=&#34;_blank&#34;&gt;openAPI schema&lt;/a&gt;, I can define the structure and values of my APIs (similar to YANG but much easier to write).&lt;/li&gt;
&lt;li&gt;I get built-in multitenancy through namespaces.&lt;/li&gt;
&lt;li&gt;I get AAA with Role-based Access Control, and not just a simple passwords-in-a-text file kind of AAA, but proper TLS-based authentication with oAuth integration.&lt;/li&gt;
&lt;li&gt;I get a client-side code with libraries in python, js and go.&lt;/li&gt;
&lt;li&gt;I get admission controls that allow me to mutate (e.g. expand interface ranges) and validate (e.g. enforce per-tenant separation) requests before they get accepted.&lt;/li&gt;
&lt;li&gt;I get secret management to store sensitive information (e.g. device inventory)&lt;/li&gt;
&lt;li&gt;All data is stored in etcd, which can be easily backed up/restored.&lt;/li&gt;
&lt;li&gt;All variables, scripts, templates and data models are stored as k8s configmap resources and can be retrieved, updated and versioned.&lt;/li&gt;
&lt;li&gt;Operator pattern allows me to write a very simple code to &amp;ldquo;watch&amp;rdquo; the incoming requests and do some arbitrary logic described in any language or framework of my choice.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Not to mention all of the more standard capabilities like container orchestration, lifecycle management and auto-healing.&lt;/p&gt;

&lt;h2 id=&#34;the-foundation-of-naas&#34;&gt;The foundation of NaaS&lt;/h2&gt;

&lt;p&gt;Before I get to the end-user API part, I need to make sure I have the mechanism to modify the configuration of my network devices. Below is the high-level diagram that depicts how this can be implemented using two services:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scheduler&lt;/strong&gt; - a web server that accepts requests with the list of devices to be provisioned and schedules the enforcers to push it. This service is built on top of a K8s &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34; target=&#34;_blank&#34;&gt;deployment&lt;/a&gt; which controls the expected number and health of scheduler pods and recreates them if any one of them fails.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enforcer&lt;/strong&gt; - one or more job runners created by the scheduler, combining the data models and templates and using the result to replace the running configuration of the devices. This service is ephemeral as &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/&#34; target=&#34;_blank&#34;&gt;jobs&lt;/a&gt; will run to completion and stop, however, logs can still be viewed for some time after the completion.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/naas-p1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;scheduler-architecture&#34;&gt;Scheduler architecture&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-1/scheduler/scheduler.py&#34; target=&#34;_blank&#34;&gt;Scheduler&lt;/a&gt;, just like all the other services in NaaS, is written in Python. The web server component has a single webhook that handles incoming HTTP POST requests with JSON payload containing the list of devices.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@app.route(&amp;quot;/configure&amp;quot;, methods=[&amp;quot;POST&amp;quot;])
def webhook():
    log.info(f&amp;quot;Got incoming request from {request.remote_addr}&amp;quot;)
    payload = request.get_json(force=True)
    devices = payload.get(&amp;quot;devices&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next thing it does is read the device inventory mounted as a local volume from the Kubernetes secret store and decide how many devices to schedule on a single runner. This gives the flexibility to change the number of devices processed by a single runner (scale-up vs scale-out).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sliced_inventory = [x for x in inv_slicer(devices_inventory, step)]
schedule(sliced_inventory)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, for each slice of the inventory, scheduler creates a Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/&#34; target=&#34;_blank&#34;&gt;job&lt;/a&gt; based on a pre-defined template, with base64-encoded inventory slice as an environment variable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t = Template(job_template)
job_manifest = t.render(
  job={&amp;quot;name&amp;quot;: job_name, &amp;quot;inventory&amp;quot;: encode(inventory_slice)}
)

return api.create_namespaced_job(
  get_current_namespace(), yaml.safe_load(job_manifest), pretty=True
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order for the scheduler to function, it needs to have several supporting Kubernetes resources:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt; to perform the lifecycle management of the app&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt; to expose the deployed application internally&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ingress&lt;/strong&gt; to expose the above service to the outside world&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Configmap&lt;/strong&gt; to store the actual python script&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Secret&lt;/strong&gt; to store the device inventory&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RBAC&lt;/strong&gt; rules to allow scheduler to read configmaps and create jobs&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Most of these resources (with the exception of configmaps) are defined in a single &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-1/scheduler/manifest.yaml&#34; target=&#34;_blank&#34;&gt;manifest file&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;enforcer-architecture&#34;&gt;Enforcer architecture&lt;/h2&gt;

&lt;p&gt;The current implementation of the enforcer uses &lt;a href=&#34;https://nornir.readthedocs.io/en/stable/index.html&#34; target=&#34;_blank&#34;&gt;Nornir&lt;/a&gt; together with &lt;a href=&#34;https://nornir.readthedocs.io/en/stable/plugins/tasks/text.html#nornir.plugins.tasks.text.template_string&#34; target=&#34;_blank&#34;&gt;Jinja&lt;/a&gt; and &lt;a href=&#34;https://nornir.readthedocs.io/en/stable/plugins/tasks/networking.html#nornir.plugins.tasks.networking.napalm_configure&#34; target=&#34;_blank&#34;&gt;Napalm&lt;/a&gt; plugins. The choice of the framework here is arbitrary and Nornir can easily be replaced with Ansible or any other framework or script. The only coupling between the enforcer and the scheduler is the format of the inventory file, which can be changed quite easily if necessary.&lt;/p&gt;

&lt;p&gt;The enforcer runner is built out of two containers. The first one to run is an &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/init-containers/&#34; target=&#34;_blank&#34;&gt;init container&lt;/a&gt; that decodes the base64-encoded inventory and saves it into a file that is later used by the main container.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;encoded_inv = os.getenv(&amp;quot;INVENTORY&amp;quot;, &amp;quot;&amp;quot;)
decoded_inv = base64.b64decode(encoded_inv)
inv_yaml = yaml.safe_load(decoded_inv.decode())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The second container is the one that runs the device configuration &lt;a href=&#34;https://github.com/networkop/network-as-a-service/blob/part-1/enforcer/enforcer.py&#34; target=&#34;_blank&#34;&gt;logic&lt;/a&gt;. Firstly, it retrieves the list of all device data models and templates and passes them to the &lt;code&gt;push_config&lt;/code&gt; task.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;models = get_configmaps(labels={&amp;quot;app&amp;quot;: &amp;quot;naas&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;model&amp;quot;})
templates = get_configmaps(labels={&amp;quot;app&amp;quot;: &amp;quot;naas&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;template&amp;quot;})
result = nr.run(task=push_config, models=models, templates=templates)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inside that task, a list of sorted data models get combined with jinja templates to build the full device configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for ordered_model in sorted(my_models):
  model = yaml.safe_load(ordered_model.data.get(&amp;quot;structured-config&amp;quot;))
  template_name = ordered_model.metadata.annotations.get(&amp;quot;template&amp;quot;)
  for template in templates:
    if template.metadata.name == template_name:
      r = task.run(
        name=f&amp;quot;Building {template_name}&amp;quot;,
        task=template_string,
        template=template.data.get(&amp;quot;template&amp;quot;),
        model=model,
      )
      cli_config += r.result
      cli_config += &amp;quot;\n&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we push the resulting config to all the devices in the local inventory:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = task.run(
  task=networking.napalm_configure,
  replace=True,
  configuration=task.host[&amp;quot;config&amp;quot;],
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;

&lt;p&gt;Before we begin the demonstration, I wanted to mention a few notes about my code and test environments:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All code for this blogpost series will be stored in &lt;a href=&#34;https://github.com/networkop/network-as-a-service&#34; target=&#34;_blank&#34;&gt;NaaS&lt;/a&gt; Github repository, separated in different tagged branches (part-1, part-2, etc.)&lt;/li&gt;
&lt;li&gt;For this and subsequent demos I&amp;rsquo;ll be using a couple of Arista EOS devices connected back-to-back with 20 interfaces.&lt;/li&gt;
&lt;li&gt;All bash commands, their dependencies and variables are stored in a number of makefiles in the &lt;code&gt;.mk&lt;/code&gt; directory. I&amp;rsquo;ll provide the actual bash commands only when it&amp;rsquo;s needed for clarity, but all commands can be looked up in makefiles.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The code for this post can be downloaded &lt;a href=&#34;https://github.com/networkop/network-as-a-service/archive/part-1.zip&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;build-the-test-topology&#34;&gt;Build the test topology&lt;/h4&gt;

&lt;p&gt;Any two EOS devices can be used as a testbed, as long as they can be accessed over eAPI. I build my testbed with &lt;a href=&#34;https://github.com/networkop/docker-topo&#34; target=&#34;_blank&#34;&gt;docker-topo&lt;/a&gt; and c(vEOS) &lt;a href=&#34;https://github.com/networkop/docker-topo/tree/master/topo-extra-files/veos&#34; target=&#34;_blank&#34;&gt;image&lt;/a&gt;. This step will build a local topology with two containerised vEOS-lab devices:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make topo
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;build-the-local-kubernetes-cluster&#34;&gt;Build the local Kubernetes cluster&lt;/h4&gt;

&lt;p&gt;The following step will build a docker-based &lt;a href=&#34;https://github.com/kubernetes-sigs/kind&#34; target=&#34;_blank&#34;&gt;kind&lt;/a&gt; cluster with a single control plane and a single worker node.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;check-that-the-cluster-is-functional&#34;&gt;Check that the cluster is functional&lt;/h4&gt;

&lt;p&gt;The following step will build a base docker image and push it to dockerhub. It is assumed that the user has done &lt;code&gt;docker login&lt;/code&gt; and has his username saved in &lt;code&gt;DOCKERHUB_USER&lt;/code&gt; environment variable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export KUBECONFIG=&amp;quot;$(kind get kubeconfig-path --name=&amp;quot;naas&amp;quot;)&amp;quot;
make warmup
kubectl get pod test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a 100MB image, so it may take a few minutes for test pod to transition from &lt;code&gt;ContainerCreating&lt;/code&gt; to &lt;code&gt;Running&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;deploy-the-services&#34;&gt;Deploy the services&lt;/h4&gt;

&lt;p&gt;This next command will perform the following steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Upload the enforcer and scheduler scripts as configmaps.&lt;/li&gt;
&lt;li&gt;Create Traefik (HTTP proxy) daemonset to be used as ingress.&lt;/li&gt;
&lt;li&gt;Upload generic device data model along with its template and label them accordingly.&lt;/li&gt;
&lt;li&gt;Create a deployment, service and ingress resources for the scheduler service.&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;make scheduler-build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If running as non-root, the user may be prompted for a sudo password.&lt;/p&gt;

&lt;h4 id=&#34;test&#34;&gt;Test&lt;/h4&gt;

&lt;p&gt;In order to demonstrate how it works, I will do two things. First, I&amp;rsquo;ll issue a POST request from my localhost to the address registered on ingress (&lt;a href=&#34;http://api.naas/configure&#34; target=&#34;_blank&#34;&gt;http://api.naas/configure&lt;/a&gt;) with payload requesting the provisioning of all devices.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget -O- --post-data=&#39;{&amp;quot;devices&amp;quot;:[&amp;quot;all&amp;quot;]}&#39; --header=&#39;Content-Type:application/json&#39; http://api.naas/configure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A few seconds later, we can view the logs of the scheduler to confirm that it received the request:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl logs deploy/scheduler

2019-06-13 10:29:22 INFO scheduler - webhook: Got incoming request from 10.32.0.3
2019-06-13 10:29:22 INFO scheduler - webhook: Request JSON payload {&#39;devices&#39;: [&#39;all&#39;]}
2019-06-13 10:29:22 INFO scheduler - get_inventory: Reading the inventory file
2019-06-13 10:29:22 INFO scheduler - webhook: Scheduling 2 devices on a single runner
2019-06-13 10:29:22 INFO scheduler - create_job: Creating job job-eiw829
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can also view the logs of the scheduled Nornir runner:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl logs jobs/job-eiw829    
2019-06-13 10:29:27 INFO enforcer - push_configs: Found models: [&#39;generic-cm&#39;]
2019-06-13 10:29:27 INFO enforcer - push_configs: Downloading Template configmaps
2019-06-13 10:29:27 INFO enforcer - get_configmaps: Retrieving the list of ConfigMaps matching labels {&#39;app&#39;: &#39;naas&#39;, &#39;type&#39;: &#39;template&#39;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, when logged into one of the devices, we should see the new configuration changes applied, including the new alias:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;devicea#show run | include alias
alias FOO BAR
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another piece of configuration that has been added is a special event-handler that issues an API call to the scheduler every time its startup configuration is overwritten. This may potentially be used as an enforcement mechanism to prevent anyone from saving the changes done manually, but included here mainly to demonstrate the scheduler API:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;devicea#show run | i alias
alias FOO BAR
devicea#conf t 
devicea(config)#no alias FOO
devicea(config)#end
devicea#write
Copy completed successfully.
devicea#show run |  i alias
devicea#show run |  i alias
alias FOO BAR
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;coming-up&#34;&gt;Coming up&lt;/h2&gt;

&lt;p&gt;Now that we have the mechanism to push the network changes based on models and templates, we can start building the user-facing part of the NaaS platform. In the next post, I&amp;rsquo;ll demonstrate the architecture and implementation of a &lt;strong&gt;watcher&lt;/strong&gt; - a service that listens to custom resources and builds a device interface data model to be used by the scheduler.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terraform your physical network with YANG</title>
      <link>https://networkop.co.uk/post/2019-04-tf-yang/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2019-04-tf-yang/</guid>
      <description>

&lt;p&gt;Every time when I get bored from my day job I tend to find some small interesting project that I can do that can give me an instant sense of accomplishment and as the result lift my spirits and improve motivation. So this time I remembered when someone once asked me if they could use Terraform to control their physical network devices and I had to explain how this is the wrong tool for the job. Somehow the question got stuck in my head and now it came to fruition in the form of &lt;a href=&#34;https://github.com/networkop/terraform-yang&#34; target=&#34;_blank&#34;&gt;terraform-yang&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is a small Terraform plugin (provider) that allows users to manipulate interface-level settings of a network device. And I&amp;rsquo;m not talking about a VM in the cloud that runs network OS of your favourite vendor, this stuff is trivial and doesn&amp;rsquo;t require anything special from Terraform. I&amp;rsquo;m talking about Terraform controlling your individual physical network devices over an OpenConfig&amp;rsquo;s gNMI interface with standard Create/Read/Update/Delete operations exposed all the way to Terraform&amp;rsquo;s playbooks (or whatever they are called). Network Infrastructure as code nirvana&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/tf-gnmi.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;writing-a-custom-terraform-provider-for-a-network-device&#34;&gt;Writing a custom Terraform provider for a network device&lt;/h2&gt;

&lt;p&gt;Although this may look scary at the beginning, the process of creating your own TF provider is fairly easy. In fact a provider is nothing but a pointer to a remote API, which from the client point of view is just a URL (or a session to that URL) along with the necessary authentication credentials. TF provider simply combines all that information in a struct, which is later made available to various resource-specific API calls. For a network device with a gNMI interface, this is all the work that needs to be done to initialise the provider:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;cfg := &amp;amp;gnmi.Config{
	Addr:     d.Get(&amp;quot;address&amp;quot;).(string),
	TLS:      d.Get(&amp;quot;tls&amp;quot;).(bool),
	Username: d.Get(&amp;quot;username&amp;quot;).(string),
	Password: d.Get(&amp;quot;password&amp;quot;).(string),
}
client, err := gnmi.Dial(cfg)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only problem with this approach is that we have multiple devices and obviously it wouldn&amp;rsquo;t make sense to write a dedicated provider for each one. This is where Terraform &lt;a href=&#34;https://www.terraform.io/docs/configuration/providers.html#alias-multiple-provider-instances&#34; target=&#34;_blank&#34;&gt;aliases&lt;/a&gt; come to the rescue. With aliases we can define different providers that all use the same custom gNMI provider logic. This is how a &lt;code&gt;provider.tf&lt;/code&gt; file may look like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;provider &amp;quot;gnmi&amp;quot; {
  alias    = &amp;quot;SW1&amp;quot;
  address  = &amp;quot;192.0.2.0:6030&amp;quot;
  username = &amp;quot;admin&amp;quot;
  password = &amp;quot;admin&amp;quot;
}

provider &amp;quot;gnmi&amp;quot; {
  alias    = &amp;quot;SW2&amp;quot;
  address  = &amp;quot;192.0.2.1:6030&amp;quot;
  username = &amp;quot;admin&amp;quot;
  password = &amp;quot;admin&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;writing-a-resource-for-an-interface&#34;&gt;Writing a resource for an interface&lt;/h2&gt;

&lt;p&gt;Most of the work and logic goes into resources. Each resource represents an object hosted by a provider, that can be manipulated, i.e. created, updated and deleted. For public clouds, this could be a VM, a disk or a security group. For my little experiment, I&amp;rsquo;ve picked the simplest (and most common) configuration object that exists on a network device - an interface. I didn&amp;rsquo;t have time to boil the ocean so I decided to expose only a subset of interface-level settings:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;description&lt;/li&gt;
&lt;li&gt;switchport flag&lt;/li&gt;
&lt;li&gt;IPv4 Address&lt;/li&gt;
&lt;li&gt;Access VLAN&lt;/li&gt;
&lt;li&gt;Trunk VLANs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to build the structured configuration data, I&amp;rsquo;m using Go structs generated by &lt;a href=&#34;https://github.com/openconfig/ygot&#34; target=&#34;_blank&#34;&gt;ygot&lt;/a&gt; based on OpenConfig&amp;rsquo;s YANG models. A little hint for those of you who&amp;rsquo;ve read my Ansible &amp;amp; YANG &lt;a href=&#34;https://networkop.co.uk/tags/ansible-yang/&#34; target=&#34;_blank&#34;&gt;series&lt;/a&gt; and know what pyangbind or YDK are: ygot to gNMI is what pyangbind/YDK is to ncclient. So to configure a new interface, I first build an empty struct skeleton with ygot, populate it with values inside &lt;a href=&#34;https://github.com/networkop/terraform-yang/blob/master/resource_interface.go#L64&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;resourceInterfaceCreate()&lt;/code&gt;&lt;/a&gt; and then do &lt;code&gt;gnmi.Set()&lt;/code&gt; to send them off to the device. The logic for &lt;a href=&#34;https://github.com/networkop/terraform-yang/blob/master/resource_interface.go#L156&#34; target=&#34;_blank&#34;&gt;resource update&lt;/a&gt; is slightly more complicated since it should take into account mutually exclusive modes (e.g. switchport) and the behaviour when multiple conflicting arguments are defined. But ultimately you can decide how far you want to go and for a simple use case I&amp;rsquo;ve chosen, it only took me a few hours to &lt;a href=&#34;https://github.com/networkop/terraform-yang/blob/master/resource_interface.go#L64&#34; target=&#34;_blank&#34;&gt;codify&lt;/a&gt; the logic I wanted.&lt;/p&gt;

&lt;h2 id=&#34;using-a-gnmi-interface-resource&#34;&gt;Using a gNMI interface resource&lt;/h2&gt;

&lt;p&gt;With all of the provider/resource work done, making interface changes becomes really easy. Here&amp;rsquo;s an example of two different interfaces being configured on two different devices. The &lt;code&gt;provider&lt;/code&gt; argument points TF to one of the pre-defined aliases (i.e. network devices) and &lt;code&gt;name&lt;/code&gt; tells it which interface to configure. The rest of the arguments should be fairly self-explanatory.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;resource &amp;quot;gnmi_interface&amp;quot; &amp;quot;SW1_Eth1&amp;quot; {
    provider = &amp;quot;gnmi.SW1&amp;quot;
    name = &amp;quot;Ethernet1&amp;quot;
    description = &amp;quot;TF_INT_ETH1&amp;quot;
    switchport = false
    ipv4_address = &amp;quot;12.12.12.1/24&amp;quot;
}
resource &amp;quot;gnmi_interface&amp;quot; &amp;quot;SW2_Eth1&amp;quot; {
    provider = &amp;quot;gnmi.SW1&amp;quot;
    name = &amp;quot;Ethernet1&amp;quot;
    description = &amp;quot;TF_INT_ETH1&amp;quot;
    switchport = true
    trunk_vlans = [100, 200]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;surprises-and-gotchas&#34;&gt;Surprises and Gotchas&lt;/h2&gt;

&lt;p&gt;While writing this plugin I&amp;rsquo;ve stumbled across several interesting and what I thought were surprising issues with gNMI and OpenConfig models in general.&lt;/p&gt;

&lt;p&gt;Firstly, because the gNMI spec is in a constant state of flux, the official &lt;a href=&#34;https://github.com/openconfig&#34; target=&#34;_blank&#34;&gt;tools&lt;/a&gt; may not work with your device out of the box. There may be slightly different implementations of gNMI/gRPC clients, which obviously make it difficult to operate in a multivendor environment.&lt;/p&gt;

&lt;p&gt;Second, I was surprised to discover that a lot of structured data is still encoded as JSON. This JSON is serialised into a string and later encoded as protobuf as it gets sent to the device but still, my naive assumption was that protobuf was used for everything.&lt;/p&gt;

&lt;p&gt;Third, there are still a lot of vendor augments to standard openconfig models, which results in a vendor-specific ygot code. This feels almost like we&amp;rsquo;ve gone back to automating vendor-specific CLIs with all their quirks and corner cases.&lt;/p&gt;

&lt;p&gt;Fourth, there&amp;rsquo;s still a lot of YANG&amp;lt;-&amp;gt;CLI translation going on under the hood, especially for the configuration part (less for telemetry), so always expect the unexpected.&lt;/p&gt;

&lt;p&gt;Finally, I was initially bemused by the gNMI message format. I didn&amp;rsquo;t understand why I can have multiple updates in a single notification message and what&amp;rsquo;s the purpose of &lt;a href=&#34;https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#21-reusable-notification-message-format&#34; target=&#34;_blank&#34;&gt;duplicates&lt;/a&gt;. Until I realised that one of the primary use cases for gNMI was streaming telemetry and the protocol format was designed to work for both that and configuration updates. Some of these and other protocol-specific things still don&amp;rsquo;t make a lot of sense to me, and the &lt;a href=&#34;https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#21-reusable-notification-message-format&#34; target=&#34;_blank&#34;&gt;GNMI specification&lt;/a&gt; doesn&amp;rsquo;t do a very good job explaining why (not sure if it&amp;rsquo;s even supposed to).&lt;/p&gt;

&lt;p&gt;But as I&amp;rsquo;ve said multiple times before, just having the gNMI support that we have today, is way, way much better than not having it and having to rely on vendor-specific CLIs.&lt;/p&gt;

&lt;h2 id=&#34;outro&#34;&gt;Outro&lt;/h2&gt;

&lt;p&gt;I always liked writing plugins. They may look like some serious piece of software but in reality, they&amp;rsquo;re just a bunch of for loops and conditionals, so writing them is really easy. Not only do you get all of the boilerplate code that exposes all the bells and whistles you might need, but you also have tons of production-grade examples of how to write this kind of stuff &lt;a href=&#34;https://github.com/terraform-providers&#34; target=&#34;_blank&#34;&gt;available on Github&lt;/a&gt;. So don&amp;rsquo;t treat &lt;a href=&#34;https://github.com/networkop/terraform-yang&#34; target=&#34;_blank&#34;&gt;terraform-yang&lt;/a&gt; as a serious project, this was just a proof-of-concept and a learning exercise. I&amp;rsquo;m not convinced this is the right way to configure your network, although I feel the same way about most of the other popular automation tools out there.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Vendor Network Simulations at Scale with meshnet-cni and vrnetlab</title>
      <link>https://networkop.co.uk/post/2019-01-k8s-vrnetlab/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2019-01-k8s-vrnetlab/</guid>
      <description>

&lt;p&gt;In the &lt;a href=&#34;https://networkop.co.uk/post/2018-11-k8s-topo-p2/&#34;&gt;previous post&lt;/a&gt; I&amp;rsquo;ve demonstrated how to build virtual network topologies on top of Kubernetes with the help of &lt;a href=&#34;https://github.com/networkop/meshnet-cni&#34; target=&#34;_blank&#34;&gt;meshnet-cni&lt;/a&gt; plugin. As an example, I&amp;rsquo;ve shown topologies with 50 cEOS instances and 250 Quagga nodes. In both of these examples virtual network devices were running natively inside Docker containers, meaning they were running as (a set of) processes directly attached to the TCP/IP stack of the network namespace provided by the k8s pod. This works well for the native docker images, however, the overwhelming majority of virtual network devices are still being released as VMs. In addition to that, some of them require more than one VM and some special bootstrapping before they can they can be used for the first time. This means that in order to perform true multi-vendor network simulations, we need to find a way to run VMs inside containers, which, despite the seeming absurdity, is quite a common thing to do.&lt;/p&gt;

&lt;h2 id=&#34;option-1-kubevirt&#34;&gt;Option 1 - kubevirt&lt;/h2&gt;

&lt;p&gt;Kubevirt is a very popular project that provides the ability to run VMs inside k8s. It uses the power of &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions&#34; target=&#34;_blank&#34;&gt;Custom Resource Definitions&lt;/a&gt; to extend the native k8s API to allow the definition of VM parameters (libvirt domainxml) same as you would do for any other native k8s resource. It runs each VM inside the containerised KVM hypervisor, attaching them to libvirt-managed networking stack.&lt;/p&gt;

&lt;p&gt;However, since kubevirt is built for general-purpose VMs, making it work with virtual network devices requires a lot of work. Most of the bootstrapping tasks like startup configuration injection, disabling of ZTP and various OS-specific quirks like serial/video output selection for CSR or VCP reboot for VMX, would still need to be done after the pod is created. None of that is a major obstacle and hopefully virtual network OSs will also adopt standard server bootstrapping techniques like cloud-init, but until that happens we&amp;rsquo;d want to deal with those problems with as little effort as possible, which is where vrnetlab comes to the rescue.&lt;/p&gt;

&lt;h2 id=&#34;option-2-vrnetlab&#34;&gt;Option 2 - vrnetlab&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/plajjan/vrnetlab&#34; target=&#34;_blank&#34;&gt;vrnetlab&lt;/a&gt; is an open-source project that runs virtual network devices in Docker containers for &amp;ldquo;convenient labbing, development and testing&amp;rdquo;. At the time of writing, vrnetlab supported close to a dozen of virtual NOSs across most of the major vendors:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cisco - CSR, NXOS and XRV&lt;/li&gt;
&lt;li&gt;Juniper - VMX and vQFX&lt;/li&gt;
&lt;li&gt;Arista - vEOS&lt;/li&gt;
&lt;li&gt;Nokia - VSR/SROS&lt;/li&gt;
&lt;li&gt;Huawei - VRP&lt;/li&gt;
&lt;li&gt;HP - VSR1000&lt;/li&gt;
&lt;li&gt;Mikrotik - ROS&lt;/li&gt;
&lt;li&gt;OpenWRT&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The way I see it, vrnetlab accomplishes two things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Automates generation of Docker images from the original qcow2 or vmdk files&lt;/li&gt;
&lt;li&gt;Interconnect virtual routers based on the user-defined topology&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The above two things are loosely coupled and although vrnetlab docker images are built to expose VM&amp;rsquo;s network interfaces as TCP sockets (stitched together by the topology machine later), it&amp;rsquo;s still possible to use them for other purposes. My specific interest was to try and run vrnetlab images inside the kubernetes cluster with networking orchestrated by meshnet-cni.&lt;/p&gt;

&lt;h2 id=&#34;patching-vrnetlab&#34;&gt;Patching vrnetlab&lt;/h2&gt;

&lt;p&gt;Making it work turned out to be easier than I thought. All that I had to do was introduce a flag to control how the network interfaces are connected and add a special case for meshnet. This is a high-level logic of how the patch works:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;vrnetlab images now accept an additional optional argument called &lt;code&gt;--meshnet&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;this argument controls whether to connect VM to native docker interfaces or use the default TCP socket option&lt;/li&gt;
&lt;li&gt;for every ethernet interface inside a container a bridge is created, enslaving this interface&lt;/li&gt;
&lt;li&gt;VM is now attached to each one of those bridges instead of the TCP sockets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This patch is still in a &lt;a href=&#34;https://github.com/plajjan/vrnetlab/pull/188&#34; target=&#34;_blank&#34;&gt;pull request&lt;/a&gt; waiting to be tested so for the rest of this post I&amp;rsquo;ll be using &lt;a href=&#34;https://github.com/networkop/vrnetlab&#34; target=&#34;_blank&#34;&gt;my fork&lt;/a&gt;, which has all of these changes already merged.&lt;/p&gt;

&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ll assume that the Kubernetes cluster is already installed along with both &lt;a href=&#34;https://networkop.co.uk/post/2018-11-k8s-topo-p1/&#34;&gt;meshnet-cni&lt;/a&gt; and &lt;a href=&#34;https://networkop.co.uk/post/2018-11-k8s-topo-p2/&#34;&gt;k8s-topo&lt;/a&gt;. For demonstration purposes, I&amp;rsquo;ll use a random topology with a mix of Juniper vMX (v17.2R1.13) and Cisco CSR1000v (v16.04.01) devices, both built using vrnetlab.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/k8s-vrnetlab.png&#34; alt=&#34;vrnetlab topology&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;building-images&#34;&gt;Building images&lt;/h3&gt;

&lt;p&gt;The first thing to do is download the patched version of vrnetlab:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --depth 1 https://github.com/networkop/vrnetlab.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now copy both images into their respective directories and for each one of them run:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make docker-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The expected result is to have two local images that look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;core@node1 ~ $ docker images | grep vrnetlab
vrnetlab/vr-csr   16.04.01    b701e7811221   2 days ago   1.76GB
vrnetlab/vr-vmx   17.2R1.13   9a6af68dde78   2 days ago   4.7GB
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;uploading-images-to-a-private-registry&#34;&gt;Uploading images to a private registry&lt;/h3&gt;

&lt;p&gt;Now we need to make these images available to all nodes in the cluster and the easiest way to do that is to upload them into a private docker registry. So from a node with cluster credentials, create a local registry:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create -f https://raw.githubusercontent.com/networkop/k8s-topo/master/examples/docker-registry/docker-registry.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now use the service IP to create the registry URL variable:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export REGISTRY=$(kubectl get service docker-registry -o json | jq -r &#39;.spec.clusterIP&#39;):5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming both images are stored on the localhost do:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker tag vrnetlab/vr-csr:16.04.01 $REGISTRY/vr-csr:16.04.01
docker push $REGISTRY/vr-csr:16.04.01

docker tag vrnetlab/vr-vmx:17.2R1.13 $REGISTRY/vr-vmx:17.2R1.13
docker push $REGISTRY/vr-vmx:17.2R1.13
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once uploaded, we can query the following registry URL to confirm that:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X GET http://$REGISTRY/v2/_catalog
{&amp;quot;repositories&amp;quot;:[&amp;quot;vr-csr&amp;quot;,&amp;quot;vr-vmx&amp;quot;]}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;creating-the-network-topology&#34;&gt;Creating the network topology&lt;/h3&gt;

&lt;p&gt;First, connect to the k8s-topo pod:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl exec -it k8s-topo sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create the image URL environment variables for both CSR and vMX. These will later be used by the &lt;code&gt;k8s-topo&lt;/code&gt; script.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export REGISTRY=$(kubectl get service docker-registry -o json | jq -r &#39;.spec.clusterIP&#39;):5000
export CSR_IMAGE=$REGISTRY/vr-csr:16.04.01
export VMX_IMAGE=$REGISTRY/vr-vmx:17.2R1.13
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Generate a random spanning-tree topology with a mix of vmx and csr devices. The prefix argument accepts a list of one or more prefixes which determine the image to be used for the device.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./examples/builder/builder 20 0 --prefix vmx csr
Total number of links generated: 19
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now use the &lt;code&gt;k8s-topo&lt;/code&gt; script to create the topology and corresponding services:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/k8s-topo --create examples/builder/random.yml 
INFO:__main__:All data has been uploaded to etcd
INFO:__main__:All pods have been created successfully
INFO:__main__:
 alias csr-8=&#39;ssh -p 30010 vrnetlab@localhost&#39;
 alias vmx-15=&#39;ssh -p 30014 vrnetlab@localhost&#39;
 alias csr-4=&#39;ssh -p 30008 vrnetlab@localhost&#39;
 alias csr-14=&#39;ssh -p 30003 vrnetlab@localhost&#39;
 alias vmx-19=&#39;ssh -p 30016 vrnetlab@localhost&#39;
 alias vmx-11=&#39;ssh -p 30012 vrnetlab@localhost&#39;
 alias vmx-5=&#39;ssh -p 30018 vrnetlab@localhost&#39;
 alias csr-20=&#39;ssh -p 30007 vrnetlab@localhost&#39;
 alias csr-16=&#39;ssh -p 30004 vrnetlab@localhost&#39;
 alias csr-10=&#39;ssh -p 30001 vrnetlab@localhost&#39;
 alias vmx-1=&#39;ssh -p 30011 vrnetlab@localhost&#39;
 alias csr-2=&#39;ssh -p 30006 vrnetlab@localhost&#39;
 alias vmx-13=&#39;ssh -p 30013 vrnetlab@localhost&#39;
 alias vmx-17=&#39;ssh -p 30015 vrnetlab@localhost&#39;
 alias csr-12=&#39;ssh -p 30002 vrnetlab@localhost&#39;
 alias csr-18=&#39;ssh -p 30005 vrnetlab@localhost&#39;
 alias csr-6=&#39;ssh -p 30009 vrnetlab@localhost&#39;
 alias vmx-7=&#39;ssh -p 30019 vrnetlab@localhost&#39;
 alias vmx-9=&#39;ssh -p 30020 vrnetlab@localhost&#39;
 alias vmx-3=&#39;ssh -p 30017 vrnetlab@localhost&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If LLDP is required between nodes, it can be enabled with this command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/k8s-topo --lldp examples/builder/random.yml 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;verification&#34;&gt;Verification&lt;/h3&gt;

&lt;p&gt;Finally, it&amp;rsquo;s time to verify the connectivity between the nodes. Since all of the devices come up with minimal configuration, I&amp;rsquo;ll pick a random pair to demonstrate the LLDP and IP connectivity:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vmx-11
Warning: Permanently added &#39;[localhost]:30012&#39; (ECDSA) to the list of known hosts.
Password:
--- JUNOS 17.2R1.13 Kernel 64-bit  JNPR-10.3-20170523.350481_build
vrnetlab&amp;gt; configure 
Entering configuration mode
vrnetlab# set interfaces ge-0/0/3 unit 0 family inet address 12.12.12.1/24 
vrnetlab# set protocols lldp interface all 
vrnetlab# set protocols lldp port-id-subtype interface-name 
vrnetlab# commit and-quit 
commit complete
Exiting configuration mode
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now configure the other side of the link:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ csr-6
Warning: Permanently added &#39;[localhost]:30009&#39; (RSA) to the list of known hosts.
Password: 

csr1000v#conf t
Enter configuration commands, one per line.  End with CNTL/Z.
csr1000v(config)#lldp run
csr1000v(config)#int gigabitEthernet 2
csr1000v(config-if)#ip address 12.12.12.2 255.255.255.0
csr1000v(config-if)#no shut
csr1000v(config-if)#exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point, both devices should be able to ping and see each other as LLDP neighbors.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;csr1000v#sh lldp neighbors 
Capability codes:
    (R) Router, (B) Bridge, (T) Telephone, (C) DOCSIS Cable Device
    (W) WLAN Access Point, (P) Repeater, (S) Station, (O) Other

Device ID           Local Intf     Hold-time  Capability      Port ID
0005.86f0.f7c0      Gi2            120        B,R             ge-0/0/3

Total entries displayed: 1

csr1000v#ping 12.12.12.1
Type escape sequence to abort.
Sending 5, 100-byte ICMP Echos to 12.12.12.1, timeout is 2 seconds:
!!!!!
Success rate is 100 percent (5/5), round-trip min/avg/max = 2/8/18 ms
csr1000v#
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Large-scale network simulations in Kubernetes, Part 2 - Network topology orchestration</title>
      <link>https://networkop.co.uk/post/2018-11-k8s-topo-p2/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2018-11-k8s-topo-p2/</guid>
      <description>

&lt;p&gt;In the &lt;a href=&#34;https://networkop.co.uk/post/2018-11-k8s-topo-p1/&#34;&gt;previous post&lt;/a&gt; I&amp;rsquo;ve demonstrated a special-purpose CNI plugin for network simulations inside kubernetes called &lt;a href=&#34;https://github.com/networkop/meshnet-cni&#34; target=&#34;_blank&#34;&gt;meshnet&lt;/a&gt;. I&amp;rsquo;ve shown how relatively easy it is to build a simple 3-node topology spread across multiple kubernetes nodes. However, when it comes to real-life large-scale topology simulations, using meshnet &amp;ldquo;as is&amp;rdquo; becomes problematic due to the following reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Uploading topology information into etcd requires a lot of manual effort.&lt;/li&gt;
&lt;li&gt;Any customisation like startup configuration injection or exposure of internal ports is still a manual process.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;That is why I built &lt;a href=&#34;https://github.com/networkop/k8s-topo&#34; target=&#34;_blank&#34;&gt;k8s-topo&lt;/a&gt; - an orchestrator for network simulations inside kubernetes. It automates a lot of these manual steps and provides a simple and user-friendly interface to create networks of any size and configuration.&lt;/p&gt;

&lt;h1 id=&#34;k8s-topo-overview&#34;&gt;k8s-topo overview&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/networkop/k8s-topo&#34; target=&#34;_blank&#34;&gt;k8s-topo&lt;/a&gt; is a Python script that creates network topologies inside k8s based on a simple YAML file. It uses syntax similar to &lt;a href=&#34;https://github.com/networkop/arista-ceos-topo&#34; target=&#34;_blank&#34;&gt;docker-topo&lt;/a&gt; with a few modifications to account for the specifics of kubernetes environment. For instance, the following file is all what&amp;rsquo;s required to create and configure a simple 3-node topology:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;etcd_port: 32379
links:
  - endpoints: [&amp;quot;host-1:eth1:12.12.12.1/24&amp;quot;, &amp;quot;host-2:eth1:12.12.12.2/24&amp;quot;]
  - endpoints: [&amp;quot;host-1:eth2:13.13.13.1/24&amp;quot;, &amp;quot;host-3:eth1:13.13.13.3/24&amp;quot;]
  - endpoints: [&amp;quot;host-2:eth2:23.23.23.2/24&amp;quot;, &amp;quot;host-3:eth2:23.23.23.3/24&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Behind the scenes it uses &lt;a href=&#34;https://github.com/kubernetes-client/python&#34; target=&#34;_blank&#34;&gt;kubernetes&lt;/a&gt; and &lt;a href=&#34;https://github.com/kragniz/python-etcd3&#34; target=&#34;_blank&#34;&gt;etcd&lt;/a&gt; python libraries to accomplish the following things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Upload topology information into etcd&lt;/li&gt;
&lt;li&gt;Create a pod for each network device mentioned in the topology file&lt;/li&gt;
&lt;li&gt;If present, mount devices startup configuration as volumes inside pods&lt;/li&gt;
&lt;li&gt;Expose internal HTTPs port as a &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/#nodeport&#34; target=&#34;_blank&#34;&gt;NodePort&lt;/a&gt; service for every device&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/k8s-topo.png&#34; alt=&#34;k8s-topo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At the time of writing, k8s-topo supported three devices types, that get matched based on the device hostname prefix:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Host device - an Alpine image, matched by prefix &lt;code&gt;host&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;cEOS device - an Arista cEOS image, matched by prefix &lt;code&gt;sw&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Quagga device - an Alpine image with Quagga package installed, matched by prefix &lt;code&gt;qrtr&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As an optional extra, k8s-topo can generate a &lt;a href=&#34;https://www.tutorialspoint.com/d3js/d3js_graphs.htm&#34; target=&#34;_blank&#34;&gt;D3.js&lt;/a&gt; graph that visualises the deployed network topology on an interactive web graph as will be shown later.&lt;/p&gt;

&lt;h1 id=&#34;installation&#34;&gt;Installation&lt;/h1&gt;

&lt;p&gt;There are two main ways to install k8s-topo. The more traditional way will install k8s-topo as a python script on a local machine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip3 install git+https://github.com/networkop/k8s-topo.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another option is to install k8s-topo as a pod on top of a kubernetes cluster (it could be the same cluster that will be used for network simulations). For this option, we first need to build a k8s-topo docker image:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;build.sh &amp;lt;dockerhub_username&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then create a pod and its associated service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f kube-k8s-topo.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Technically, it doesn&amp;rsquo;t matter where the k8s-topo is installed as long as it can access the k8s cluster and meshnet&amp;rsquo;s etcd service. However, for the sake of simplicity, examples below will assume hosted k8s install, which means that we only need to specify the &lt;code&gt;etcd_port&lt;/code&gt; variable, leaving all others as default (e.g. &lt;code&gt;etcd_host = localhost&lt;/code&gt;).&lt;/p&gt;

&lt;h1 id=&#34;random-topology-examples&#34;&gt;Random topology examples&lt;/h1&gt;

&lt;p&gt;To demonstrate capabilities of our orchestrator, I&amp;rsquo;ve written a random topology &lt;a href=&#34;https://github.com/networkop/k8s-topo/blob/master/examples/builder/builder&#34; target=&#34;_blank&#34;&gt;builder script&lt;/a&gt; that generates a &lt;a href=&#34;https://en.wikipedia.org/wiki/Loop-erased_random_walk&#34; target=&#34;_blank&#34;&gt;uniform spanning tree&lt;/a&gt; graph, which is then used to create a topology definition YAML file along with a set of configuration files for each device. These configuration files accomplish two things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Configure a unique Loopback IP address in the &lt;code&gt;198.51.100.0/24&lt;/code&gt; range&lt;/li&gt;
&lt;li&gt;Enable OSPF on all directly connected interfaces&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The goal of this script is to be able to generate random large-scale network topologies that would be easy to test by simply ping-sweeping the range of all configured loopback addresses.&lt;/p&gt;

&lt;p&gt;All following demos assume that meshnet CNI  plugin has already been installed, as described in the &lt;a href=&#34;https://networkop.co.uk/post/2018-11-k8s-topo-p1/&#34;&gt;previous post&lt;/a&gt;. Let&amp;rsquo;s start with a relatively small example of 50 cEOS containers.&lt;/p&gt;

&lt;h2 id=&#34;building-a-50-node-ceos-topology&#34;&gt;Building a 50-node cEOS topology&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Before we can start building cEOS topologies, we need to make the cEOS Docker image available in a private docker registry. Refer to the k8s-topo Github repository for a complete list of &lt;a href=&#34;https://github.com/networkop/k8s-topo#private-docker-registry-setup&#34; target=&#34;_blank&#34;&gt;instructions&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, we&amp;rsquo;ll generate a random 50-node topology. From inside the k8s-topo pod run:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./examples/builder/builder --prefix sw 50 0
Total number of links generated: 49
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prefix &lt;code&gt;sw&lt;/code&gt; ensures that configured devices will be based on Arista cEOS image.&lt;/p&gt;

&lt;p&gt;Next, we can use k8s-topo to create our random topology inside k8s:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/k8s-topo --create examples/builder/random.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some of the versions of cEOS don&amp;rsquo;t have the &lt;code&gt;net.ipv4.ip_forward&lt;/code&gt; bit set which means no transit traffic will be allowed. In order to fix that, we can run the following command which modifies this setting on all running cEOS devices:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/k8s-topo --eif examples/builder/random.yml
INFO:__main__:All pods are running, trying to enable ip forwarding for cEOS pods
INFO:__main__:All cEOS pods have IP forwarding enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To be able see the generated topology we can run the following command, which creates a D3 graph and prints the URL we can use to access it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/k8s-topo # ./bin/k8s-topo --graph examples/builder/random.yml
INFO:__main__:D3 graph created
INFO:__main__:URL: http://10.83.30.252:32080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The built-in nginx server renders a simple interactive web page with each device coloured according to the k8s node they are running on (in my case there are 4 nodes in total):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/k8s-topo-20.png&#34; alt=&#34;20-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At this point we can run the ping-sweep test from any device to verify that we have complete end-to-end reachability:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# kubectl exec -it sw-1 bash
bash-4.3# for i in `seq 1 50`; do ping -c 1 -W 1 198.51.100.$i|grep from; done
64 bytes from 198.51.100.1: icmp_seq=1 ttl=64 time=0.061 ms
64 bytes from 198.51.100.2: icmp_seq=1 ttl=54 time=187 ms
64 bytes from 198.51.100.3: icmp_seq=1 ttl=56 time=139 ms
64 bytes from 198.51.100.4: icmp_seq=1 ttl=49 time=238 ms
64 bytes from 198.51.100.5: icmp_seq=1 ttl=53 time=189 ms
64 bytes from 198.51.100.6: icmp_seq=1 ttl=50 time=238 ms
64 bytes from 198.51.100.7: icmp_seq=1 ttl=61 time=71.6 ms
64 bytes from 198.51.100.8: icmp_seq=1 ttl=62 time=42.3 ms
64 bytes from 198.51.100.9: icmp_seq=1 ttl=59 time=91.0 ms
64 bytes from 198.51.100.10: icmp_seq=1 ttl=61 time=43.8 ms
64 bytes from 198.51.100.11: icmp_seq=1 ttl=60 time=60.8 ms
64 bytes from 198.51.100.12: icmp_seq=1 ttl=60 time=70.7 ms
64 bytes from 198.51.100.13: icmp_seq=1 ttl=57 time=134 ms
64 bytes from 198.51.100.14: icmp_seq=1 ttl=48 time=251 ms
64 bytes from 198.51.100.15: icmp_seq=1 ttl=63 time=27.8 ms
64 bytes from 198.51.100.16: icmp_seq=1 ttl=62 time=35.6 ms
64 bytes from 198.51.100.17: icmp_seq=1 ttl=54 time=182 ms
64 bytes from 198.51.100.18: icmp_seq=1 ttl=60 time=68.4 ms
64 bytes from 198.51.100.19: icmp_seq=1 ttl=59 time=97.9 ms
64 bytes from 198.51.100.20: icmp_seq=1 ttl=64 time=9.81 ms
64 bytes from 198.51.100.21: icmp_seq=1 ttl=58 time=114 ms
64 bytes from 198.51.100.22: icmp_seq=1 ttl=52 time=192 ms
64 bytes from 198.51.100.23: icmp_seq=1 ttl=59 time=102 ms
64 bytes from 198.51.100.24: icmp_seq=1 ttl=59 time=87.5 ms
64 bytes from 198.51.100.25: icmp_seq=1 ttl=61 time=66.7 ms
64 bytes from 198.51.100.26: icmp_seq=1 ttl=55 time=148 ms
64 bytes from 198.51.100.27: icmp_seq=1 ttl=61 time=60.6 ms
64 bytes from 198.51.100.28: icmp_seq=1 ttl=62 time=47.2 ms
64 bytes from 198.51.100.29: icmp_seq=1 ttl=63 time=18.8 ms
64 bytes from 198.51.100.30: icmp_seq=1 ttl=52 time=202 ms
64 bytes from 198.51.100.31: icmp_seq=1 ttl=61 time=49.2 ms
64 bytes from 198.51.100.32: icmp_seq=1 ttl=62 time=42.9 ms
64 bytes from 198.51.100.33: icmp_seq=1 ttl=49 time=252 ms
64 bytes from 198.51.100.34: icmp_seq=1 ttl=60 time=77.8 ms
64 bytes from 198.51.100.35: icmp_seq=1 ttl=49 time=217 ms
64 bytes from 198.51.100.36: icmp_seq=1 ttl=49 time=232 ms
64 bytes from 198.51.100.37: icmp_seq=1 ttl=50 time=218 ms
64 bytes from 198.51.100.38: icmp_seq=1 ttl=63 time=18.6 ms
64 bytes from 198.51.100.39: icmp_seq=1 ttl=63 time=24.6 ms
64 bytes from 198.51.100.40: icmp_seq=1 ttl=49 time=223 ms
64 bytes from 198.51.100.41: icmp_seq=1 ttl=61 time=48.4 ms
64 bytes from 198.51.100.42: icmp_seq=1 ttl=48 time=233 ms
64 bytes from 198.51.100.43: icmp_seq=1 ttl=64 time=11.0 ms
64 bytes from 198.51.100.44: icmp_seq=1 ttl=51 time=210 ms
64 bytes from 198.51.100.45: icmp_seq=1 ttl=62 time=51.6 ms
64 bytes from 198.51.100.46: icmp_seq=1 ttl=57 time=125 ms
64 bytes from 198.51.100.47: icmp_seq=1 ttl=51 time=222 ms
64 bytes from 198.51.100.48: icmp_seq=1 ttl=53 time=181 ms
64 bytes from 198.51.100.49: icmp_seq=1 ttl=63 time=33.8 ms
64 bytes from 198.51.100.50: icmp_seq=1 ttl=60 time=71.1 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This test proves that &lt;code&gt;sw-1&lt;/code&gt; can reach the loopback IP of every other device in the topology and, since the topology does not have any redundant links, also proves that k8s-topo, together with meshnet, have interconnected all devices correctly. If we had incorrectly connected at least one of the links, the OSPF adjacency would not have formed (due to incorrect source IP in the OSPF hello on NBMA network) and some of the pings would have failed.&lt;/p&gt;

&lt;p&gt;To destroy the network topology and clean-up any state stored in etcd, we can run the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./bin/k8s-topo --destroy examples/builder/random.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;building-a-250-node-quagga-topology&#34;&gt;Building a 250-node Quagga topology&lt;/h2&gt;

&lt;p&gt;Now let&amp;rsquo;s take this up a notch and test a 250-node topology built out of Quagga containers. Once again, we&amp;rsquo;ll use the builder script to generate a random spanning-tree graph and create all the required configuration files:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./examples/builder/builder 250 0
Total number of links generated: 249
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can spin-up our 250-node topology:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/k8s-topo --create examples/builder/random.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The generated graph is not as neat anymore but can be very handy when troubleshooting connectivity issues between different parts of topology&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/k8s-topo-200.png&#34; alt=&#34;200-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, we can do a loopback ping-sweep test from any random node in our topology to prove that everything has been interconnected correctly:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# kubectl exec -it qrtr-19 sh
/ # for i in `seq 1 250`; do ping -c 1 -W 1 198.51.100.$i|grep from; done
64 bytes from 198.51.100.1: seq=0 ttl=39 time=2.867 ms
64 bytes from 198.51.100.2: seq=0 ttl=42 time=1.979 ms
64 bytes from 198.51.100.3: seq=0 ttl=23 time=3.339 ms
64 bytes from 198.51.100.4: seq=0 ttl=37 time=2.348 ms
64 bytes from 198.51.100.5: seq=0 ttl=52 time=1.277 ms
64 bytes from 198.51.100.6: seq=0 ttl=33 time=2.662 ms
64 bytes from 198.51.100.7: seq=0 ttl=49 time=1.054 ms
64 bytes from 198.51.100.8: seq=0 ttl=40 time=2.320 ms
64 bytes from 198.51.100.9: seq=0 ttl=48 time=1.127 ms
64 bytes from 198.51.100.10: seq=0 ttl=61 time=0.425 ms
&amp;lt;...&amp;gt;
64 bytes from 198.51.100.240: seq=0 ttl=50 time=1.101 ms
64 bytes from 198.51.100.241: seq=0 ttl=62 time=0.254 ms
64 bytes from 198.51.100.242: seq=0 ttl=35 time=2.288 ms
64 bytes from 198.51.100.243: seq=0 ttl=51 time=0.939 ms
64 bytes from 198.51.100.244: seq=0 ttl=32 time=2.468 ms
64 bytes from 198.51.100.245: seq=0 ttl=64 time=0.523 ms
64 bytes from 198.51.100.246: seq=0 ttl=44 time=1.452 ms
64 bytes from 198.51.100.247: seq=0 ttl=41 time=1.705 ms
64 bytes from 198.51.100.248: seq=0 ttl=44 time=1.429 ms
64 bytes from 198.51.100.249: seq=0 ttl=42 time=1.722 ms
64 bytes from 198.51.100.250: seq=0 ttl=34 time=1.968 ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;For a very long time, when building real-life virtual network topologies, we had to compromise on the number of real network devices that can be simulated. This led to topology simplification and often resulted in parts of the real network topologies either missed or collapsed into a single virtual device. With k8s-topo and meshnet CNI plugin, we can now build one-to-one replicas of physical network topologies of any size and complexity, without sacrificing the level of detail.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Large-scale network simulations in Kubernetes, Part 1 - Building a CNI plugin</title>
      <link>https://networkop.co.uk/post/2018-11-k8s-topo-p1/</link>
      <pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2018-11-k8s-topo-p1/</guid>
      <description>

&lt;p&gt;Building virtualised network topologies has been one of the best ways to learn new technologies and to test new designs before implementing them on a production network. There are plenty of tools that can help build arbitrary network topologies, some with an interactive GUI (e.g. &lt;a href=&#34;https://www.gns3.com/&#34; target=&#34;_blank&#34;&gt;GNS3&lt;/a&gt; or &lt;a href=&#34;http://eve-ng.net/&#34; target=&#34;_blank&#34;&gt;EVE-NG/Unetlab&lt;/a&gt;) and some &amp;ldquo;headless&amp;rdquo;, with text-based configuration files (e.g. &lt;a href=&#34;https://github.com/plajjan/vrnetlab&#34; target=&#34;_blank&#34;&gt;vrnetlab&lt;/a&gt; or &lt;a href=&#34;https://github.com/CumulusNetworks/topology_converter&#34; target=&#34;_blank&#34;&gt;topology-converter&lt;/a&gt;). All of these tools work by spinning up multiple instances of virtual devices and interconnecting them according to a user-defined topology.&lt;/p&gt;

&lt;h1 id=&#34;problem-statement&#34;&gt;Problem statement&lt;/h1&gt;

&lt;p&gt;Most of these tools were primarily designed to work on a single host. This may work well for a relatively small topology but may become a problem as the number of virtual devices grows. Let&amp;rsquo;s take Juniper vMX as an example. From the official hardware requirements &lt;a href=&#34;https://www.juniper.net/documentation/en_US/vmx14.1/topics/reference/general/vmx-hw-sw-minimums.html&#34; target=&#34;_blank&#34;&gt;page&lt;/a&gt;, the smallest vMX instance will require:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2 VMs - one for control and one for data plane&lt;/li&gt;
&lt;li&gt;2 vCPUs - one for each of the VMs&lt;/li&gt;
&lt;li&gt;8 GB of RAM - 2GB for VCP and 6GB for VFP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This does not include the resources consumed by the underlying hypervisor, which can easily eat up another vCPU + 2GB of RAM. It&amp;rsquo;s easy to imagine how quickly we can hit the upper limit of devices in a single topology if we can only use a single hypervisor host. Admittedly, vMX is one of the most resource-hungry virtual routers and using other vendor&amp;rsquo;s virtual devices may increase that upper limit. However, if the requirement is to simulate topologies with 100+ devices, no single server will be able to cope with the required load and a potential resource contention may lead to instabilities and various software bugs manifesting themselves in places we don&amp;rsquo;t expect.&lt;/p&gt;

&lt;h1 id=&#34;exploring-possible-solutions&#34;&gt;Exploring possible solutions&lt;/h1&gt;

&lt;p&gt;Ideally, in large-scale simulations, we&amp;rsquo;d want to spread the devices across multiple hosts and interconnect them so that, from the device perspective, it&amp;rsquo;d look like they are still running on the same host. To take it a step further, we&amp;rsquo;d want the virtual links to be simple point-to-point L2 segments, without any bridges in between, so that we don&amp;rsquo;t have to deal with issues when virtual bridges consume or block some of the &amp;ldquo;unexpected&amp;rdquo; traffic, e.g. LACP/STP on &lt;a href=&#34;https://patchwork.ozlabs.org/patch/819153/&#34; target=&#34;_blank&#34;&gt;Linux bridges&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;containers-vs-vms&#34;&gt;Containers vs VMs&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s possible to build multi-host VM topologies on top of a private cloud like solution like OpenStack or VMware. The operational overhead involved would be minimal as all the scheduling and network plumbing should be taken care of by virtual infrastructure manager. However this approach has several disadvantages:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In order to not depend on the underlay, all inter-VM links would need to be implemented as overlay (VMware would require NSX)&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;VMs would still be interconnected via virtual switches&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Life-cycle management of virtual topologies is not trivial, e.g. VMware requires DRS, OpenStack requires masakari&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Injecting of additional data into VMs (e.g. configuration files) requires guest OS awareness and configuration (e.g. locating and mounting of a new partition)&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In contrast, containers provide an easy way to mount volumes inside a container&amp;rsquo;s filesystem, have plenty of options for resource scheduling and orchestrators and are substantially more lightweight and customizable. As a bonus, we get a unified way to package, distribute and manage lifecycle of our containers, independent from the underlying OS.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: AFAIK only Arista and Juniper build docker container images for their devices (cEOS and cSRX). However it is possible to run any VM-based network device inside a docker container, with many examples and makefiles available on &lt;a href=&#34;https://github.com/plajjan/vrnetlab&#34; target=&#34;_blank&#34;&gt;virtnetlab&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;kubernetes-vs-swarm&#34;&gt;Kubernetes vs Swarm&lt;/h2&gt;

&lt;p&gt;If we focus on Docker, the two most popular options for container orchestration would be Kubernetes and Swarm. Swarm is a Docker&amp;rsquo;s native container orchestration tool, it requires less customisation out of the box and has a simpler data model. The primary disadvantages of using Swarm for network simulations are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/moby/moby/issues/24862&#34; target=&#34;_blank&#34;&gt;Lack of support&lt;/a&gt; for privileged containers (network admin (CAP_NET_ADMIN) capabilities may be required by virtualised network devices)&lt;/li&gt;
&lt;li&gt;Unpredictable network interface &lt;a href=&#34;https://github.com/moby/moby/issues/25181&#34; target=&#34;_blank&#34;&gt;naming and order&lt;/a&gt; inside the container&lt;/li&gt;
&lt;li&gt;Docker&amp;rsquo;s main networking plugin libnetwork is &lt;a href=&#34;https://kubernetes.io/blog/2016/01/why-kubernetes-doesnt-use-libnetwork/&#34; target=&#34;_blank&#34;&gt;opinionated&lt;/a&gt; and difficult to extend or modify&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the other hand, the approach chosen by K8s provides an easier way to modify the default behaviour of a network plugin or to create a completely new implementation. However, K8s itself imposes several requirements on CNI plugins:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All containers can communicate with all other containers without NAT&lt;/li&gt;
&lt;li&gt;All nodes can communicate with all containers (and vice-versa) without NAT&lt;/li&gt;
&lt;li&gt;The IP that a container sees itself as is the same IP that others see it as&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The above also implies that communication between the containers happens at L3, which means that no container should make any assumptions about the underlying L2 transport, i.e. not use any L2 protocols(apart from ARP). Another corollary of the above requirements is that every container only has a single IP and hence a single interface, which, together with the previous L2 limitation, makes network simulations in K8s nearly impossible.&lt;/p&gt;

&lt;h2 id=&#34;multus-vs-diy&#34;&gt;Multus vs DIY&lt;/h2&gt;

&lt;p&gt;There are multiple solutions that solve the problem of a single network interface per container/pod - CNI-Genie, Knitter and &lt;a href=&#34;https://github.com/intel/multus-cni&#34; target=&#34;_blank&#34;&gt;Multus&lt;/a&gt; CNI. All of them were primarily designed for containerised VNF use cases, with the assumption that connectivity would still be provided by one of the existing plugins, which still leaves us with a number of issues:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We have to be transparent to the underlay, so we can&amp;rsquo;t use plugins that interact with the underlay (e.g. macvlan, calico)&lt;/li&gt;
&lt;li&gt;Most of the CNI plugins only provide L3 connectivity between pods (e.g. flannel, ovn, calico)&lt;/li&gt;
&lt;li&gt;The few plugins that do provide L2 overlays (e.g contiv, weave) do not support multiple interfaces and still use virtual bridges underneath&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Perhaps it would have been possible to hack one of the plugins to do what I wanted but I felt like it&amp;rsquo;d be easier to build a specialised CNI plugin to do just what I want and nothing more. As I&amp;rsquo;ve mentioned previously, developing a simple CNI plugin is not that difficult, especially if you have a clearly defined use case, which is why I&amp;rsquo;ve built &lt;a href=&#34;https://github.com/networkop/meshnet-cni&#34; target=&#34;_blank&#34;&gt;meshnet&lt;/a&gt; - a CNI plugin to build arbitrary network topologies out of point-to-point links.&lt;/p&gt;

&lt;h1 id=&#34;cni-plugin-overview&#34;&gt;CNI plugin overview&lt;/h1&gt;

&lt;p&gt;At a very high level, every CNI plugin is just a binary and a configuration file installed on K8s worker nodes. When a pod is scheduled to run on a particular node, a local node agent (kubelet) calls a CNI binary and passes all the necessary information to it. That CNI binary connects and configures network interfaces and returns the result back to kubelet. The information is passed to CNI binary in two ways - through environment variables and CNI configuration file. This is how a CNI &lt;strong&gt;ADD&lt;/strong&gt; call &lt;a href=&#34;https://www.cncf.io/wp-content/uploads/2017/11/Introduction-to-CNI-2.pdf#page=7&#34; target=&#34;_blank&#34;&gt;may&lt;/a&gt; look like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CNI_COMMAND=ADD \
CNI_CONTAINERID=$id \
CNI_NETNS=/proc/$pid/ns/net \
CNI_ARGS=K8S_POD_NAMESPACE=$namepsace;K8S_POD_NAME=$name
/opt/cni/bin/my-plugin &amp;lt; /etc/cni/net.d/my-config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The runtime parameters get passed to the plugin as environment variables and CNI configuration file gets passed to stdin. The CNI binary runs to completion and is expected to return the configured network settings back to the caller. The format of input and output, as well as environment variables, are documented in a CNI &lt;a href=&#34;https://github.com/containernetworking/cni/blob/master/SPEC.md&#34; target=&#34;_blank&#34;&gt;specification document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are plenty of other resources that cover CNI plugin development in much greater detail, I would recommend reading at least these four:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://schd.ws/hosted_files/kccnceu18/64/Kubernetes-and-the-CNI-Kubecon-218.pdf&#34; target=&#34;_blank&#34;&gt;CNI plugins best practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.altoros.com/blog/kubernetes-networking-writing-your-own-simple-cni-plug-in-with-bash/&#34; target=&#34;_blank&#34;&gt;Writing a sample CNI plugin in bash&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://logingood.github.io/kubernetes/cni/2016/05/14/netns-and-cni.html&#34; target=&#34;_blank&#34;&gt;EVPN CNI plugin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://dougbtv.com/nfvpe/2017/06/22/cni-tutorial/&#34; target=&#34;_blank&#34;&gt;Workflow for writing CNI plugins&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;meshnet-cni-architecture&#34;&gt;Meshnet CNI architecture&lt;/h1&gt;

&lt;p&gt;The goal of &lt;a href=&#34;https://github.com/networkop/meshnet-cni&#34; target=&#34;_blank&#34;&gt;meshnet&lt;/a&gt; plugin is to interconnect pods via direct point-to-point links according to some user-defined topology.  To do that, the plugin uses two types of links:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;veth&lt;/strong&gt; - to interconnect pods running on the same node&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;vxlan&lt;/strong&gt; - to interconnect pods running on different nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One thing to note is that point-to-point links are connected directly between pods, without any software bridges in between, which makes the design a lot simpler and provides a cleaner abstraction of a physical connection between network devices.&lt;/p&gt;

&lt;p&gt;The plugin consists of three main components:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;etcd&lt;/strong&gt; - a private cluster storing topology information and runtime pod metadata (e.g. pod IP address and NetNS fd)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;meshnet&lt;/strong&gt; - a CNI binary called by kubelet, responsible for pod&amp;rsquo;s network configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;meshnetd&lt;/strong&gt; - a daemon responsible for Vxlan link configuration updates&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Just like Multus, meshnet has the concept of master/default plugin, which sets up the first interface of the pod. This interface is setup by one of the existing plugins (e.g. bridge or flannel) and is used for pod&amp;rsquo;s external connectivity. The rest of the interfaces are setup according to a topology information stored in etcd.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/meshnet-arch.png&#34; alt=&#34;Meshnet Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Although the original idea of a CNI plugin was to have a single stateless binary, most of the time there&amp;rsquo;s a need to maintain some runtime state (e.g. ip routes, ip allocations etc.), which is why a lot of CNI plugins have daemons. In our case, daemon&amp;rsquo;s role is to ensure Vxlan link configurations are correct across different hosts. Using the above diagram as an example, if pod-2 comes up after pod-3, there must be a way of signalling the (node-1) VTEP IP to the remote node (node-2) and making sure that the Vxlan link on node-2 is moved into pod-3&amp;rsquo;s namespace. This is accomplished by meshnet binary issuing an HTTP PUT request to the remote node&amp;rsquo;s daemon with all the required Vxlan link attributes attached as a payload.&lt;/p&gt;

&lt;h1 id=&#34;meshnet-design-walkthrough&#34;&gt;Meshnet design walkthrough&lt;/h1&gt;

&lt;p&gt;One of the assumptions I made in the design is that topology information is uploaded into the etcd cluster before we spin up the first pod. I&amp;rsquo;ll focus on how exactly this can be done in the next post but for now, let&amp;rsquo;s assume that it&amp;rsquo;s is already there. This information needs to be structured in a very specific way and must cover every interface of every pod. The presence of this information in etcd tells meshnet binary what p2p interfaces (if any) need to be setup for the pod. Below is a sample definition of a link from pod2 to pod3:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{ &amp;quot;uid&amp;quot;:          21,
  &amp;quot;peer_pod&amp;quot;:     &amp;quot;pod3&amp;quot;,
  &amp;quot;local_intf&amp;quot;:   &amp;quot;eth2&amp;quot;,
  &amp;quot;local_ip&amp;quot;:     &amp;quot;23.23.23.2/24&amp;quot;,
  &amp;quot;peer_intf&amp;quot;:    &amp;quot;eth2&amp;quot;,
  &amp;quot;peer_ip&amp;quot;:      &amp;quot;23.23.23.3/24&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Meshnet binary is written in go and, like many other CNI plugins, contains a common skeleton code which parses input arguments and variables. Most of the plugin logic goes into &lt;code&gt;cmdAdd&lt;/code&gt; and &lt;code&gt;cmdDel&lt;/code&gt; functions that get called automatically when CNI binary is invoked by kubelet.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (
    &amp;quot;github.com/containernetworking/cni/pkg/skel&amp;quot;
    &amp;quot;github.com/containernetworking/cni/pkg/types&amp;quot;
)
func cmdAdd(args *skel.CmdArgs) error {
    // Parsing cni .conf file
    n, err := loadConf(args.StdinData)
    // Parsing CNI_ARGS environment variable
    cniArgs := k8sArgs{}
    types.LoadArgs(args.Args, &amp;amp;cniArgs)
}
func main() {
	skel.PluginMain(cmdAdd, cmdGet, cmdDel, version.All, &amp;quot;TODO&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One of the first things that happen in a &lt;code&gt;cmdAdd&lt;/code&gt; function is a &lt;code&gt;DelegateAdd&lt;/code&gt; call to let the master plugin setup the first interface of the pod. Master plugin configuration is extracted from the &lt;code&gt;delegate&lt;/code&gt; field of the meshnet CNI configuration file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func cmdAdd(args *skel.CmdArgs) error {
    ...
    r, err := delegateAdd(ctx, n.Delegate, args.IfName)
    ...
}
func delegateAdd(ctx context.Context, netconf map[string]interface{}, 
                  intfName string) 
                 (types.Result, error) {
	...
    result, err = invoke.DelegateAdd(ctx, netconf[&amp;quot;type&amp;quot;].(string), netconfBytes, nil)
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When master plugin is finished, we upload current pod&amp;rsquo;s runtime metadata to etcd. This is required so that peer pods can find and connect to our pod when needed. Specifically, they would need VTEP IP for remote vxlan links and namespace file descriptor for local veth links.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (pod *podMeta) setPodAlive(ctx context.Context, kv clientv3.KV, 
                                 netNS, srcIP string) error {

	srcIPKey := fmt.Sprintf(&amp;quot;/%s/src_ip&amp;quot;, pod.Name)
	_, err := kv.Put(ctx, srcIPKey, srcIP)

	NetNSKey := fmt.Sprintf(&amp;quot;/%s/net_ns&amp;quot;, pod.Name)
	_, err = kv.Put(ctx, NetNSKey, netNS)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this stage, we&amp;rsquo;re ready to setup pod&amp;rsquo;s links. Instead of manipulating netlink directly, I&amp;rsquo;m using &lt;a href=&#34;https://github.com/redhat-nfvpe/koko&#34; target=&#34;_blank&#34;&gt;koko&lt;/a&gt; - a high-level library that creates veth and vxlan links for containers. The simplified logic of what happens at this stage is summarised in the following code snippet:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt; // Iterate over each link of the local pod
for _, link := range *localPod.Links { 

    // Download peer pod&#39;s runtime metadata
    peerPod := &amp;amp;podMeta{Name: link.PeerPod}
    peerPod.getPodMetadata(ctx, kv)

    if peerPod.isAlive() { // If SrcIP and NetNS keys are set

        if peerPod.SrcIP == localPod.SrcIP { // If we&#39;re on the same host

            koko.MakeVeth(*myVeth, *peerVeth)

        } else  { // If we&#39;re on different hosts

            koko.MakeVxLan(*myVeth, *vxlan)
            putRequest(remoteUrl, bytes.NewBuffer(jsonPayload))

        }
    } else {
        // skip and continue
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We start by downloading metadata for each pod that we have a link to and check if it has already come up. The value of &lt;code&gt;peerPod.SrcIP&lt;/code&gt; determines whether we&amp;rsquo;re on the same node and need to setup a veth link or on different nodes and we need to setup a vxlan tunnel between them. The latter is done in two steps - first, a local Vxlan link is setup and moved to a pod&amp;rsquo;s namespace, followed by an HTTP PUT sent to the remote node&amp;rsquo;s meshnet daemon to setup a similar link on the other end.&lt;/p&gt;

&lt;h1 id=&#34;meshnet-cni-demo&#34;&gt;Meshnet CNI demo&lt;/h1&gt;

&lt;p&gt;The easiest way to walk through this demo is by running it inside a docker:dind container, with a few additional packages installed on top of it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker run --rm -it --privileged docker:dind sh
# /usr/local/bin/dockerd-entrypoint.sh &amp;amp;
# apk add --no-cache jq sudo wget git bash curl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/meshnet-demo.png&#34; alt=&#34;Meshnet Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this demo, we&amp;rsquo;ll build a simple triangle 3-node topology as shown in the figure above. We start by cloning the meshnet &lt;a href=&#34;https://github.com/networkop/meshnet-cni&#34; target=&#34;_blank&#34;&gt;Github repository&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/networkop/meshnet-cni.git &amp;amp;&amp;amp; cd meshnet-cni
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, create a local 3-node K8s cluster using &lt;a href=&#34;https://github.com/kubernetes-sigs/kubeadm-dind-cluster&#34; target=&#34;_blank&#34;&gt;kubeadm-dind-cluster&lt;/a&gt;, which uses docker-in-docker to simulate individual k8s nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://raw.githubusercontent.com/kubernetes-sigs/kubeadm-dind-cluster/master/fixed/dind-cluster-v1.11.sh 
chmod +x ./dind-cluster-v1.11.sh 
./dind-cluster-v1.11.sh up
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last command may take a few minutes to download all the required images. Once the K8s cluster is ready, we can start by deploying the private etcd cluster&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export PATH=&amp;quot;$HOME/.kubeadm-dind-cluster:$PATH&amp;quot;
kubectl create -f utils/etcd.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;./tests&lt;/code&gt; directory already contains link databases for our 3-node test topology, ready to be uploaded to etcd:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ETCD_HOST=$(kubectl get service etcd-client -o json |  jq -r &#39;.spec.clusterIP&#39;)
ENDPOINTS=$ETCD_HOST:2379

echo &amp;quot;Copying JSON files to kube-master&amp;quot;
sudo cp tests/*.json /var/lib/docker/volumes/kubeadm-dind-kube-master/_data/

echo &amp;quot;Copying etcdctl to kube-master&amp;quot;
sudo cp utils/etcdctl /var/lib/docker/volumes/kubeadm-dind-kube-master/_data/
docker exec kube-master cp /dind/etcdctl /usr/local/bin/

for pod in pod1 pod2 pod3
do
    # First cleanup any existing state
    docker exec -it kube-master sh -c &amp;quot;ETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS del --prefix=true \&amp;quot;/$pod\&amp;quot;&amp;quot;

    # Next Update the links database
    docker exec -it kube-master sh -c &amp;quot;cat /dind/$pod.json | ETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS put /$pod/links&amp;quot;

    # Print the contents of links databse
    docker exec -it kube-master sh -c &amp;quot;ETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS get --prefix=true \&amp;quot;/$pod\&amp;quot;&amp;quot;

done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final missing piece is the meshnet daemonset, which installs the binary, configuration file and the meshnet daemon on every node.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl create -f kube-meshnet.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only thing that&amp;rsquo;s required now is the master plugin configuration update. Since different K8s clusters can use a different plugins, the configuration file installed by the daemonset contains a dummy value which needs to be overwritten. In our case, the kubeadm-dind-cluster we&amp;rsquo;ve installed should use a default &lt;code&gt;bridge&lt;/code&gt; plugin which can be merged into our meshnet configuration file like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ETCD_HOST=$(kubectl get service etcd-client -o json |  jq -r &#39;.spec.clusterIP&#39;)
for container in kube-master kube-node-1 kube-node-2
do
    # Merge the default CNI plugin with meshnet
    docker exec $container bash -c &amp;quot;jq  -s &#39;.[1].delegate = (.[0]|del(.cniVersion))&#39; /etc/cni/net.d/cni.conf /etc/cni/net.d/meshnet.conf  | jq .[1] &amp;gt; /etc/cni/net.d/00-meshnet.conf&amp;quot;
    docker exec $container bash -c &amp;quot;sed -i &#39;s/ETCD_HOST/$ETCD_HOST/&#39; /etc/cni/net.d/00-meshnet.conf&amp;quot;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now meshnet CNI plugin is installed and configured and everything&amp;rsquo;s ready for us to create our test topology.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat tests/2node.yml | kubectl create -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command will verify that the topology has been created and confirm that pods are scheduled to the correct nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl --namespace=default get pods -o wide  |  grep pod
pod1    1/1 Running 0   1m  10.244.2.7  kube-node-1
pod2    1/1 Running 0   1m  10.244.2.6  kube-node-1
pod3    1/1 Running 0   1m  10.244.3.5  kube-node-2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we can do a simple ping test to verify that we have connectivity between all 3 pods:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl exec pod1 -- sudo ping -c 1 12.12.12.2
kubectl exec pod2 -- sudo ping -c 1 23.23.23.3
kubectl exec pod3 -- sudo ping -c 1 13.13.13.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;coming-up&#34;&gt;Coming up&lt;/h1&gt;

&lt;p&gt;The process demonstrated above is quite rigid and requires a lot of manual effort to create a required topology inside a K8s cluster. In the next post, we&amp;rsquo;ll have a look at &lt;a href=&#34;https://github.com/networkop/k8s-topo&#34; target=&#34;_blank&#34;&gt;k8s-topo&lt;/a&gt; - a simple tool that orchestrates most of the above steps - generates topology data and creates pods based on a simple YAML-based topology definition file.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Serverless SDN - Network Engineering Analysis of Appswitch</title>
      <link>https://networkop.co.uk/post/2018-05-29-appswitch-sdn/</link>
      <pubDate>Thu, 21 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2018-05-29-appswitch-sdn/</guid>
      <description>

&lt;p&gt;Virtual networking has been one of the hottest areas of research and development in recent years. Kubernetes alone has, at the time of writing, 20 different &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/networking/&#34; target=&#34;_blank&#34;&gt;networking plugins&lt;/a&gt;, some of which can be &lt;a href=&#34;https://github.com/projectcalico/canal&#34; target=&#34;_blank&#34;&gt;combined&lt;/a&gt; to build even more plugins. However, if we dig a bit deeper, most of these plugins and solutions are built out of two very simple constructs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a virtual switch - anything from a linux bridge through VPP and IOVisor to OVS&lt;/li&gt;
&lt;li&gt;ACL/NAT - most commonly implemented as iptables, with anything from netfilter to eBPF under the hood&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Note1: for the purpose of this article I won&amp;rsquo;t consider service meshes as a network solution, although it clearly is one, simply because it operates higher than TCP/IP and ultimately still requires network plumbing to be in place&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If those look familiar, you&amp;rsquo;re not mistaken, they are the &lt;strong&gt;same exact&lt;/strong&gt; things that were used to connect VMs together and enforce network security policies at the dawn of SDN era almost a decade ago. Although some of these technologies have gone a long way in both features and performance, they still treat containers the same way they treated VMs. There are a few exceptions that don&amp;rsquo;t involve the above constructs, like SR-IOV, macvlan/ipvlan and running containers in host namespace, however they represent a small fraction of corner case solutions and can be safely ignored for the purpose of this discussion. That&amp;rsquo;s why for networking folk it won&amp;rsquo;t be too big a mistake to think of containers as VMs, let&amp;rsquo;s see why:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/docker-vs-vm.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At a high level both container and VM networking are exactly the same, doesn&amp;rsquo;t matter what plugin, what networking model (CNM/CNI), what vSwitch flavour or what offload technology you use. Any virtual workload must have a virtual patch cable connecting it to a vSwitch, which implements forwarding and security policies programmed by an SDN controller. These tenets have gone unchallenged since the early days of containers and this is how I, personally, always imagined a typical virtual networking solution would look like. Until I read about &lt;a href=&#34;https://apporbit.com/a-test-drive-of-appswitch-the-network-stack-from-the-future/&#34; target=&#34;_blank&#34;&gt;AppSwitch&lt;/a&gt; and got so excited I decided to sign up for a &lt;a href=&#34;https://apporbit.com/appswitch/&#34; target=&#34;_blank&#34;&gt;beta program&lt;/a&gt; just to take it apart and see how it works. But before I dive deep into its architecture, I need to provide some theoretical background and I&amp;rsquo;ll do that by zooming in on a part of the (Linux) networking stack that sits right above TCP/IP.&lt;/p&gt;

&lt;h1 id=&#34;network-socket-api&#34;&gt;Network Socket API&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s start our exploration by examining what happens when a TCP client wants to communicate with a TCP server on a remote host. The first thing that the client library does is it creates a socket by making a &lt;code&gt;socket()&lt;/code&gt; system call for a specific address family (local, IPv4, IPv6) and transport protocol (TCP, UDP). The returned value is a file descriptor that points to a socket. This file descriptor is used in all subsequent network calls.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;sockfd = socket(AF_INET, SOCK_STREAM, 0) /* Create an IPv4 TCP socket */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next the client issues a &lt;code&gt;connect()&lt;/code&gt; system call, where it passes the new socket file descriptor along with a pointer to &lt;code&gt;serv_addr&lt;/code&gt; data structures that contains the destination IP and port of the TCP server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;connect(sockfd, &amp;amp;serv_addr, sizeof(serv_addr)) /* Returns 0 if connected */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Behind the scenes the last call initiates a TCP 3-way handshake with the remote server and returns 0 if TCP sessions transitions to the &lt;strong&gt;Established&lt;/strong&gt; state.&lt;/p&gt;

&lt;p&gt;Finally, when TCP session is established, the client interacts with the socket the same way it does with any normal file by calling &lt;code&gt;read()&lt;/code&gt; and &lt;code&gt;write()&lt;/code&gt; to receive and send data to the remote server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;read(sockfd, buffer, strlen(buffer))  /* Returns the number of bytes read */
write(sockfd, buffer, strlen(buffer)) /* Returns the number of bytes written */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below is a simplified diagram that shows the above syscalls and maps them to different stages of TCP connection. Note that only the layers relevant to the purpose of this discussion are shown. For a more comprehensive overview of Linux networking stack refer to &lt;a href=&#34;http://140.120.7.21/LinuxRef/Network/LinuxNetworkStack.html&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;http://www.microhowto.info/howto/listen_for_and_accept_tcp_connections_in_c.html&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt; and &lt;a href=&#34;https://www.ibm.com/developerworks/aix/library/au-tcpsystemcalls/index.html&#34; target=&#34;_blank&#34;&gt;3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/tcp-syscalls.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;On the server side the sequence of system calls is a little different. After a server creates a socket with the &lt;code&gt;socket()&lt;/code&gt; call, it tries to &lt;code&gt;bind()&lt;/code&gt; to a particular IP and port number of the host:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;bind(sockfd, &amp;amp;serv_addr, sizeof(serv_addr)) /* Returns 0 if successful */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to start accepting incoming TCP SYN requests, the socket needs to be marked as &lt;strong&gt;passive&lt;/strong&gt; by making a &lt;code&gt;listen()&lt;/code&gt; call, which also specifies the maximum size of a queue for pending TCP connections:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;listen(sockfd, SOMAXCONN) /* Returns 0 if successful */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the TCP 3-way handshake with the client is complete and the connection transitions to the &lt;strong&gt;Established&lt;/strong&gt; state, it will be pulled off the queue by an &lt;code&gt;accept()&lt;/code&gt; call, running in an infinite loop, which returns a new &lt;strong&gt;connected&lt;/strong&gt; socket file descriptor back to the server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;newsockfd = accept(sockfd, &amp;amp;client_addr, &amp;amp;client_len) /* Create a new socket */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The server application now proceeds to read and write data similar to the client side.&lt;/p&gt;

&lt;h1 id=&#34;appswitch-high-level-architecture&#34;&gt;Appswitch high-level architecture&lt;/h1&gt;

&lt;p&gt;Appswitch is a distributed virtual networking solution that intercepts application&amp;rsquo;s network events at the system call interface level, before they get to the TCP/IP stack of the host. This allows it to make routing decisions and enforce security policies without the need for vSwitches or iptables. The way it abstracts host&amp;rsquo;s TCP/IP stack (underlay) from the Appswitch-managed application IP address space (overlay) is not too much dissimilar from the way traditional routing protocols operate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ax-overview.png&#34; alt=&#34;Appswitch architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Just like a typical distributed routing protocol, Appswitch builds a database of all known local endpoint addresses at each node, distributes it to other members of a cluster with itself as the next-hop, calculates how to forward connection setup calls between local and remote endpoints and uses those results to steer network traffic between them. Sounds very much like something BGP/OSPF/IS-IS would do, doesn&amp;rsquo;t it? Let&amp;rsquo;s have a closer look at the actual elements involved, using the diagram from the original Appswitch &lt;a href=&#34;http://hci.stanford.edu/cstr/reports/2017-01.pdf&#34; target=&#34;_blank&#34;&gt;research paper&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ax-highlevel.png&#34; alt=&#34;Appswitch architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The first thing Appswitch does when it starts is neighbor discovery. The element responsible for this is a &lt;strong&gt;Service Router&lt;/strong&gt; which uses &lt;a href=&#34;https://www.serf.io/docs/internals/gossip.html&#34; target=&#34;_blank&#34;&gt;Serf&lt;/a&gt; to efficiently discover and disseminate information in a cluster of Appswitch nodes. Serf is Hashicorp&amp;rsquo;s implementation of a &lt;a href=&#34;https://pdfs.semanticscholar.org/8712/3307869ac84fc16122043a4a313604bd948f.pdf&#34; target=&#34;_blank&#34;&gt;gossip protocol&lt;/a&gt; - a cluster membership and communication protocol. Serf should be very easy to understand for those familiar with flood-and-learn behaviour of OSPF and IS-IS on broadcast multiaccess links, with the biggest distinction being that designated routers are chosen randomly and independently by each node of the link.&lt;/p&gt;

&lt;p&gt;Whenever a new application is launched inside Appswitch, it gets moved to a dedicated network namespace, where a &lt;strong&gt;Trap Generator&lt;/strong&gt; listens for network system calls and forwards them to a &lt;strong&gt;Trap Handler&lt;/strong&gt; over a shared Unix domain socket. Together these two elements form the forwarding plane for network system calls. Every time an application issues a &lt;code&gt;socket()&lt;/code&gt; syscall, the trap generator forwards it to the trap handler, which in turn creates a socket in the host namespace and passes its reference all the way back to the calling application. Once the connection is established, all &lt;code&gt;read()&lt;/code&gt; and &lt;code&gt;write()&lt;/code&gt; syscalls will be made using the returned &lt;code&gt;sockfd&lt;/code&gt;, effectively achieving the same performance as if our application was running directly on host OS.&lt;/p&gt;

&lt;p&gt;The central element in Appswitch architecture is a &lt;strong&gt;Service Table&lt;/strong&gt; - a distributed, eventually consistent database which stores mappings between running applications and their next-hop transport addresses. This information is used by the Trap Handler in &lt;code&gt;connect()&lt;/code&gt; syscall to build the &lt;code&gt;serv_addr&lt;/code&gt; data structure with the real IP and port of the target application. In a steady state each node in an Appswitch cluster will have the same view of this database and all updates to this database will be gossiped to other members of a cluster by a Service Router.&lt;/p&gt;

&lt;h1 id=&#34;appswitch-detailed-overview&#34;&gt;Appswitch detailed overview&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s have a closer look at what happens at different stages of Appswitch lifecycle.&lt;/p&gt;

&lt;h2 id=&#34;1-installation-and-startup&#34;&gt;1. Installation and Startup&lt;/h2&gt;

&lt;p&gt;At the time of writing Appswitch is being distributed as a docker container hosted in a private docker repository on docker hub (access provided based on request). It contains a single binary executable called &lt;code&gt;ax&lt;/code&gt; which needs to be started in PID and network namespaces of the host in order to be able to talk to host&amp;rsquo;s syscall interface and track application threads. By default, Appswitch attaches to the TCP/IP stack of the host in two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It starts management REST API and Serf processes that bind to ports 6664 and 7946.&lt;/li&gt;
&lt;li&gt;It selects one of the host&amp;rsquo;s IP addresses and reserves a pool of ports (e.g. 40000-60000) that will be dynamically allocated to applications in response to &lt;code&gt;bind()&lt;/code&gt; and &lt;code&gt;connect()&lt;/code&gt; system calls.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;2-joining-a-cluster&#34;&gt;2. Joining a cluster&lt;/h2&gt;

&lt;p&gt;One of the first things a running Appswitch instance has to do is discover all members of a cluster. This is accomplished by providing an IP address of at least one node of an existing cluster as a startup parameter, which is then used to discover all other members. The information exchanged during neighbor discovery phase, which includes host IPs, names and roles, gets recorded in the Service Table and can be viewed with &lt;code&gt;ax get nodes&lt;/code&gt; command. The mechanism of neighbor discovery and monitoring inside a cluster is not Appswitch-specific so I won&amp;rsquo;t spend much time on it here, however I highly recommend reading at least the protocol overview on the official &lt;a href=&#34;https://www.serf.io/docs/internals/gossip.html&#34; target=&#34;_blank&#34;&gt;Serf website&lt;/a&gt;, as this is a very common approach in modern-day eventually consistent cluster architectures.&lt;/p&gt;

&lt;h2 id=&#34;3-cluster-scale-out&#34;&gt;3. Cluster scale-out&lt;/h2&gt;

&lt;p&gt;A cluster is a set of nodes running Appswitch daemons that can directly communicate with one another and all use Serf to monitor state of other cluster members. Like any other cluster protocol, Serf has its limitations and Appswitch has a very neat mechanism to allow services to scale beyond a single cluster boundary, e.g. between a LAN and a WAN, called Federation. In principle, it&amp;rsquo;s somewhat similar to hierarchical BGP RR design with route reflectors doing &lt;code&gt;next-hop self&lt;/code&gt;. A set of Appswitch nodes can be designated as Federation gateways, which allows them to propagate information about running applications between two different clusters. When doing so they change the IP/port information to point to themselves, which forces all inter-cluster traffic to go via one of these nodes.&lt;/p&gt;

&lt;h2 id=&#34;4-registering-a-server-application&#34;&gt;4. Registering a server application&lt;/h2&gt;

&lt;p&gt;Once Appswitch instance has joined a cluster, it&amp;rsquo;s ready to onboard its first application. Let&amp;rsquo;s start with a web server that needs to be accessible from other Appswitch-managed applications (East-West) and externally (North-South). When starting a server application, we need to provide a unique identifier that will be used by other endpoints to reach it. That identifier can be either an IP address (provided with &lt;code&gt;--ip&lt;/code&gt; argument) or a hostname (provided with &lt;code&gt;--name&lt;/code&gt; argument). If only hostname is specified, IP address will still be allocated behind the scenes and Appswitch will program its embedded DNS to make sure all other applications can reach our web server by its name.&lt;/p&gt;

&lt;p&gt;Moments after it&amp;rsquo;s been started, the web server issues a &lt;code&gt;bind()&lt;/code&gt; syscall, specifying the IP/port it wants to bind to, which gets intercepted by the trap generator and forwarded to the trap handler. The latter generates another &lt;code&gt;bind()&lt;/code&gt; syscall, however the new &lt;code&gt;bind()&lt;/code&gt; has two notable differences:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It contains the host IP and one of the reserved ports (40000 - 60000) specified at the startup&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s destined towards a host&amp;rsquo;s syscall interface&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The resulting mappings between the internal socket address of the web server, the overlay IP address assigned to it by the Appswitch and the &amp;ldquo;real&amp;rdquo; or underlay address of the host get recorded in the Service Table and gossiped to all other members of the cluster.&lt;/p&gt;

&lt;h2 id=&#34;3-processing-client-requests&#34;&gt;3. Processing client requests&lt;/h2&gt;

&lt;p&gt;Now let&amp;rsquo;s see what happens when another Appswitch application tries to connect to the previously started web server. The client may try to connect to the web server by its hostname, assuming the server was configured with one, in which case the DNS request will get handled by an embedded DNS server running on all Appswitch nodes, as a part of the same binary that runs service table and service router, and the client library will receive the overlay IP address of the web server.&lt;/p&gt;

&lt;p&gt;As soon as the &lt;code&gt;connect()&lt;/code&gt; syscall gets intercepted by the Trap Generator and forwarded to the Trap Handler, the latter consults the local Service Table and finds an entry for the web server, which was previously broadcasted to all members of the cluster. This is also a point at which Appswitch performs security isolation and load balancing:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It checks whether security zone of a client matches the one of the server and only proceeds if they are the same. This allows users to split application into different security groups and enforce isolation between them without the need for any dataplane ACLs&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;It also determines which exact server to include in the &lt;code&gt;connect()&lt;/code&gt; syscall, in case multiple servers have registered with the same IP or hostname. This provides a fully-distributed client-side load-balancing solution without the need for any middleware.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once all the checks have passed and a destination server has been determined, Trap Handler issues a &lt;code&gt;connect()&lt;/code&gt; syscall with the underlay IP and port of that server app. At the end of the 3-way TCP handshake the &lt;code&gt;connect()&lt;/code&gt; call returns 0 and the two applications continue to &lt;code&gt;read()&lt;/code&gt; and &lt;code&gt;write()&lt;/code&gt; using the sockfd provided by Appswitch, as if they were running directly in host network namespace.&lt;/p&gt;

&lt;h2 id=&#34;4-ingress-forwarding&#34;&gt;4. Ingress forwarding&lt;/h2&gt;

&lt;p&gt;When starting a server-side application, we have an option to make it available externally to clients outside of an Appswitch cluster, by mapping a server port to an arbitrary port of the host. This is accomplished with an &lt;code&gt;--expose INSIDE:OUTSIDE&lt;/code&gt; argument, which will map the &lt;strong&gt;INSIDE&lt;/strong&gt; application port to a specified &lt;strong&gt;OUTSIDE&lt;/strong&gt; port on the host. We can also simulate the behaviour of k8s &lt;strong&gt;NodePort&lt;/strong&gt; by changing this argument to &lt;code&gt;--expose INSIDE:0.0.0.0:OUTSIDE&lt;/code&gt;, which will expose the same port on ALL members of an Appswitch cluster.&lt;/p&gt;

&lt;h2 id=&#34;5-egress-forwarding&#34;&gt;5. Egress forwarding&lt;/h2&gt;

&lt;p&gt;All outbound client-side syscalls that don&amp;rsquo;t find a match in the local Service Table, will be forwarded to one of the Egress Gateway nodes. Any number of Appswitch instances can be designated as Egress Gateway nodes at startup, which makes them pick a random port from a reserved pool, broadcast their presence to the rest of the cluster and start listening for incoming connections from other members. When the client-side Trap Handler intercepts the &lt;code&gt;connect()&lt;/code&gt; syscall to an external service, it tweaks the address and sends it to one of the egress gateways instead. At the same time it communicates the real external service address to the egress gateway out-of-band. When the egress gateway receives the external service address, it will splice the two TCP sessions together connecting the Appswitch application to its intended external destination.&lt;/p&gt;

&lt;h1 id=&#34;demo&#34;&gt;Demo&lt;/h1&gt;

&lt;p&gt;Finally, the time has come for the proof of the pudding. To demonstrate how Appswitch works in real life I&amp;rsquo;ll use the famous Docker &lt;a href=&#34;https://github.com/dockersamples/example-voting-app&#34; target=&#34;_blank&#34;&gt;voting app&lt;/a&gt; - a simple application comprised of 5 (micro)services that all interact over standard APIs. To make it more realistic, I&amp;rsquo;ll split the voting app between two Docker hosts to demonstrate how different parts of the same application are able to communicate remotely, assisted by Appswitch. The following diagram shows how different application components are mapped to a pair docker hosts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ax-voting.png&#34; alt=&#34;Appswitch demo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Unlike the original voting app &lt;a href=&#34;https://github.com/dockersamples/example-voting-app/blob/master/architecture.png&#34; target=&#34;_blank&#34;&gt;diagram&lt;/a&gt; which shows the flow of data within the app, the arrows in the diagram above show the direction and destination of the initial TCP SYN, which will be important for the following explanation.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that in the demo we&amp;rsquo;ll deploy all necessary components manually, using &lt;code&gt;docker-compose&lt;/code&gt; and &lt;code&gt;docker run&lt;/code&gt; commands, which doesn&amp;rsquo;t mean the same can&amp;rsquo;t be done by an orchestrator. One of the goals of this demo is to be easy to reproduce and understand and I felt like using k8s would make it more confusing and distract the reader from the main point. For demonstration of how to use Appswitch inside k8s environment refer to the official &lt;a href=&#34;http://appswitch.readthedocs.io/en/latest/integrations.html&#34; target=&#34;_blank&#34;&gt;integration manual&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;1-installation&#34;&gt;1. Installation&lt;/h2&gt;

&lt;p&gt;In order to deploy all 5 components of this app, I&amp;rsquo;ll use two docker-compose files, which are based on the original &lt;a href=&#34;https://github.com/dockersamples/example-voting-app/blob/master/docker-compose.yml&#34; target=&#34;_blank&#34;&gt;docker-compose file&lt;/a&gt;, with a few modifications to make them work with Appswitch:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The command argument is now prepended with &lt;code&gt;/usr/bin/ax run&lt;/code&gt;, which is an Appswitch wrapper that containts the Trap Generator to tracks the network system calls&lt;/li&gt;
&lt;li&gt;Network mode is set to none - we won&amp;rsquo;t need &lt;strong&gt;any&lt;/strong&gt; network interface inside a Docker container whatsoever&lt;/li&gt;
&lt;li&gt;Volumes now include three additional items:

&lt;ul&gt;
&lt;li&gt;/var/run/appswitch - a communication channel between a trap generator running inside a container and a trap handler running in the host namespace&lt;/li&gt;
&lt;li&gt;/usr/bin/ax - an Appswitch binary made available inside a container&lt;/li&gt;
&lt;li&gt;/etc/resolv.conf - overrides container DNS settings to redirect queries to an embedded Appswitch DNS service.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These two docker-compose files are stored in my personal fork of the voting app and are the only thing that is different between my fork and the original repo. So the first step is to download the voting app:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/networkop/example-voting-app.git ax; cd ax
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To be able to run everything on a single laptop, I simulate docker hosts by running Docker daemon inside a Docker container, a pattern commonly known as docker-in-docker or &lt;a href=&#34;https://github.com/jpetazzo/dind&#34; target=&#34;_blank&#34;&gt;dind&lt;/a&gt;. Additionally I expose the Docker API ports of the two dind containers to be able to manage them from my host OS.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d --privileged --name worker-1 --hostname=worker-1 \
-p 5000:5000 -p 12375:2375 \
-v /usr/local/bin/docker-compose:/usr/local/bin/docker-compose \
-v $(pwd):$(pwd) \
docker:stable-dind

docker run -d --privileged --name worker-2 --hostname=worker-2 \
-p 5001:5001 -p 22375:2375 \
-v /usr/local/bin/docker-compose:/usr/local/bin/docker-compose \
-v $(pwd):$(pwd) \
docker:stable-dind
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These two commands will create a simple two-node topology as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://networkop.co.uk/img/ax-dind.png&#34; alt=&#34;Appswitch dind&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each docker host will have its own unique IP assigned to interface &lt;code&gt;eth0&lt;/code&gt; and will most likely have the same IP assigned to internal &lt;code&gt;docker0&lt;/code&gt; bridge. We can safely ignore the latter since docker bridge is not used by Appswitch, however we would need to know the IPs assigned to &lt;code&gt;eth0&lt;/code&gt; in order to be able to bootstrap the Serf cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export IP_1=$(docker inspect worker-1 -f &amp;quot;{{ .NetworkSettings.Networks.bridge.IPAddress }}&amp;quot;)
export IP_2=$(docker inspect worker-2 -f &amp;quot;{{ .NetworkSettings.Networks.bridge.IPAddress }}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, on each node we need to start an Appswitch daemon and pass the above IPs as input parameters.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker --host=localhost:12375 run -d --name=ax \
--pid=host `# To identify and track application threads from host namespace` \
--net=host `# To make host network available for the application` \
--privileged `# To set seccomp filter` \
-v /usr/bin:/hostbin `# To install ax to /hostbin/` \
-v /var/run/appswitch:/var/run/appswitch `# To share the UNIX socket between app and ax daemon` \
-e AX_NODE_INTERFACE=${IP_1} `# Make sure the right IP is selected`\
-e AX_NEIGHBORS=${IP_2} `# IP of peer container`\
docker.io/appswitch/ax `# Starting the main process`

docker --host=localhost:22375 run -d --name=ax \
--pid=host `# To identify and track application threads from host namespace` \
--net=host `# To make host network available for the application` \
--privileged `# To set seccomp filter` \
-v /usr/bin:/hostbin `# To install ax to /hostbin/` \
-v /var/run/appswitch:/var/run/appswitch `# To share the UNIX socket between app and ax daemon` \
-e AX_NODE_INTERFACE=${IP_2} `# Make sure the right IP is selected`\
-e AX_NEIGHBORS=${IP_1} `# IP of peer container`\
docker.io/appswitch/ax `# Starting the main process`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point our Appswitch cluster should be bootstrapped and fully functional, which we can verify by listing its members from either one of the worker nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker exec -it worker-1 ax get nodes
    NAME     CLUSTER       IP      EXTERNALIP    ROLE     APPCOUNT  
------------------------------------------------------------------
  worker-1  appswitch  172.17.0.2              [compute]  0         
  worker-2  appswitch  172.17.0.3              [compute]  0         

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the only thing that is left is to deploy our voting app using docker compose:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose --host localhost:12375 --file ./docker-compose-1.yml up -d
docker-compose --host localhost:22375 --file ./docker-compose-2.yml up -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A few minutes later the voting side of the app should become available on &lt;a href=&#34;http://localhost:5000/&#34; target=&#34;_blank&#34;&gt;localhost:5000&lt;/a&gt;, while the results can be viewed on &lt;a href=&#34;http://localhost:5001/&#34; target=&#34;_blank&#34;&gt;localhost:5001&lt;/a&gt;.You can put in more votes for cats or dogs by using &amp;ldquo;a&amp;rdquo; or &amp;ldquo;b&amp;rdquo; in the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sS -X POST --data &amp;quot;vote=a&amp;quot; http://localhost:5000 &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-verification&#34;&gt;2. Verification&lt;/h2&gt;

&lt;p&gt;Now let&amp;rsquo;s examine how Appswitch has managed to establish connectivity between all components of our voting app. The two internal server-side components are DB and Redis, and all client-side apps (vote, worker and result) are expecting to connect to them  by using their respective hostnames - &lt;code&gt;db&lt;/code&gt; and &lt;code&gt;redis&lt;/code&gt;. Appswitch enables that by running an embedded DNS server which gets programmed with hostnames based on the &lt;code&gt;--name&lt;/code&gt; argument of &lt;code&gt;ax run&lt;/code&gt; command, as shown in the following snippet from the Redis service in docker-compose-1.yml:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;command: /usr/bin/ax run --name redis redis-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Embedded DNS server returns the overlay IP assigned to Redis, which will be used later by client-side apps in their syscalls. In our case we didn&amp;rsquo;t specify this IP explicitly, so Appswitch picked a random IP address which can be viewed with &lt;code&gt;ax get apps&lt;/code&gt; command:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;APPID&lt;/th&gt;
&lt;th&gt;NODEID&lt;/th&gt;
&lt;th&gt;CLUSTER&lt;/th&gt;
&lt;th&gt;APPIP&lt;/th&gt;
&lt;th&gt;DRIVER&lt;/th&gt;
&lt;th&gt;LABELS&lt;/th&gt;
&lt;th&gt;ZONES&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;redis&lt;/td&gt;
&lt;td&gt;f00005bb&lt;/td&gt;
&lt;td&gt;worker-2&lt;/td&gt;
&lt;td&gt;appswitch&lt;/td&gt;
&lt;td&gt;10.244.17.175&lt;/td&gt;
&lt;td&gt;user&lt;/td&gt;
&lt;td&gt;zone=default&lt;/td&gt;
&lt;td&gt;[zone==default]&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;db&lt;/td&gt;
&lt;td&gt;f00004cd&lt;/td&gt;
&lt;td&gt;worker-1&lt;/td&gt;
&lt;td&gt;appswitch&lt;/td&gt;
&lt;td&gt;10.12.33.253&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;zone=default&lt;/td&gt;
&lt;td&gt;[zone==default]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As soon as DB and Redis are fully initialised and issue a &lt;code&gt;listen()&lt;/code&gt; syscall, Appswitch broadcasts the overlay-underlay address mapping to all other members of a cluster, so that each node ends up with an identical view of this table:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;CLUSTER&lt;/th&gt;
&lt;th&gt;APPID&lt;/th&gt;
&lt;th&gt;PROTO&lt;/th&gt;
&lt;th&gt;SERVICEADDR&lt;/th&gt;
&lt;th&gt;IPV4ADDR&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;worker-2&lt;/td&gt;
&lt;td&gt;appswitch&lt;/td&gt;
&lt;td&gt;f00005b9&lt;/td&gt;
&lt;td&gt;tcp&lt;/td&gt;
&lt;td&gt;redis:6379&lt;/td&gt;
&lt;td&gt;172.17.0.3:40000&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;worker-1&lt;/td&gt;
&lt;td&gt;appswitch&lt;/td&gt;
&lt;td&gt;f000056d&lt;/td&gt;
&lt;td&gt;tcp&lt;/td&gt;
&lt;td&gt;db:5432&lt;/td&gt;
&lt;td&gt;172.17.0.2:40001&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Now, when a client-side app decides to establish a TCP session with &lt;code&gt;redis&lt;/code&gt; on port 6379, it tries to &lt;code&gt;connect()&lt;/code&gt; to &lt;code&gt;10.244.17.175:6379&lt;/code&gt;, which makes the Trap Handler issue a new &lt;code&gt;connect()&lt;/code&gt; to &lt;code&gt;172.17.0.3:40000&lt;/code&gt;, the real/underlay address of redis on worker-2 node.&lt;/p&gt;

&lt;p&gt;At the same time we have two client-side apps that act as server-side apps for external connections - vote and result. Both of these apps were started with their internal http ports exposed, like in the following example from the vote service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;command: /usr/bin/ax run --expose 80:5000 python app.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When they attempt to &lt;code&gt;bind()&lt;/code&gt; to port 80, Appswitch will not try to use the dynamic port range and will try to bind to the port specified in the expose command instead:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;CLUSTER&lt;/th&gt;
&lt;th&gt;APPID&lt;/th&gt;
&lt;th&gt;PROTO&lt;/th&gt;
&lt;th&gt;SERVICEADDR&lt;/th&gt;
&lt;th&gt;IPV4ADDR&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;worker-1&lt;/td&gt;
&lt;td&gt;appswitch&lt;/td&gt;
&lt;td&gt;f00004cc&lt;/td&gt;
&lt;td&gt;tcp&lt;/td&gt;
&lt;td&gt;172.17.0.2:80&lt;/td&gt;
&lt;td&gt;172.17.0.2:5000&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;worker-2&lt;/td&gt;
&lt;td&gt;appswitch&lt;/td&gt;
&lt;td&gt;f0000627&lt;/td&gt;
&lt;td&gt;tcp&lt;/td&gt;
&lt;td&gt;172.17.0.3:80&lt;/td&gt;
&lt;td&gt;172.17.0.3:5001&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This port mapping will only make these ports available inside their respective docker hosts, which is why we&amp;rsquo;ve exposed ports 5000 and 5001 when starting dind containers earlier.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CONTAINER ID&lt;/th&gt;
&lt;th&gt;IMAGE&lt;/th&gt;
&lt;th&gt;COMMAND&lt;/th&gt;
&lt;th&gt;CREATED&lt;/th&gt;
&lt;th&gt;STATUS&lt;/th&gt;
&lt;th&gt;PORTS&lt;/th&gt;
&lt;th&gt;NAMES&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;303d5ae4d63d&lt;/td&gt;
&lt;td&gt;docker:stable-dind&lt;/td&gt;
&lt;td&gt;&amp;ldquo;dockerd-entrypoint.…&amp;rdquo;&lt;/td&gt;
&lt;td&gt;12 hours ago&lt;/td&gt;
&lt;td&gt;Up 7 hours&lt;/td&gt;
&lt;td&gt;0.0.0.0:5001-&amp;gt;5001/tcp, 0.0.0.0:22375-&amp;gt;2375/tcp&lt;/td&gt;
&lt;td&gt;worker-2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2421e7556bc1&lt;/td&gt;
&lt;td&gt;docker:stable-dind&lt;/td&gt;
&lt;td&gt;&amp;ldquo;dockerd-entrypoint.…&amp;rdquo;&lt;/td&gt;
&lt;td&gt;12 hours ago&lt;/td&gt;
&lt;td&gt;Up 7 hours&lt;/td&gt;
&lt;td&gt;0.0.0.0:5000-&amp;gt;5000/tcp, 0.0.0.0:12375-&amp;gt;2375/tcp&lt;/td&gt;
&lt;td&gt;worker-1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So a TCP SYN towards localhost:5000 will get PAT&amp;rsquo;ed by Docker-managed iptables of the Docker host, will hit the TCP/IP stack of the worker-1 node, which, once the handshake is complete, will issue an accept() syscall towards a Trap Handler and ultimately towards port 80 of the vote app.&lt;/p&gt;

&lt;h1 id=&#34;outro&#34;&gt;Outro&lt;/h1&gt;

&lt;p&gt;The reason why I called this article &amp;ldquo;Serverless SDN&amp;rdquo; is because I find this to be a rather fitting description of what Appswitch is. Just like serverless computing, which abstracts away the underlying OS and server management, Appswitch abstracts away the networking stack of the host and provides networking abstractions at a well-defined socket layer. Reading through the &lt;a href=&#34;https://appswitch.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34;&gt;official documentation&lt;/a&gt; and Appswitch &lt;a href=&#34;http://hci.stanford.edu/cstr/reports/2017-01.pdf&#34; target=&#34;_blank&#34;&gt;research paper&lt;/a&gt;, I can&amp;rsquo;t get rid of the thought that this is what container networking should have looked like in the first place - not linux bridges and veth pairs and not even macvlan and ipvlan devices. The original goal of containers was to encapsulate a single process and in majority of cases that single process does not need to have a full TCP/IP stack with its own interface, IP address, MAC address - all what it cares about is sending and receiving data - and this is exactly what Appswitch provides.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The problem of unpredictable interface order in multi-network Docker containers</title>
      <link>https://networkop.co.uk/post/2018-03-03-docker-multinet/</link>
      <pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2018-03-03-docker-multinet/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: It appears that the issue described in this post has been fixed or at least it doesn&amp;rsquo;t manifest itself the same way it did back in 2018. Therefore, please treat this article purely as a historical record.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Whether we like it or not, the era of DevOps is upon us, fellow network engineers, and with it come opportunities to approach and solve common networking problems
in new, innovative ways. One such problem is automated network change validation and testing in virtual environments, something I&amp;rsquo;ve already &lt;a href=&#34;https://networkop.co.uk/blog/2016/02/19/network-ci-intro/&#34;&gt;written about&lt;/a&gt; a few years ago. The biggest problem with my original approach was that I had to create a custom &lt;a href=&#34;https://networkop.co.uk/blog/2016/01/01/rest-for-neteng/&#34;&gt;REST API SDK&lt;/a&gt; to work with a network simulation environment (UnetLab) that was never designed to be interacted with in a programmatic way. On the other hand, technologies like Docker have been very interesting since they were built around the idea of non-interactive lifecycle management and came with all &lt;a href=&#34;http://docker-py.readthedocs.io/en/stable/containers.html&#34; target=&#34;_blank&#34;&gt;API batteries&lt;/a&gt; already included. However, Docker was never intended to be used for network simulations and its support for multiple network interfaces is&amp;hellip; somewhat problematic.&lt;/p&gt;

&lt;h1 id=&#34;problem-demonstration&#34;&gt;Problem demonstration&lt;/h1&gt;

&lt;p&gt;The easiest way to understand the problem is to see it. Let&amp;rsquo;s start with a blank Docker host and create a few networks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker network create net1
docker network create net2
docker network create net3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s see what prefixes have been allocated to those networks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker network inspect -f &amp;quot;{{range .IPAM.Config }}{{.Subnet}}{{end}}&amp;quot; net1
172.17.0.0/16
docker network inspect -f &amp;quot;{{range .IPAM.Config }}{{.Subnet}}{{end}}&amp;quot; net2
172.18.0.0/16
docker network inspect -f &amp;quot;{{range .IPAM.Config }}{{.Subnet}}{{end}}&amp;quot; net3
172.19.0.0/16
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, let&amp;rsquo;s create a container and attach it to these networks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker create --name test -it alpine sh
docker network connect net1 test
docker network connect net2 test
docker network connect net3 test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now obviously you would expect for networks to appear in the same order as they were attached, right? Let&amp;rsquo;s see if it&amp;rsquo;s true:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker start test
docker exec -it test sh -c &amp;quot;ip a | grep &#39;inet&#39;&amp;quot;
inet 127.0.0.1/8 scope host lo
inet 172.26.0.2/16 brd 172.26.255.255 scope global eth0
inet 172.17.0.2/16 brd 172.17.255.255 scope global eth1
inet 172.18.0.2/16 brd 172.18.255.255 scope global eth2
inet 172.19.0.2/16 brd 172.19.255.255 scope global eth3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks good so far. The first interface (172.26.0.2/16) is the docker bridge that was attached by default in &lt;code&gt;docker create&lt;/code&gt; command. Now let&amp;rsquo;s add another network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker network create net4
docker stop test
docker network connect net4 test
docker start test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s examine our interfaces again:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker exec -it test sh -c &amp;quot;ip a | grep &#39;inet&#39;&amp;quot;
inet 127.0.0.1/8 scope host lo
inet 172.26.0.2/16 brd 172.26.255.255 scope global eth0
inet 172.20.0.2/16 brd 172.20.255.255 scope global eth3
inet 172.17.0.2/16 brd 172.17.255.255 scope global eth2
inet 172.18.0.2/16 brd 172.18.255.255 scope global eth1
inet 172.19.0.2/16 brd 172.19.255.255 scope global eth4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;re seeing that networks are in a completely different order. Looks like net1 is connected to eth2, net2 to eth1, net3 to eth4 and net4 to eth3. In fact, this issue should manifest itself even with 2 or 3 networks, however, I&amp;rsquo;ve found that it doesn&amp;rsquo;t always reorder them in that case.&lt;/p&gt;

&lt;h1 id=&#34;cnm-and-libnetwork-architecture&#34;&gt;CNM and libnetwork architecture&lt;/h1&gt;

&lt;p&gt;In order to better understand the issue, it helps to know the CNM terminology and network lifecycle events which are explained in libnetwork&amp;rsquo;s &lt;a href=&#34;https://github.com/docker/libnetwork/blob/master/docs/design.md&#34; target=&#34;_blank&#34;&gt;design document&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/docker/libnetwork/raw/master/docs/cnm-model.jpg?raw=true&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each time we run a &lt;code&gt;docker network create&lt;/code&gt; command a new &lt;strong&gt;CNM network&lt;/strong&gt; object is created. This object has a specific network type (&lt;code&gt;bridge&lt;/code&gt; by default) which identifies the driver to be used for the actual network implementation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;network, err := controller.NewNetwork(&amp;quot;bridge&amp;quot;, &amp;quot;net1&amp;quot;, &amp;quot;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When container gets attached to its networks, first time in &lt;code&gt;docker create&lt;/code&gt; and subsequently in &lt;code&gt;docket network connect&lt;/code&gt; commands, an &lt;strong&gt;endpoint object&lt;/strong&gt; is created on each of the networks being connected. This endpoint object represents container&amp;rsquo;s point of attachment (similar to a switch port) to docker networks and may allocate IP settings for a future network interface.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;ep, err := network.CreateEndpoint(&amp;quot;ep1&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At the time when container gets attached to its first network, a &lt;strong&gt;sandbox object&lt;/strong&gt; is created. This object represents a container inside CNM object model and stores pointers to all attached network endpoints.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;sbx, err := controller.NewSandbox(&amp;quot;test&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, when we start a container using &lt;code&gt;docker start&lt;/code&gt; command, the corresponding &lt;strong&gt;sandbox gets attached&lt;/strong&gt; to all associated network endpoints using the &lt;code&gt;ep.Join(sandbox)&lt;/code&gt; call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;for _, ep := range epList {
	if err := ep.Join(sb); err != nil {
		logrus.Warnf(&amp;quot;Failed attach sandbox %s to endpoint %s: %v\n&amp;quot;, sb.ID(), ep.ID(), err)
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;going-down-the-rabbit-hole&#34;&gt;Going down the rabbit hole&lt;/h1&gt;

&lt;p&gt;Looking at the above snippet from &lt;code&gt;sandbox.go&lt;/code&gt;, we can assume that the order in which networks will be attached to a container will depend on the order of elements inside the &lt;code&gt;epList&lt;/code&gt; array, which gets built earlier in the function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;epList := sb.getConnectedEndpoints()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s see what happens inside that method call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (sb *sandbox) getConnectedEndpoints() []*endpoint {
	sb.Lock()
	defer sb.Unlock()

	eps := make([]*endpoint, len(sb.endpoints))
	for i, ep := range sb.endpoints {
		eps[i] = ep
	}

	return eps
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So &lt;code&gt;epList&lt;/code&gt; is just an array of endpoints that gets built by copying values from &lt;code&gt;sb.endoints&lt;/code&gt;, which itself is an attribute (or field) inside the &lt;code&gt;sb&lt;/code&gt; struct.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type epHeap []*endpoint

type sandbox struct {
  id                 string
  containerID        string
...
  endpoints          epHeap
...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point it looks like &lt;code&gt;endpoints&lt;/code&gt; is just an array of pointers to endpoint objects, which still doesn&amp;rsquo;t explain the issue we&amp;rsquo;re investigating. Perhaps it would make more sense if we saw how a sandbox object gets created.&lt;/p&gt;

&lt;p&gt;Since sandbox object gets created by calling &lt;code&gt;controller.NewSandbox()&lt;/code&gt; method, let&amp;rsquo;s see exactly how this is done by looking at the code inside the &lt;code&gt;controller.go&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (c *controller) NewSandbox(containerID string, options ...SandboxOption) (Sandbox, error) {
...
  // Create sandbox and process options first. Key generation depends on an option
  if sb == nil {
    sb = &amp;amp;sandbox{
      id:                 sandboxID,
      containerID:        containerID,
      endpoints:          epHeap{},
...
    }
  }

  heap.Init(&amp;amp;sb.endpoints)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last statement explains why sandbox connects networks in random order. The &lt;code&gt;endpoints&lt;/code&gt; array is, in fact, a &lt;a href=&#34;https://golang.org/pkg/container/heap/&#34; target=&#34;_blank&#34;&gt;heap&lt;/a&gt; - an ordered tree, where parent node is always smaller than (or equal to) its children (minheap). Heap is used to implement a priority queue, which should be familiar to every network engineer who knows QoS. One of heap&amp;rsquo;s properties is that it re-orders elements every time an element gets added or removed, in order to maintain the heap invariant (parent &amp;lt;= child).&lt;/p&gt;

&lt;h1 id=&#34;problem-solution&#34;&gt;Problem solution&lt;/h1&gt;

&lt;p&gt;It turns out the problem demonstrated above is a very well-known problem with multiple opened issues on Github [&lt;a href=&#34;https://github.com/moby/moby/issues/25181&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;,&lt;a href=&#34;https://github.com/moby/moby/issues/23742&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;,&lt;a href=&#34;https://github.com/moby/moby/issues/35221&#34; target=&#34;_blank&#34;&gt;3&lt;/a&gt;]. I was lucky enough to have discovered this problem right after &lt;a href=&#34;https://github.com/docker/libnetwork/issues/2093&#34; target=&#34;_blank&#34;&gt;this pull request&lt;/a&gt; got submitted, which is what helped me understand what the issue was in the first place. This pull request reference a &lt;a href=&#34;https://github.com/cziebuhr/libnetwork/commit/d047825d4d156bc4cf01bfe410cb61b3bc33f572&#34; target=&#34;_blank&#34;&gt;patch&lt;/a&gt; that swaps the heapified array with a normal one. Below I&amp;rsquo;ll show how to build a custom docker daemon binary using this patch. We&amp;rsquo;ll start with a privileged centos-based Docker container:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Update 2018-04-28&lt;/strong&gt;: Much easier procedure is documented &lt;a href=&#34;https://github.com/networkop/libnetwork-multinet.git&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --privileged -it centos bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inside this container we need to install all the dependencies along with Docker. Yes, you need Docker to build Docker:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yum install -y git iptables \
            make &amp;quot;Development Tools&amp;quot; \
            yum-utils device-mapper-persistent-data \
            lvm2

yum-config-manager --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

yum install docker-ce -y

# Start docker in the background
/usr/bin/dockerd &amp;gt;/dev/null &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next let&amp;rsquo;s clone the Docker master branch and the patched fork of libnetwork:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --depth=1 https://github.com/docker/docker.git /tmp/docker-repo
git clone https://github.com/cziebuhr/libnetwork.git /tmp/libnetwork-patch
cd /tmp/libnetwork-patch
git checkout d047825d4d156bc4cf01bfe410cb61b3bc33f572
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I tried using &lt;a href=&#34;https://github.com/LK4D4/vndr&#34; target=&#34;_blank&#34;&gt;VNDR&lt;/a&gt; to update the libnetwork files inside the Docker repository, however I ran into problems with incompatible git options on CentOS. So instead I&amp;rsquo;ll update libnetwork manually, with just the files that are different from the original repo:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /tmp/libnetwork-patch
/usr/bin/cp controller.go endpoint.go sandbox.go sandbox_store.go /tmp/docker-repo/vendor/github.com/docker/libnetwork/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Final step is to build docker binaries. This step may require up to 100G of free disk space and may take up to 60 minutes depending on your network speed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /tmp/docker-repo
make build
make binary
...
Created binary: bundles/binary-daemon/dockerd-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;verification&#34;&gt;Verification&lt;/h1&gt;

&lt;p&gt;Once done, we can retrieve the binaries outside of the build container:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;find /var/lib/docker -name dockerd
/var/lib/docker/overlay2/ac310ef5172acac7e8cb748092a9c9d1ddc3c25a91e636ab581cfde0869f5d76/diff/tmp/docker-repo/bundles/binary-daemon/dockerd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can swap the current docker daemon with the patched one:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yum install which -y
systemctl stop docker.service
DOCKERD=$(which dockerd)
mv $DOCKERD $DOCKERD-old
cp /tmp/docker-repo/bundles/latest/binary-daemon/dockerd $DOCKERD
systemctl start docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure that SELinux security context on both $DOCKERD and $DOCKERD-old are the same&lt;/p&gt;

&lt;p&gt;If we re-run our tests now, the interfaces are returned in the same exact order they were added:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker start test
docker exec -it test sh -c &amp;quot;ip a | grep &#39;inet&#39;&amp;quot;
inet 127.0.0.1/8 scope host lo
inet 172.26.0.2/16 brd 172.26.255.255 scope global eth0
inet 172.17.0.2/16 brd 172.17.255.255 scope global eth1
inet 172.18.0.2/16 brd 172.18.255.255 scope global eth2
inet 172.19.0.2/16 brd 172.19.255.255 scope global eth3
inet 172.20.0.2/16 brd 172.20.255.255 scope global eth4
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Huge kudos to the original &lt;a href=&#34;https://github.com/cziebuhr&#34; target=&#34;_blank&#34;&gt;author&lt;/a&gt; of the &lt;a href=&#34;https://github.com/cziebuhr/libnetwork/commit/d047825d4d156bc4cf01bfe410cb61b3bc33f572&#34; target=&#34;_blank&#34;&gt;libnetwork patch&lt;/a&gt; which is the sole reason this blogpost exists. I really hope that this issue will get resolved, in this form or another (could it be possible to keep track of the order in which endpoints are added to a sandbox and use that as a criteria for heap sort?), as this will make automated network testing much more approachable.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack SDN</title>
      <link>https://networkop.co.uk/tags/openstack-sdn/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/tags/openstack-sdn/</guid>
      <description>

&lt;h2 id=&#34;openstack-sdn-tags-openstack-sdn-learn-through-hands-on-experience-everything-you-need-to-know-about-vanilla-openstack-neutron-implementation-of-virtual-networks-including-custom-sdn-controllers-like-ovn-opendaylight-and-opencontrail&#34;&gt;&lt;a href=&#34;https://networkop.co.uk/tags/openstack-sdn/&#34;&gt;OpenStack SDN&lt;/a&gt; - Learn through hands-on experience everything you need to know about vanilla OpenStack Neutron implementation of virtual networks, including custom SDN controllers like OVN, OpenDaylight and OpenContrail&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
