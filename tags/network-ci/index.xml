<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Network-ci on networkop</title>
    <link>https://networkop.co.uk/tags/network-ci/</link>
    <description>Recent content in Network-ci on networkop</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Michael Kashin 2018</copyright>
    <lastBuildDate>Thu, 17 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://networkop.co.uk/tags/network-ci/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Multi-Vendor Network Simulations at Scale with meshnet-cni and vrnetlab</title>
      <link>https://networkop.co.uk/post/2019-01-k8s-vrnetlab/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2019-01-k8s-vrnetlab/</guid>
      <description>In the previous post I&amp;rsquo;ve demonstrated how to build virtual network topologies on top of Kubernetes with the help of meshnet-cni plugin. As an example, I&amp;rsquo;ve shown topologies with 50 cEOS instances and 250 Quagga nodes. In both of these examples virtual network devices were running natively inside Docker containers, meaning they were running as (a set of) processes directly attached to the TCP/IP stack of the network namespace provided by the k8s pod.</description>
    </item>
    
    <item>
      <title>Large-scale network simulations in Kubernetes, Part 2 - Network topology orchestration</title>
      <link>https://networkop.co.uk/post/2018-11-k8s-topo-p2/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2018-11-k8s-topo-p2/</guid>
      <description>In the previous post I&amp;rsquo;ve demonstrated a special-purpose CNI plugin for network simulations inside kubernetes called meshnet. I&amp;rsquo;ve shown how relatively easy it is to build a simple 3-node topology spread across multiple kubernetes nodes. However, when it comes to real-life large-scale topology simulations, using meshnet &amp;ldquo;as is&amp;rdquo; becomes problematic due to the following reasons:
 Uploading topology information into etcd requires a lot of manual effort. Any customisation like startup configuration injection or exposure of internal ports is still a manual process.</description>
    </item>
    
    <item>
      <title>Large-scale network simulations in Kubernetes, Part 1 - Building a CNI plugin</title>
      <link>https://networkop.co.uk/post/2018-11-k8s-topo-p1/</link>
      <pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2018-11-k8s-topo-p1/</guid>
      <description>Building virtualised network topologies has been one of the best ways to learn new technologies and to test new designs before implementing them on a production network. There are plenty of tools that can help build arbitrary network topologies, some with an interactive GUI (e.g. GNS3 or EVE-NG/Unetlab) and some &amp;ldquo;headless&amp;rdquo;, with text-based configuration files (e.g. vrnetlab or topology-converter). All of these tools work by spinning up multiple instances of virtual devices and interconnecting them according to a user-defined topology.</description>
    </item>
    
    <item>
      <title>The problem of unpredictable interface order in multi-network Docker containers</title>
      <link>https://networkop.co.uk/post/2018-03-03-docker-multinet/</link>
      <pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/post/2018-03-03-docker-multinet/</guid>
      <description>Whether we like it or not, the era of DevOps is upon us, fellow network engineers, and with it come opportunities to approach and solve common networking problems in new, innovative ways. One such problem is automated network change validation and testing in virtual environments, something I&amp;rsquo;ve already written about a few years ago. The biggest problem with my original approach was that I had to create a custom REST API SDK to work with a network simulation environment (UnetLab) that was never designed to be interacted with in a programmatic way.</description>
    </item>
    
    <item>
      <title>Network-CI Part 3 - OSPF to BGP Migration in Active/Standby DC</title>
      <link>https://networkop.co.uk/blog/2016/03/03/network-ci-demo-large/</link>
      <pubDate>Wed, 23 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2016/03/03/network-ci-demo-large/</guid>
      <description>Current network overview Let&amp;rsquo;s start by taking a high-level look at our DC network routing topology. The core routing protocol is OSPF, it is responsible for distributing routing information between the Core and WAN layers of the network. WAN layer consists of two MPLS L3VPN services running BGP as PE-CE protocol and two DMVPN Hubs running EIGRP. All WAN layer devices perform mutual redistribution between the respective WAN protocol and OSPF.</description>
    </item>
    
    <item>
      <title>Network-CI Part 2 - Small Network Demo</title>
      <link>https://networkop.co.uk/blog/2016/03/03/network-ci-demo-small/</link>
      <pubDate>Thu, 03 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2016/03/03/network-ci-demo-small/</guid>
      <description>Demo network overview The network consists of 4 nodes interconnected via point-to-point links and running EIGRP as a routing protocol.
To create a local development environment you can clone my repository and reset it to work with your own Github account using git remote set-url origin https://github.com/USERNAME/OTHERREPOSITORY.git command.
Local development environment contains the following files describing the modelled topology:
 Configuration files for each node under the ./config directory Network topology in .</description>
    </item>
    
    <item>
      <title>Network-CI Part 1 - Automatically Building a VM With UNetLab and Jenkins</title>
      <link>https://networkop.co.uk/blog/2016/02/25/network-ci-dev-setup/</link>
      <pubDate>Thu, 25 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2016/02/25/network-ci-dev-setup/</guid>
      <description>Packer intro Packer is a tool that can automatically create virtual machines for different hypervisors and cloud platforms. The goal is to produce identically configured VMs for either VirtualBox, VMWare, Amazon or Google clouds based on a single template file. If you&amp;rsquo;re familiar with Vagrant, then you can also use Packer to create custom Vagrant boxes. In our case, however, we&amp;rsquo;re only concerned about VMWare since it&amp;rsquo;s the only type-2 hypervisor that supports nested hardware virtualisation (e.</description>
    </item>
    
    <item>
      <title>Network Continuous Integration and Delivery</title>
      <link>https://networkop.co.uk/blog/2016/02/19/network-ci-intro/</link>
      <pubDate>Fri, 19 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://networkop.co.uk/blog/2016/02/19/network-ci-intro/</guid>
      <description>CI/CD vs ITIL How do you implement changes in your network? In today&amp;rsquo;s world there&amp;rsquo;s 95% chance that you have to write up an RFC, submit it at least a week before the planned implementation date, go through at least one CAB meeting and only then, assuming it got approved, can you implement it. But the most important question is &amp;lsquo;how do you test&amp;rsquo;? Do you simply content yourself with a few pings or do you make sure all main routes are in place?</description>
    </item>
    
  </channel>
</rss>