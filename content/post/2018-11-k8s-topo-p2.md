+++
title = "Large-scale network simulations in Kubernetes, Part 2 - Network topology orchestration"
date = 2018-11-11T00:00:00Z
categories = ["automation"]
tags = ["network-ci", "devops"]
summary = "Using k8s-topo - network topology orchestration tool for kubernetes"
+++

In the [previous post][first-post] I've demonstrated a special-purpose CNI plugin for network simulations inside kubernetes called [meshnet][meshnet]. I've shown how relatively easy it is to build a simple 3-node topology spread across multiple kubernetes nodes. However, when it comes to real-life large-scale topology simulations, using meshnet "as is" becomes problematic due to the following reasons:

1. Uploading topology information into etcd requires a lot of manual effort.
2. Any customisation like startup configuration injection or exposure of internal ports is still a manual process.

That is why I built [k8s-topo] - an orchestrator for network simulations inside kubernetes. It automates a lot of these manual steps and provides a simple and user-friendly interface to create networks of any size and configuration.

# k8s-topo overview

[k8s-topo][k8s-topo] is a Python script that creates network topologies inside k8s based on a simple YAML file. It uses syntax similar to [docker-topo][docker-topo] with a few modifications to account for the specifics of kubernetes environment. For instance, the following file is all what's required to create and configure a simple 3-node topology:

```yaml
etcd_port: 32379
links:
  - endpoints: ["host-1:eth1:12.12.12.1/24", "host-2:eth1:12.12.12.2/24"]
  - endpoints: ["host-1:eth2:13.13.13.1/24", "host-3:eth1:13.13.13.3/24"]
  - endpoints: ["host-2:eth2:23.23.23.2/24", "host-3:eth2:23.23.23.3/24"]
```

Behind the scenes it uses [kubernetes][k8s-python] and [etcd][etcd-python] python libraries to accomplish the following:

* Upload topology information into etcd
* Create a pod for each network device mentioned in the topology file
* If present, mount devices startup configuration as volumes inside pods
* Expose internal HTTPs port as a [NodePort][nodeport] service for every device

At the time of writing, k8s-topo supported three devices types, that get matched based on the device name prefix:

* Host device - simulated by Alpine image, matched by `host` prefix
* cEOS device - simulated by Arista cEOS image, matched by `sw` prefix 
* Quagga device - simulated by Alpine image with Quagga installed, matched by `qrtr` prefix

As an optional extra, k8s-topo can generate a [D3.js][d3js]-compliant graph that visualises the deployed network topology on an interactive web graph as will be shown later.


# Installation

There are two main ways to install k8s-topo. The more traditional way will install k8s-topo as a python script on a local machine:

```
pip3 install git+https://github.com/networkop/k8s-topo.git
```

Another option is to install k8s-topo as a pod on top of a kubernetes cluster (it could be the same cluster that will be used for network simulations). For this option, we first need to build a k8s-topo docker image:

```
build.sh <dockerhub_username>
```

And then create a pod and its associated service:

```
kubectl create -f kube-k8s-topo.yml
```

Technically, it doesn't matter where k8s-topo is installed as long as it can access the k8s cluster and meshnet's etcd service. However, for the sake of simplicity, examples below will assume hosted k8s install, which means that we only need to specify the `etcd_port` variable, leaving all others as default.

# Random topology examples

To demonstrate capabilities of our orchestrator, I've written a random topology builder [script](https://github.com/networkop/k8s-topo/blob/master/examples/builder/builder) that first generates a [uniform spanning tree](https://en.wikipedia.org/wiki/Loop-erased_random_walk) graph, which is then used to create a topology definition YAML file along with a set of configuration files for each device. These configuration files accomplish two things:

* Configure a unique Loopback IP address in the `198.51.100.0/24` range
* Enable OSPF on all directly connected interfaces

The goal of this script is to be able to generate random large-scale network topologies that would be easy to test by simply ping-sweeping the range of all configured loopback addresses. Let's start with a relatively small example of 20 cEOS containers.

## Building a 20-node cEOS topology

> Before we can start building cEOS topologies, we need to make cEOS Docker image available in a private docker registry. Refer to the k8s-topo Github repository for a complete list of [instructions](https://github.com/networkop/k8s-topo#private-docker-registry-setup).

The first step is to generate a random 20-node topology. From inside the k8s-topo pod run:

```bash
./examples/builder/builder --prefix sw 20 0
Total number of links generated: 19
```

Next, we can use k8s-topo to create our random topology inside k8s:

```bash
./bin/k8s-topo --create examples/builder/random.yml
```

```
/bin/k8s-topo --eif examples/builder/random.yml
INFO:__main__:All pods are running, trying to enable ip forwarding for cEOS pods
INFO:__main__:All cEOS pods have IP forwarding enabled
```

To be able easily see the generated topology we can create a D3 graph.

```bash
/k8s-topo # ./bin/k8s-topo --graph examples/builder/random.yml
INFO:__main__:D3 graph created
INFO:__main__:URL: http://10.83.30.252:32080
```
The built-in nginx server is able to render a simple web page with each device coloured according to the k8s node they are running on:

![20-node](/img/k8s-topo-20.png)

From here we can see that `sw-14` is one of the most distant nodes of the topology, so it would make sense to run the ping-sweep test from that device:

```bash
/k8s-topo # kubectl exec -it sw-14 bash
bash-4.3# for i in `seq 1 20`; do ping -c 1 -W 1 198.51.100.$i|grep from; done
64 bytes from 198.51.100.1: icmp_seq=1 ttl=62 time=93.1 ms
64 bytes from 198.51.100.2: icmp_seq=1 ttl=57 time=152 ms
64 bytes from 198.51.100.3: icmp_seq=1 ttl=56 time=282 ms
64 bytes from 198.51.100.4: icmp_seq=1 ttl=58 time=91.6 ms
64 bytes from 198.51.100.5: icmp_seq=1 ttl=64 time=12.7 ms
64 bytes from 198.51.100.6: icmp_seq=1 ttl=58 time=92.9 ms
64 bytes from 198.51.100.7: icmp_seq=1 ttl=56 time=148 ms
64 bytes from 198.51.100.8: icmp_seq=1 ttl=56 time=358 ms
64 bytes from 198.51.100.9: icmp_seq=1 ttl=59 time=104 ms
64 bytes from 198.51.100.10: icmp_seq=1 ttl=55 time=206 ms
64 bytes from 198.51.100.11: icmp_seq=1 ttl=57 time=238 ms
64 bytes from 198.51.100.12: icmp_seq=1 ttl=55 time=364 ms
64 bytes from 198.51.100.13: icmp_seq=1 ttl=55 time=331 ms
64 bytes from 198.51.100.14: icmp_seq=1 ttl=64 time=0.033 ms
64 bytes from 198.51.100.15: icmp_seq=1 ttl=63 time=22.5 ms
64 bytes from 198.51.100.16: icmp_seq=1 ttl=61 time=50.3 ms
64 bytes from 198.51.100.17: icmp_seq=1 ttl=59 time=80.8 ms
64 bytes from 198.51.100.18: icmp_seq=1 ttl=60 time=59.0 ms
64 bytes from 198.51.100.19: icmp_seq=1 ttl=58 time=92.5 ms
64 bytes from 198.51.100.20: icmp_seq=1 ttl=56 time=126 ms
```

This test proves that `sw-14` can reach the loopback IP of every other device in the topology and, since the topology does not have redundant links, that means that k8s-topo, together with meshnet, have interconnected all devices correctly.

To destroy the topology:

```
./bin/k8s-topo --destroy examples/builder/random.yml
```

## Building a 200-node Quagga topology

Now let's take it up a notch and test the topology with 200 Quagga nodes. The first step is to generate the topology definition and configuration files:

```bash
/examples/builder/builder 200 0
Total number of links generated: 199
```

```bash
./bin/k8s-topo --create examples/builder/random.yml
```

![200-node](/img/k8s-topo-200.png)



[vrnetlab]: https://github.com/plajjan/vrnetlab
[k8s-python]: https://github.com/kubernetes-client/python
[docker-topo]: https://github.com/networkop/arista-ceos-topo
[nodeport]: https://kubernetes.io/docs/concepts/services-networking/service/#nodeport
[d3js]: https://www.tutorialspoint.com/d3js/d3js_graphs.htm
[first-post]: /post/2018-11-k8s-topo-p1/
[etcd-python]: https://github.com/kragniz/python-etcd3
[eve-ng]: http://eve-ng.net/
[topology-converter]: https://github.com/CumulusNetworks/topology_converter
[vmx]: https://www.juniper.net/documentation/en_US/vmx14.1/topics/reference/general/vmx-hw-sw-minimums.html
[linux-bridge]: https://patchwork.ozlabs.org/patch/819153/
[multus]: https://github.com/intel/multus-cni
[cni-spec]: https://github.com/containernetworking/cni/blob/master/SPEC.md
[meshnet]: https://github.com/networkop/meshnet-cni
[koko]: https://github.com/redhat-nfvpe/koko
[k8s-topo]: https://github.com/networkop/k8s-topo
[ratchet]: https://github.com/dougbtv/ratchet-cni
[kokonet]: https://github.com/s1061123/kokonet