<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Routing | Network-oriented programming]]></title>
  <link href="http://networkop.github.io/blog/categories/routing/atom.xml" rel="self"/>
  <link href="http://networkop.github.io/"/>
  <updated>2017-04-03T20:55:29+01:00</updated>
  <id>http://networkop.github.io/</id>
  <author>
    <name><![CDATA[Michael Kashin]]></name>
    <email><![CDATA[mmkashin@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Network-CI Part 3 - OSPF to BGP Migration in Active/Standby DC]]></title>
    <link href="http://networkop.github.io/blog/2016/03/23/network-ci-demo-large/"/>
    <updated>2016-03-23T00:00:00+00:00</updated>
    <id>http://networkop.github.io/blog/2016/03/23/network-ci-demo-large</id>
    <content type="html"><![CDATA[<p>The final post in a series demonstrates how to use the <strong>network-ci</strong> tools to safely replace a core routing protocol inside a small Active/Standby Data Centre.</p>

<!--more-->


<h2>Current network overview</h2>

<p>Let&rsquo;s start by taking a high-level look at our DC network routing topology. The core routing protocol is OSPF, it is responsible for distributing routing information between the Core and WAN layers of the network. WAN layer consists of two MPLS L3VPN services running BGP as PE-CE protocol and two DMVPN Hubs running EIGRP. All WAN layer devices perform mutual redistribution between the respective WAN protocol and OSPF.</p>

<p><img class="center" src="/images/network-ci-dc-before.png" title="Current network topology" ></p>

<h2>Target network overview</h2>

<p>The task is to replace OSPF with BGP as the core routing protocol inside the Data Centre. There are many advantages to using BGP inside a DC, in our case they are:</p>

<ul>
<li>Enhanced traffic routing and filtering policies</li>
<li>Reduced number of redistribution points</li>
<li>Because Ivan Pepelnjak <a href="http://blog.ipspace.net/2016/02/using-bgp-in-data-center-fabrics.html">said so</a></li>
</ul>


<p>We&rsquo;re not going getting rid of OSPF completely, but rather reduce its function to a simple distribution of <em>internal</em> DC prefixes. BGP will be running on top of OSPF and distribute all the DC and WAN <em>summary</em> prefixes.</p>

<p><img class="center" src="/images/network-ci-dc-after.png" title="Target network topology" ></p>

<h2>Physical topology overview</h2>

<p>Now let&rsquo;s take a closer look at the network that we&rsquo;re going to emulate. All green devices on the left-hand side constitute the <strong>Active</strong> Data Centre, that is where all the traffic will flow under normal conditions. All green devices have red <strong>Standby</strong> counterparts. These devices will pick up the function of traffic forwarding in case their green peer becomes unavailable.</p>

<p><img class="center" src="/images/network-ci-dc-full.png" title="Full demo topology" ></p>

<p>When simulating a real-life network it&rsquo;s often impossible to fit an exact replica inside a network emulator. That&rsquo;s why using <strong>mock</strong> devices is a crucial part in every simulation. The function of a mock is to approximate a set of network devices. There&rsquo;s a number of mock devices on our diagram colour-coded in purple. These devices simulate the remaining parts of the network. For example, <strong>Cloud</strong> devices may represent <abbr title=" Top-Of-the-Rack">TOR</abbr> switches, while <strong>MPLS/DMVPN</strong> devices represent remote WAN sites. Normally these devices will have some made-up configuration that best reflects real life, but not necessarily a copy-paste from an existing network device.</p>

<p>It&rsquo;s also important to pick the right amount of mock devices to strike the balance between accuracy and complexity. For example, for WAN sites it may suffice to create one site per unique combination of WAN links to make sure WAN failover works as expected.</p>

<h2>Traffic flow definition</h2>

<p>Let&rsquo;s define how we would expect the traffic to flow through our network. Let&rsquo;s assume that we should always try to use MPLS links when possible and only use DMVPN when both MPLS links are down. This translates to the following order of WAN links' precedence:</p>

<ol>
<li>Primary MPLS link</li>
<li>Standby MPLS link</li>
<li>Primary DMVPN link</li>
<li>Standby DMVPN link</li>
</ol>


<p>Based on that we can create the following traffic flows definition for network working under normal conditions.</p>

<pre><code class="text /network/tests/traffic_flows.txt">1 Failed None
  From FW to MPLS-DMVPN via Primary-WAN, Primary-MPLS
  From FW to DMVPN-ONLY via Primary-CORE-SW, Primary-DMVPN
  From FW to MPLS-ONLY via Primary-WAN, Primary-MPLS
  From Cloud-1 to FW Loopback0 via Primary-CORE-SW
  From Cloud-2 to MPLS-DMVPN via Primary-WAN, Primary-MPLS
</code></pre>

<p>We expect all traffic to flow through active devices even when the path may be suboptimal, like it&rsquo;s the case with traffic from Cloud-2. Similarly, we can create traffic flows definitions for different failure conditions. The complete <a href="https://github.com/networkop/network-ci/blob/master/acme-large/network/tests/traffic_flows.txt">traffic flows definition file</a> contains 2 additional failure scenarios covering the outage of the primary MPLS link and a complete outage of the primary core switch.</p>

<h2>Workflow example</h2>

<p>This is how you would approach a project like this.</p>

<ol>
<li>Get a copy of network-ci <a href="http://networkop.github.io/blog/2016/02/25/network-ci-dev-setup/">VM</a></li>
<li>Get a local copy of network-ci <a href="https://github.com/networkop/network-ci/tree/master/acme-large">tools</a></li>
<li>Copy configuration from real-life devices into the <a href="https://github.com/networkop/network-ci/tree/master/acme-large/config">config directory</a></li>
<li>Add configuration files for mock devices to the same directory</li>
<li>Review the <a href="https://github.com/networkop/network-ci/blob/master/acme-large/network/topology.py">topology definition file</a> to make sure it reflects our physical diagram</li>
<li>Review the UNL <a href="https://github.com/networkop/network-ci/blob/master/acme-large/network/unetlab.yml">configuration file</a> to make sure it points to the correct IP address assigned to your network-ci VM</li>
<li>Kick-off topology build inside UNL by running <code>./0_built_topo.py</code> script</li>
<li>Verify that traffic flows as expected with <code>2_test.py</code> script</li>
<li>Start the real-time monitoring with <code>1_monitor.py</code> script</li>
<li>Implement required changes on individual devices (all required changes can be found in <a href="https://github.com/networkop/network-ci/blob/master/acme-large/network/ospf-bgp.txt">ospf-bgp.txt</a> file)</li>
<li>Make sure that the network still behaves as before by running <code>2_test.py</code> script</li>
<li>Destroy the topology in UNL by running <code>3_destroy_topo.py</code></li>
</ol>


<h2>Continuous Integration</h2>

<p>In the <a href="http://networkop.github.io/blog/2016/03/03/network-ci-demo-small/">previous post</a> I&rsquo;ve showed how to use Jenkins to setup the CI environment for a small demo network. The same method can be applied to setup the job for our small Data Centre. It is simply a matter of changing the directory name from <strong>acme-small</strong> to <strong>acme-large</strong> in the first build step.</p>

<h2>Source code</h2>

<p>All code from this and previous posts is available on <a href="https://github.com/networkop/network-ci/tree/master/acme-large">Github</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Enterprise IP Routing Best Practices]]></title>
    <link href="http://networkop.github.io/blog/2015/06/03/ent-ip-routing-bcp/"/>
    <updated>2015-06-03T22:19:02+01:00</updated>
    <id>http://networkop.github.io/blog/2015/06/03/ent-ip-routing-bcp</id>
    <content type="html"><![CDATA[<p>What motivated me to write this post is a state of the IP routing of some of the enterprise networks I&rsquo;ve seen.
A quick <code>show ip route</code> command reveals a non-disentanglable mixture of dynamic and static route with multiple points of redistribution and complex,
rigid filtering rules, something you&rsquo;d only see in your bad dream or a CCIE-level lab. It certainly takes
a good engineer to understand how it works and even that can take up to several hours. I think the reason for that
is that people have generally been concentrated on learning about the routing protocol, how it works, all the knobs you can twist
to influence a routing decision logic. However, one thing often overlooked is the routing protocols best practice design,
i.e. <strong>when</strong> and <strong>how</strong> to use a particular protocol.
And since the latter is often an acquired skill, a lot of not-so-lucky engineers end up with wrong ideas and concepts
in the heads. Below I&rsquo;ll try to list what <em>I</em>{:.underline} consider a best practice design of today&rsquo;s enterprise networks.</p>

<!--more-->


<h2>OSPF, EIGRP, BGP? Which one to use?</h2>

<p>Golden rule is to always use a protocol where it was designed to be used. Use and constrain IGP to a single autonomous system.
For enterprise networks autonomous system can be:</p>

<ul>
<li>a single, geographically-constrained office network</li>
<li>remote branch office network</li>
<li>campus network</li>
<li>data centre</li>
</ul>


<p>Use BGP to interconnect these systems. When there&rsquo;s a choice to use iBGP vs eBGP, always prefer eBGP since it has less restrictions.
However for some designs iBGP is a better fit (i.e. Hub-and-Spoke topologies). Almost for every WAN technology there&rsquo;s a <em>preferred</em>
WAN protocol, e.g. eBGP for L3VPN, iBGP for DMVPN/FlexVPN, so always check with the vendor&rsquo;s design guide.</p>

<h2>IGP best practices</h2>

<p>The choice of a particular IGP is mainly irrelevant. EIGRP scales better in a well-structured hierarchical network, whereas link-state protocol like OSPF
don&rsquo;t require any underlying structure. In fact, best practice for OSPF design, for quite some time, has been to put all routers in a single Area 0 regardless
of their geographical location. This rule, like any, has its' exceptions and special dampening/ advertisement containment rules need to be applied
to links prone to flapping (e.g. aerial links). However, both EIGRP and OSPF have proven to be quite stable and scalable even with <em>not-so-good</em> designs.<br/>
I follow the these rules when designing an IGP:</p>

<ul>
<li>Advertise all routers' networks, i.e. <code>network 0.0.0.0 255.255.255.255</code> command</li>
</ul>


<blockquote><p>ideally within a single AS there will be a full-mesh reachability between the devices</p></blockquote>

<ul>
<li>Explicitly control which interfaces will form routing adjacencies with <code>passive interface</code> commands</li>
<li>statically set router-id to the address of loopback interface which uniquely identifies the device
(not included in any other summary and not advertise by anyone else)</li>
<li>When using EIGRP exclude bandwidth and leave only delay in metric calculation with <code>metric weights 0 0 0 1 0 0</code></li>
</ul>


<blockquote><p>as opposed to bandwidth, interface delay is uniquely used by EIGRP so changing it won&rsquo;t negatively affect any other processes</p></blockquote>

<ul>
<li>When using OSPF always update reference bandwidth on all routers to 100G with <code>auto-cost reference-bandwidth 100000</code></li>
<li>All WAN links should be known to IGP natively but should be passive at the same time</li>
<li>Avoid redistribution between IGP and BGP at all costs</li>
</ul>


<blockquote><p>redistribution can create routing loops due to loss of native routing protocol metric. troubleshooting these loops is one of the most difficult
tasks for a network engineer</p></blockquote>

<h2>BGP best practices</h2>

<p>Whenever I design a non-stub (i.e. transit) network I try to enable BGP on all transit devices. This rule helps me avoid using redistribution between
IGP and BGP. Assuming a standard dual-core, dual-wan link topology the core will become a route-reflector whereas WAN routers will become RR-clients.
The only issue is that a lot of devices used in the network core still come with limit or no BGP support. In this case redistribution can be an option, however
carefull planning and strict filtering rules need to be put in place in order to prevent any potential routing loops.
These are my BGP best practices:</p>

<ul>
<li>Always statically configure BGP router-id to be equal to ip address of loopback interface</li>
<li>Always send/receive both standard and extended communities <code>neighbor X.X.X.X send-community both</code></li>
<li>Always add description to a neighbor. You can&rsquo;t overdocument your network</li>
<li>When configuring iBGP always use loopbacks (advertised by your IGP) for peering. This will help a lot with performance optimisation described below</li>
<li>Always keep track of BGP AS numbers in use in the network</li>
<li>For every network that doesn&rsquo;t need to be transit assign community <code>local-as</code> in the inbound route-map</li>
<li>Whenever possible filter <strong>outbound</strong> rather than <strong>inbound</strong></li>
</ul>


<blockquote><p>this way only infromation <strong>that is needed</strong> is sent to the neighbor</p></blockquote>

<ul>
<li>Always configure <code>ip bgp community new-format</code> on all routers</li>
<li>Only inject <strong>summaries</strong> into BGP. The only exception can be routers' loopback address which can be used by remote SLA monitoring.</li>
</ul>


<blockquote><p>This is the key distinction between IGP and BGP. IGPs deal with all networks within an AS big or small.
BGP deals with networks that represent a whole AS, i.e. summaries.
Normally, the core device in the network originates a summary from a static <em>null</em> route and advertises it to all the neighbors.</p></blockquote>

<ul>
<li>Always tag all prefixes injected into BGP with communities. For example:</li>
</ul>


<blockquote><p>65000:0 - for site-specific summary<br/>
65000:1 - for smaller, site-specific subnets outside of summary range (e.g. DMZ)<br/>
65000:3 - for 3rd-party routes (e.g. provider-originated routes, interconnects with other clients)</p></blockquote>

<ul>
<li>Always filter based on communities rather than prefix lists or access-lists</li>
<li>Do not use route filtering as a security measure. Firewalls are designed to do that</li>
<li>For any route decision manipulation rely on explicitly configured metrics and not on, say, router-id or IGP metric</li>
</ul>


<blockquote><p>Use as few metric manipulations as possible. For example use local-preference for outbound and as-path for inbound path selection</p></blockquote>

<ul>
<li>Always tune BGP convergence timers (more on that below)</li>
</ul>


<h2>BGP performance tuning</h2>

<ul>
<li>BFD</li>
</ul>


<p>This seemingly <em>old</em> technology unfortunately still sees very little adoption in the enterprise market. It is the best option for
fast high-bandwidth links and should be used whenever possible</p>

<ul>
<li>external/internal fall-over</li>
</ul>


<p>This convergence optimisation techniques rely on the presence of route to neighbor in the routing table.
as soon the route is gone, the neighborship is brought down. Fast fall-over is enabled by default for eBGP neighbors
on Cisco devices and should be enabled manually per neighbor(-group) for iBGP neighbors.</p>

<pre><code># the following triggers fall-over only if host-route to neighbor disappears
router bgp 10
 neighbor 1.1.1.1 remote-as 10
 neigbhor 1.1.1.1 fall-over route-map RM-BGP-FALLOVER
!
ip prefix-list PL-ALL-LOOPBACKS 0.0.0.0/0 ge 32
!
route-map RM-BGP-FALLOVER
 match ip address prefix-list PL-ALL-LOOPBACKS
!
</code></pre>

<ul>
<li>BGP keepalive timer</li>
</ul>


<p>Default BGP timers 30/180 seconds are too big for most of the cases. However, if fast fall-over is properly used they never need to be modified.
Internal fall-over effectively makes BGP neighborships rely on IGP default timers instead, while external fall-over will work only for directly connected
neighbor (or if a route to this neighbor recurses over a directly connected interface). The only reason to change the default timer values would be if the route
to external neighbor can potentially recurse over an internal interface (e.g. in case default route is present). In this case using `neighbor X.X.X.X keepalive 10 30"
would set keepalive/dead timers to 10/30 seconds. The timer values are negotiated to the lowest values between the two peers during neighborship establishment.</p>

<ul>
<li>Prefix-independent convergence and ip next-hop tracking</li>
</ul>


<p>These two optimisation techniques do not require any configuration and are enabled by default in all recent code versions.
PIC decouples ip prefixes and next-hops and allows for quicker convergence when multiple BGP prefixes are present in BGP RIB with different next-hops.
IP NH-tracking triggers route recomputation based on changes in the routing table (i.e. next-hop becoming unavailable) rather than waiting for the periodic update
scanner to run every 60 seconds. There&rsquo;s plenty of additional information about both PIC (<a href="http://blog.ipspace.net/2012/01/prefix-independent-convergence-pic.html">one</a>, <a href="http://blog.ine.com/2010/11/22/understanding-bgp-convergence/">two</a>, <a href="http://www.cisco.com/c/en/us/td/docs/routers/7600/ios/15S/configuration/guide/7600_15_0s_book/BGP.html">three</a>) and
IP NH-tracking (<a href="http://blog.ine.com/2010/11/22/understanding-bgp-convergence/">one</a>, <a href="http://www.cisco.com/c/en/us/td/docs/ios/12_2sb/feature/guide/sbbnhop.html">two</a>) on the internet.</p>

<h2>Conclusion</h2>

<p>Enterprise network designers should more often look at their Service Provider counterparts and how they do things.
SP design practices have been evolving for years and proved to be stable and scalable. License permitting, we can
apply the same rules in enterprise networks and end up with a more stable and scalable network.</p>
]]></content>
  </entry>
  
</feed>
