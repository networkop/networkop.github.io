<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Unetlab | Network-oriented programming]]></title>
  <link href="http://networkop.github.io/blog/categories/unetlab/atom.xml" rel="self"/>
  <link href="http://networkop.github.io/"/>
  <updated>2016-05-10T23:16:51-07:00</updated>
  <id>http://networkop.github.io/</id>
  <author>
    <name><![CDATA[Michael Kashin]]></name>
    <email><![CDATA[mmkashin@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Building a Multi-node OpenStack Lab in UNetLab]]></title>
    <link href="http://networkop.github.io/blog/2016/04/18/os-unl-lab/"/>
    <updated>2016-04-18T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/04/18/os-unl-lab</id>
    <content type="html"><![CDATA[<p>In the <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous post</a> I&rsquo;ve demonstrated how to get a working instance of a single-node OpenStack inside <a href="http://www.unetlab.com/">UNetLab</a>. In this post we&rsquo;ll continue building on that by adding two new compute nodes and redesigning our network to resemble something you might actually see in a real life.</p>

<!--more-->


<h2>OpenStack network requirements</h2>

<p>Depending on the number of deployed <a href="https://www.openstack.org/software/project-navigator/">components</a>, OpenStack physical network requirements could be different. In our case we&rsquo;re not going to deploy any storage solution and simply use the <strong>ephemeral</strong> storage, i.e. hard disk that&rsquo;s a part of a virtual machine. However, even in minimal installations, there are a number of networks that should be considered individually due to different connectivity requirements:</p>

<ul>
<li><p>Server <abbr title=" Out-Of-Band">OOB</abbr> <strong>management</strong> network - this is usually a dedicated physical network used mainly for server bootstrapping and OS deployment. It is a Layer 3 network with DHCP relays configured at each edge L3 interface and access to Internet package repositories.</p></li>
<li><p><strong>API</strong> network - used for internal communication between various OpenStack services. This can be a routed network without Internet access. The only requirement is any-to-any reachability within a single OpenStack environment.</p></li>
<li><p><strong>External</strong> network - used for public access to internal OpenStack virtual machines. This is the <em>outside</em> of OpenStack, with a pool of IP addresses used to NAT the internal IPs of public-facing virtual machines. This network <strong>must</strong> be Layer 2 adjacent <strong>only</strong> with a network control node.</p></li>
<li><p><strong>Tenant</strong> network - used for communication between virtual machines within OpenStack environment. Thanks to the use of VXLAN overlay, this can be a simple routed network that has any-to-any reachability between all Compute and Network nodes.</p></li>
</ul>


<h2>Building a lab network</h2>

<p>For labbing purposes it&rsquo;s possible to relax some of the above network requirements without seriously affecting the outcomes of our simulation. For example, it&rsquo;s possible to combine some of the networks and still satisfy the requirements stated above. These are the networks that will be configured inside UNetLab:</p>

<ul>
<li><p><strong>Management</strong> - this network will combine the functions of OOB and API networks. To isolate it from our data centre underlay I&rsquo;ll be using separate interfaces on virtual machines and connect them directly to Workstation&rsquo;s NAT interface (192.168.91.0/24 in my case)  to give them direct access to Internet.</p></li>
<li><p><strong>External</strong> - this network will be connected to Workstation&rsquo;s host-only NIC (192.168.247.0/24) through Vlan300 configured on one of the leaf switches. Since it must be L2 adjacent with the network control node our leaf switch will not perform any routing for this subnet.</p></li>
<li><p><strong>Tenant</strong> - this will be a routed leaf/spine <a href="https://en.wikipedia.org/wiki/Clos_network">Clos</a> fabric comprised of 3 leaf and 2 spine switches running a single-area OSPF process on all their links. Each server will have its own unique tenant subnet (Vlan100) terminated on the leaf switch and subnet injected into OSPF. The subnet used for this Vlan is going to be <code>10.0.X.0/24</code>, where X is the number of the leaf switch terminating the vlan.</p>

<p>  The links between switches are all L3 point-to-point with addresses borrowed from 169.254.0.0/16 range specifically to emphasize the fact that the internal addressing does not need to be known or routed outside of the fabric. The <strong>sole function of the fabric</strong> is to provide multiple equal cost paths between any pair of leafs, thereby achieving maximum link utilisation. Here&rsquo;s an example of a traceroute between Vlan100&rsquo;s of Leaf #1 and Leaf #3.</p></li>
</ul>


<pre><code class="text Traceroute inside an ECMP routed Clos fabric">L3#traceroute 10.0.1.1 source 10.0.3.1
Type escape sequence to abort.
Tracing the route to 10.0.1.1
VRF info: (vrf in name/id, vrf out name/id)
  1 169.254.31.111 1 msec
    169.254.32.222 0 msec
    169.254.31.111 0 msec
  2 169.254.12.1 1 msec
    169.254.11.1 1 msec *
</code></pre>

<h2>Building lab servers</h2>

<p>Based on my experience a standard server would have at least 3 physical interfaces - one for OOB management and a pair of interfaces for application traffic. The two application interfaces will normally be combined in a single <abbr title=" Link Aggregation Group">LAG</abbr> and connected to a pair of MLAG-capable TOR switches. Multi-chassis LAG or <a href="http://blog.ipspace.net/2010/10/multi-chassis-link-aggregation-basics.html">MLAG</a> is a pretty old and well-understood technology so I&rsquo;m not going to try and simulate it in the lab. Instead I&rsquo;ll simply assume that a server will be connected to a TOR switch via a single physical link. That link will be setup as a dot1q trunk to allow for multiple subnets to share it.</p>

<h2>Physical lab topology</h2>

<p>All the above requirements and assumptions result in the following topology that we need to build inside UNetLab:</p>

<p><img class="center" src="/images/neutron-native.png"></p>

<p>For servers I&rsquo;ll be using OpenStack node type that I&rsquo;ve described in my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous post</a>. The two compute nodes do not need as much RAM as the control node, so I&rsquo;ll reduce it to just 2GB.</p>

<p>For switches I&rsquo;ll be using a Cisco&rsquo;s <a href="http://www.unetlab.com/2014/11/adding-cisco-iouiol-images/">L2 IOU</a> image for now, mainly due to the low resource requirements. In the future I&rsquo;ll try and swap it for something else. As you can see from the sample config below, fabric configuration is very basic and can be easily replaced by any other solution:</p>

<pre><code class="text Leaf 1 configuration">interface Ethernet0/0
 no switchport
 ip address 169.254.11.1 255.255.255.0
 ip ospf network point-to-point
 duplex auto
!
interface Ethernet0/1
 no switchport
 ip address 169.254.12.1 255.255.255.0
 ip ospf network point-to-point
 duplex auto
!
interface Ethernet0/2
 switchport trunk allowed vlan 100
 switchport trunk encapsulation dot1q
 switchport mode trunk
!
interface Vlan100
 ip address 10.0.1.1 255.255.255.0
!
router ospf 1
 network 0.0.0.0 255.255.255.255 area 0
</code></pre>

<h2>Server configuration and OpenStack installation</h2>

<p>Refer to my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous blogpost</a> for instructions on how to install OpenStack and follow the first 5 steps from &ldquo;Installing CentOS and Openstack&rdquo; section. Before doing the final step, we need to configure our VMs' new interfaces:</p>

<ul>
<li>Remove any IP configuration from <strong>eth1</strong> interface to make it look like this:</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/ifcfg-eth1">TYPE=Ethernet
BOOTPROTO=none
DEVICE=eth1
ONBOOT=yes
</code></pre>

<ul>
<li>Configure <strong>Tenant network</strong>:</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/ifcfg-eth1.100">VLAN=yes
DEVICE=eth1.100
BOOTPROTO=none
IPADDR=10.0.X.10
PREFIX=24
ONBOOT=yes
</code></pre>

<ul>
<li>Setup a <strong>static route</strong> to all other leaf nodes:</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/route-eth1.100">10.0.0.0/8 via 10.0.X.1
</code></pre>

<ul>
<li>On Control node setup <strong>External network</strong> interface</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/route-eth1.300">DEVICE=eth1.300
IPADDR=192.168.247.100
PREFIX=24
ONBOOT=yes
BOOTPROTO=none
VLAN=yes
</code></pre>

<p>Now we&rsquo;re ready to kick off OpenStack installation. This can be done with a single command that needs to be executed on the Control node. Note that <code>eth1.100</code> interface is spelled as <code>eth1_100</code> in the last line.</p>

<pre><code class="bash Controller">packstack --allinone \
    --os-cinder-install=n \
    --os-ceilometer-install=n \
    --os-trove-install=n \
    --os-ironic-install=n \
    --nagios-install=n \
    --os-swift-install=n \
    --os-gnocchi-install=n \
    --os-aodh-install=n \
    --os-neutron-ovs-bridge-mappings=extnet:br-ex \
    --os-neutron-ovs-bridge-interfaces=br-ex:eth1.300 \
    --os-neutron-ml2-type-drivers=vxlan,flat \
    --provision-demo=n \
    --os-compute-hosts=192.168.91.10,192.168.91.11,192.168.91.12 \
    --os-neutron-ovs-tunnel-if=eth1_100
</code></pre>

<h2>Creating a virtual network for a pair of VMs</h2>

<p>Once again, follow all steps from &ldquo;Configuring Openstack networking&rdquo; section of my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous blogpost</a>. Only this time when setting up a public subnet, update the subnet details to match our current environment:</p>

<pre><code class="bash Step 3 - Creating a public subnet">  neutron subnet-create --name public_subnet \
    --enable_dhcp=False \
    --allocation-pool=start=192.168.247.90,end=192.168.247.126 \
    --gateway=192.168.247.1 external_network 192.168.247.0/24
</code></pre>

<h2>Nova-scheduler and setting up host aggregates</h2>

<p>OpenStack&rsquo;s Nova project is responsible for managing virtual machines. Nova controller views all available compute nodes as a single pool of resources. When a new VM is to be instantiated, a special process called nova-scheduler examines all available compute nodes and selects the &ldquo;best&rdquo; one based on a special algorithm, which normally takes into account amount of RAM, CPU and other host capabilities.</p>

<p>To make our host selection a little bit more deterministic, we can define a group of compute servers via <strong>host aggregates</strong>, which will be used by nova-scheduler in its selection algorithm. Normally it could include all servers in a single rack or a row of racks. In our case we&rsquo;ll setup two host aggregates each with a single compute host. This way we&rsquo;ll be able to select exactly which compute host to use when instantiating a new virtual machine.</p>

<p>To setup it up, from Horizon&rsquo;s dashboard navigate to Admin -> System and create two host aggregates <strong>comp-1</strong> and <strong>comp-2</strong>, each including a single compute host.</p>

<h2>Creating workloads and final testing</h2>

<p>Using a process described in &ldquo;Spinning up a VM&rdquo; section of my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous blogpost</a>, create a couple of virtual machines assigning them to different host aggregates created earlier.</p>

<h2>Security and Remote access</h2>

<p>To access these virtual machines we need to give them a <a href="https://www.rdoproject.org/networking/floating-ip-range/">floating</a> ip address from the External subnet range. To do that navigate to Project -> Compute -> Instances and select <strong>Associate Floating IP</strong> from the Actions drop-down menu.</p>

<p>The final steps is to allow remote SSH access. Each new VM inherits ACLs from a default security group. So the easiest way to allow SSH is to go to Project -> Compute -> Access &amp; Security and add a rule to allow inbound SSH connections for the default security group.</p>

<h2>Verification</h2>

<p>At this stage you should be able to SSH into the floating IP addresses assigned to the two new VMs using the default credentials. Feel free to poke around and explore Horizon&rsquo;s interface a bit more. For example, try setting up an SSH key pair and re-build our two VMs to allow passwordless SSH access.</p>

<h2>What to expect next</h2>

<p>In the next post we&rsquo;ll explore some of the basic concepts of OpenStack&rsquo;s SDN. We&rsquo;ll peak inside the internal implementation of virtual networks and see what are some of their limitations and drawbacks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Openstack on UNetlab]]></title>
    <link href="http://networkop.github.io/blog/2016/04/04/openstack-unl/"/>
    <updated>2016-04-04T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/04/04/openstack-unl</id>
    <content type="html"><![CDATA[<p>In this post I&rsquo;m going to show how to get a running instance of Openstack inside a UNetLab virtual machine.</p>

<!--more-->


<p><img class="center" src="/images/unl-os.png"></p>

<h2>What the hell am I trying to do?</h2>

<p>I admit that running Openstack on anything other than baremetal is nonsense. So why would anyone want to run it with two layers of virtualisation underneath? My goal is to explore some of the new SDN/NFV technologies without leaving the confines on my home area network and/or racking up a triple-digit electricity bill. I also wanted to be able to swap underlay networks without spending hours trying to plumb together virtualized switches and servers from multiple vendors. That&rsquo;s why I&rsquo;ve decided to use UNetLab VM as a host for my Openstack lab. This would allow me to easily assemble any type of underlay, WAN or DCI network and with hardware virtualisation support I can afford to run Openstack double-nested inside Workstation and Qemu on my dual-core i7 without too much of a performance penalty. After all, <a href="https://www.ravellosystems.com/technology/hvx">some companies</a> even managed to turn similar things into a commercial product.</p>

<p>My interest in Openstack is strictly limited by networking, that&rsquo;s why a lot of the things you&rsquo;ll see in this and following posts will not be applicable to a real-life production environment. However, as far as networking is concerned, I&rsquo;ll try to stick as close to the official Openstack <a href="http://docs.openstack.org/openstack-ops/content/example_architecture.html">network design</a> as possible. I&rsquo;ll be using <a href="https://www.rdoproject.org">RDO</a> to deploy Openstack. The specific method will be Packstack which is a collection of Puppet modules used to deploy Openstack components.</p>

<p>Why have I not went the OpenDaylight/Mininet way if I wanted to play with SDN/NFV? Because I wanted something more realistic to play with, that wouldn&rsquo;t feel like vendor&rsquo;s powerpoint presentation. Plus there&rsquo;s plenty of resources on the &lsquo;net about it anyway.</p>

<p>So, without further ado, let&rsquo;s get cracking.</p>

<h2>Setting the scene</h2>

<p>On my Windows 8 laptop I&rsquo;ve got a UNL virtual machine running inside a VMWare Workstation.. I&rsquo;ve <a href="http://www.unetlab.com/download/">downloaded</a> and <a href="http://www.unetlab.com/2014/11/upgrade-unetlab-installation/">upgraded</a> a pre-built UNL VM image. I&rsquo;ve also downloaded a copy of the <a href="http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.iso">Centos 7 minimal ISO image</a> and copied it over to my UNL VM&rsquo;s home directory.</p>

<p>For network access I&rsquo;ll be using VMWware Workstation&rsquo;s NAT interface. It&rsquo;s currently configured with <code>192.168.91.0/24</code> subnet with DHCP range of <code>.128-.254</code>. Therefore I&rsquo;ll be using <code>.10-.126</code> to allocate IPs to my Openstack servers.</p>

<h2>Creating a custom node type in UNL</h2>

<p>Every node type inside UNL has its own unique settings. Some settings, like amount of RAM, CPU or number of network interfaces, can be changed during node instantiation, while some of them remain &ldquo;baked in&rdquo;. Say, for example, the default &ldquo;Linux&rdquo; template creates nodes with default <strong>Qemu Virtual CPU</strong> which doesn&rsquo;t support the hardware virtualisation (<strong>VT-X/AMD-V</strong>) <a href="http://docs.openstack.org/liberty/config-reference/content/kvm.html">required</a> by Openstack. In order to change that you can either edit the existing node template or follow these steps to create a new one:</p>

<ol>
<li><p>Add Openstack node definition to initialization file <code>/opt/unetlab/html/includes/init.php</code>.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='php'><span class='line'> <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">linux</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;</span>                 <span class="o">=&gt;</span>      <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">Linux</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;,</span>
</span><span class='line'> <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">openstack</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;</span>             <span class="o">=&gt;</span>      <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">Openstack</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;,</span>
</span><span class='line'> <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">mikrotik</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;</span>              <span class="o">=&gt;</span>      <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">MikroTik</span> <span class="nx">RouterOS</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;,</span>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a new Openstack node template based on existing linux node template.</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>cp /opt/unetlab/html/templates/linux.php /opt/unetlab/html/templates/openstack.php
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Edit the template file replacing all occurences of &lsquo;Linux&rsquo; with &lsquo;Openstack&rsquo;</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/Linux/Openstack/g<span class="p">;</span> s/linux/openstack/g<span class="err">&#39;</span> /opt/unetlab/html/templates/openstack.php
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Edit the template file to double the RAM and CPU and pass all host&rsquo;s CPU instructions to Openstack nodes</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/2048/4096/<span class="p">;</span> s/<span class="o">(</span>cpu.*<span class="o">)</span> <span class="o">=</span> 1/<span class="se">\1</span> <span class="o">=</span> 2/<span class="p">;</span> s/<span class="o">(</span><span class="nv">order</span><span class="o">=)</span>dc/<span class="se">\1</span>cd -cpu host/<span class="p">&amp;</span>lsquo<span class="p">;</span> /opt/unetlab/html/templates/openstack.php
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<p>At this point you should be able to navigate to UNL&rsquo;s web interface and find a new node of type Openstack. However you won&rsquo;t be able to create it until you have at least one image, which is what we&rsquo;re going to build next.</p>

<h2>Building a Linux VM inside UNetLab</h2>

<p>Now we need to create a CentOS image inside a UNL. One way to do it is build it inside a VMWare Workstation, copy it to UNL and convert the <strong>.vmdk</strong> to <strong>.qcow2</strong>. However, when I tried doing this I ran into a problem with CentOS not finding the correct disk partitions during bootup. The workaround was to boot into rescue mode and rebuild the initramfs. For those feeling adventurous, I would recommend checking out the following links [<a href="https://wiki.centos.org/TipsAndTricks/CreateNewInitrd">1</a>, <a href="http://advancelinux.blogspot.com.au/2013/06/how-to-rebuild-initrd-or-initramfs-in.html">2</a>, <a href="http://forums.fedoraforum.org/showthread.php?t=288020">3</a>] before trying this option.<br/>
The other option is to build CentOS inside UNL from scratch. This is how you can do it:</p>

<ol>
<li><p>Create a new directory for Openstack image</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>mkdir -p /opt/unetlab/addons/qemu/openstack-1
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a link to CentOS ISO boot image from our new directory</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>ln -s ~/CentOS-7-x86_64-Minimal-1511.iso /opt/unetlab/addons/qemu/openstack-1/cdrom.iso
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a blank 6Gb disk image</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>/opt/qemu/bin/qemu-img create -f qcow2 -o <span class="nv">preallocation</span><span class="o">=</span>metadata /opt/unetlab/addons/qemu/openstack-1/virtioa.qcow2 6G
</span></code></pre></td></tr></table></div></figure></p>

<p>If you want to create <strong>snapshots</strong> at any stage of the process you&rsquo;d need to use a copy of this file under /opt/unetlab/tmp/pod_id/lab_uuid/node_id/ directory</p></li>
</ol>


<p>Now you should be able to successfully create an Openstack node and connect it to Internet. Create a new network that would have Internet connectivity (in my case it&rsquo;s <strong>pnet0</strong>) and connect it to Openstack&rsquo;s <strong>eth0</strong>.  At this stage we have everything ready to start installing Openstack, but before we move on let me take a quick detour to tell you about my ordeals with VNC integration.</p>

<h2>Optional: Integrating TightVNC with UNL</h2>

<p>For some unknown reason UltraVNC does not work well on my laptop. My sessions would often crash or start minimised with the only option to close the window. That&rsquo;s not the only thing not working properly on my laptop thanks to the corporate policies with half of the sh*t locked down for <em>security</em> reasons.<br/>
So instead of mucking around with <strong>Ultra</strong> I decided to give me old pal <strong>Tight</strong>VNC a go. The setup process is very similar to the <a href="http://www.unetlab.com/2015/03/url-telnet-ssh-vnc-integration-on-windows/">official VNC integration guide</a> with the following exceptions:</p>

<ol>
<li><p>The wrapper file simply strips the leading &lsquo;vnc://&rsquo; and trailing &lsquo;/&rsquo; off the passed argument</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='winbatch'><span class='line'><span class="k">SET</span> <span class="nv">arg</span><span class="o">=</span><span class="nv">%1</span>
</span><span class='line'><span class="k">SET</span> <span class="nv">arg</span><span class="o">=</span><span class="nv">%arg:~6%</span>
</span><span class='line'><span class="k">SET</span> <span class="nv">arg</span><span class="o">=</span><span class="nv">%arg:~0</span><span class="p">,</span><span class="m">-1</span>%
</span><span class='line'>start &amp;ldquo;&amp;rdquo; &amp;ldquo;c:\Program Files\TightVNC\tvnviewer.exe&amp;rdquo; <span class="nv">%arg%</span>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>The registry entry now points to the TightVNC wrapper</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='registry'><span class='line'><span class="k">[</span><span class="nb">HKEY_CLASSES_ROOT</span><span class="k">\vnc\shell\open\command]</span>
</span><span class='line'><span class="na">@</span><span class="o">=</span><span class="s">&amp;ldquo;\&quot;c:\Program Files\TightVNC\wrapper.bat\&amp;rdquo; %1&quot;</span>
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<h2>Installing CentOS and Openstack</h2>

<p>Finally, we&rsquo;ve got all our ducks lined up in a row and we&rsquo;re ready to shoot. Fire up the Openstack node inside UNL and click on it to open a vnc session. Proceed to install CentOS with default options. You need to confirm which <strong>hard disk</strong> to use and setup the <strong>hostname</strong> and the <strong>root password</strong> during installation process.
As I mentioned earlier, we&rsquo;ll be using RDO&rsquo;s Packstack to deploy all the necessary Openstack components. The whole installation process will be quite simple and can be found on the RDO&rsquo;s <a href="https://www.rdoproject.org/install/quickstart/">quickstart page</a>. Here is my slightly modified version of installation process:</p>

<ol>
<li><p>Disable Network Manager and SELinux.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>service NetworkManager stop
</span><span class='line'> <span class="nv">$ </span>systemctl disable NetworkManager.service
</span><span class='line'> <span class="nv">$ </span>setenforce 0
</span><span class='line'> <span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/enforcing/permissive/<span class="p">&amp;</span>lsquo<span class="p">;</span> /etc/selinux/config
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Configure static IP address <code>192.168.91.10</code> on the network interface. <br/>
 Assuming your interface name is <code>eth0</code> make sure you /etc/sysconfig/network-scripts/ifcfg-eth0 looks something like this:</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">TYPE</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>Ethernet<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">BOOTPROTO</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>static<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">IPADDR</span><span class="o">=</span>192.168.91.10
</span><span class='line'> <span class="nv">PREFIX</span><span class="o">=</span>24
</span><span class='line'> <span class="nv">GATEWAY</span><span class="o">=</span>192.168.91.2
</span><span class='line'> <span class="nv">DNS1</span><span class="o">=</span>192.168.91.2
</span><span class='line'> <span class="nv">NAME</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>eth0<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">DEVICE</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>eth0<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">ONBOOT</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>yes<span class="p">&amp;</span>rdquo<span class="p">;</span>&lt;br/&gt;
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Issue a <code>service network restart</code> and reconnect to the new static IP address. Make sure that you still have access to Internet after making this change.</p></li>
<li><p>Setup RDO repositories</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>sudo yum install -y &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://rdoproject.org/repos/rdo-release.rpm&quot;</span>&gt;https://rdoproject.org/repos/rdo-release.rpm&lt;/a&gt;
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Update your current packages</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>sudo yum update -y
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Install Packstack</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>sudo yum install -y openstack-packstack
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> That&rsquo;s where it&rsquo;d make sense to take a snapshot with <code>qemu-img snapshot -c pre-install virtioa.qcow2</code> command</p></li>
<li><p>Deploy a single-node Openstack environment</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>packstack --allinone <span class="se">\</span>
</span><span class='line'> --os-cinder-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-ceilometer-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-trove-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-ironic-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --nagios-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-swift-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-neutron-ovs-bridge-mappings<span class="o">=</span>extnet:br-ex <span class="se">\</span>
</span><span class='line'> --os-neutron-ovs-bridge-interfaces<span class="o">=</span>br-ex:eth0 <span class="se">\</span>
</span><span class='line'> --os-neutron-ml2-type-drivers<span class="o">=</span>vxlan,flat <span class="se">\</span>
</span><span class='line'> --provision-demo<span class="o">=</span>n
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Here we&rsquo;re overriding some of the default Packstack options. We&rsquo;re not installing some of the components we&rsquo;re not going to use and setting up a name (<strong>extnet</strong>) for our external physical segment, which we&rsquo;ll use in the next section.</p></li>
</ol>


<p>At the end of these 4 steps you should be able to navigate to Horizon (Openstack&rsquo;s dashboard) by typing <code>http://192.168.91.10</code> in your browser. You can find login credentials in the <code>~/keystonerc_admin</code> file.</p>

<h2>Configuring Openstack networking</h2>

<p>At this stage we need to setup virtual networking infrastructure inside Openstack. This will be almost the same as described in RDO&rsquo;s external network <a href="https://www.rdoproject.org/networking/neutron-with-existing-external-network/">setup guide</a>. The only exceptions will be the <a href="https://www.rdoproject.org/networking/difference-between-floating-ip-and-private-ip/">floating IP range</a>, which will match our existing environment, and the fact that we&rsquo;re no going to setup any additional tenants yet. This is how our topology will look like:</p>

<p><img class="center" src="/images/os-net-1.png"></p>

<ol>
<li><p>Switch to Openstack&rsquo;s <code>admin</code> user
 <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span><span class="nb">source</span> ~/keystonerc_admin
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create external network
 <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron net-create external_network --provider:network_type flat <span class="se">\</span>
</span><span class='line'> --provider:physical_network extnet  <span class="se">\</span>
</span><span class='line'> --router:external <span class="se">\</span>
</span><span class='line'> --shared
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a public subnet</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron subnet-create --name public_subnet <span class="se">\</span>
</span><span class='line'> --enable_dhcp<span class="o">=</span>False <span class="se">\</span>
</span><span class='line'> --allocation-pool<span class="o">=</span><span class="nv">start</span><span class="o">=</span>192.168.91.90,end<span class="o">=</span>192.168.91.126 <span class="se">\</span>
</span><span class='line'> --gateway<span class="o">=</span>192.168.91.2 external_network 192.168.91.0/24
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Default gateway is VMware&rsquo;s NAT IP address</p></li>
<li><p>Create a private network and subnet</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron net-create private_network
</span><span class='line'> neutron subnet-create --name private_subnet private_network 10.0.0.0/24 <span class="se">\</span>
</span><span class='line'> --dns-nameserver 8.8.8.8
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p>  This network is not routable outside of Openstack and is used for inter-VM communication</p></li>
<li><p>Create a virtual router and attach it to both networks</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron router-create router
</span><span class='line'> neutron router-gateway-set router external_network
</span><span class='line'> neutron router-interface-add router private_subnet
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<p>Make sure to check out the visualisation of our newly created network topology in Horizon, it&rsquo;s amazing.</p>

<h2>Spinning up a VM</h2>

<p>There&rsquo;s no point in installing Openstack just for the sake of it. Our final step would be to create a working virtual machine that would be able to connect to Internet.</p>

<ol>
<li><p>Download a test linux image</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> curl &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img&quot;</span>&gt;http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img&lt;/a&gt; <span class="p">|</span> glance <span class="se">\</span>
</span><span class='line'> image-create --name<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>cirros image<span class="p">&amp;</span>rsquo<span class="p">;</span> <span class="se">\</span>
</span><span class='line'> --visibility<span class="o">=</span>public <span class="se">\</span>
</span><span class='line'> --container-format<span class="o">=</span>bare <span class="se">\</span>
</span><span class='line'> --disk-format<span class="o">=</span>qcow2
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>From Horizon&rsquo;s home page navigate to Project -> Compute -> Images.</p></li>
<li><p>Click on <code>Launch Instance</code> and give the new VM a name.</p></li>
<li><p>Make sure it&rsquo;s attached to <code>private_network</code> under the Networking tab.</p></li>
<li><p>Less then a minute later the status should change to <code>Active</code> and you can navigate to VM&rsquo;s console by clicking on its name and going to <code>Console</code> tab.</p></li>
<li><p>Login using the default credentials (<strong>cirros/cubswin:)</strong>) and verify Internet access by pinging google.com.</p></li>
</ol>


<p>Congratulations, we have successfully created a VM running inside a KVM inside a KVM inside a VMWare Workstation inside Windows!</p>

<h2>What to expect next</h2>

<p>Unlike my other post series, I don&rsquo;t have a clear goal at this stage so I guess I&rsquo;ll continue playing around with different underlays for multi-node Openstack and then move on to various SDN solutions available like OpenDayLight and OpenContrail. Unless I lose interest half way through, which happened in the past. But until that happens, stay tuned for more.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[REST for Network Engineers Part 3 - Advanced Operations With UnetLab]]></title>
    <link href="http://networkop.github.io/blog/2016/01/17/rest-unl-advanced/"/>
    <updated>2016-01-17T00:00:00-08:00</updated>
    <id>http://networkop.github.io/blog/2016/01/17/rest-unl-advanced</id>
    <content type="html"><![CDATA[<p>In this post we&rsquo;ll look at how to create arbitrary topologies and push configuration to Nodes in UNetlab via REST SDK. We&rsquo;ll conclude by extending our sample application to create and configure a 3-node topology and enable full connectivity between all nodes.</p>

<!--more-->


<h2>Extracting Node&rsquo;s UUID</h2>

<p>In the <a href="http://networkop.github.io/blog/2016/01/06/rest-basic-operations/">previous post</a> we have learned how to create a Node. To perform further actions on it we need to know it&rsquo;s UUID. According to HTTP specification <code>201 - Created</code> response SHOULD return a <code>Location</code> header with resource URI, which would contain resource UUID. However, UNetLab&rsquo;s implementation does not return a Location header so we need to extract that information ourselves. To do that we&rsquo;ll use the previously defined <code>.get_nodes()</code> method which returns all attributes of all configured Nodes in the following format:</p>

<p><img class="centre" src="/images/rest-unl-get-nodes.png" title="REST SDK GET ALL NODES" ></p>

<p>The best place to extract UUID would be when Node is being created. After the <code>Create</code> request has been sent to a server we&rsquo;ll send another <code>Read</code> request and extract all attributes of a Node based on its name.</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">class UnlNode(object):

    def __init__(self, lab, device):
        ...
        self.node = self._get_node()
        self.id = self.node['id']
        self.url = self.node['url']

    def _get_node(self):
        nodes = self.lab.get_nodes().json()['data']
        return get_obj_by_name(nodes, self.device.name)
</code></pre>

<p> To extract data from the payload we need to call <code>.json()</code> on the returned HTTP response and look for the <code>data</code> key inside that JSON object. The returned value will contain all attributes including the UUID and access URL which we&rsquo;ll use later. To help us find a Node object matching a name we&rsquo;ll use a helper function defined below:</p>

<pre><code class="python /rest-blog-unl-client/restunl/helper.py">def get_obj_by_name(objects, name):
    for obj_id in objects:
        if objects[obj_id]["name"] == name:
            return objects[obj_id]
    return None
</code></pre>

<p>Needless to say that we MUST have unique names for all nodes otherwise it won&rsquo;t be possible to do the matching. It&rsquo;s quite a safe assumption to make in most cases however no built-in error checking will be performed by the REST SDK to prevent you from doing it.</p>

<h2>UnlNet implementation</h2>

<p>Before we start connecting Nodes together we need to create a Network. As per the <a href="http://networkop.github.io/blog/2016/01/06/rest-basic-operations/">design</a>, UnlNet will be a class holding a pointer to the UnlLab object which created it. The structure of the class will be very similar to UnlNode.</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">REST_SCHEMA = { 
                ... ,
                'create_net': '/labs/{lab_name}/networks',
                'get_nets': '/labs/{lab_name}/networks'
            }

class UnlLab(object):
    ...
    def create_net(self, name):
        return UnlNet(self, name)

    def get_nets(self):
        api_call = REST_SCHEMA['get_nets']
        api_url = api_call.format(api_call, lab_name=append_unl(self.name))
        resp = self.unl.get_object(api_url)
        return resp


class UnlNet(object):

    def __init__(self, lab, name):
        api_call = REST_SCHEMA['create_net']
        self.unl, self.lab, self.name = lab.unl, lab, name
        payload = {'type': 'bridge', 'name': self.name}
        api_url = api_call.format(api_call, lab_name=append_unl(self.lab.name))
        self.resp = self.unl.add_object(api_url, data=payload)
        self.net = self._get_net()
        self.id = self.net['id']

    def _get_net(self):
        nets = self.lab.get_nets().json()['data']
        return get_obj_by_name(nets, self.name)
</code></pre>

<h2>Connecting Nodes to a network</h2>

<p>Official <a href="http://www.unetlab.com/2015/09/using-unetlab-apis/">Unetlab API guide</a> is still under development and doesn&rsquo;t specify how to connect a Node to a network. If you want to find out the syntax for this or any other unspecified API call you can always try that in a Web GUI while capturing traffic with Wireshark. That is how I&rsquo;ve discovered that to connect a Node to a network we need to send an Update request with payload containing mapping between Node&rsquo;s interface ID and Network ID.</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">REST_SCHEMA = { 
                ... ,
                'connect_interface': '/labs/{lab_name}/nodes/{node_id}/interfaces'
            }

class UnlNode(object):
    ...

    def connect_interface(self, intf_name, net):
        api_call = REST_SCHEMA['connect_interface']
        api_url = api_call.format(api_call, lab_name=append_unl(self.lab.name), node_id=self.id)
        payload = {get_intf_id(intf_name): net.id}
        resp = self.unl.update_object(api_url, data=payload)
        return resp
</code></pre>

<p>The ID of an interface &ldquo;Ethernet x/y‚Äù of an IOU device can be easily calculated based on the formula <code>id = x + (y * 16)</code> as described <a href="http://evilrouters.net/2011/01/09/creating-a-netmap-file-for-iou/">here</a>. This will be accomplished with yet another helper function:</p>

<pre><code class="python /rest-blog-unl-client/restunl/helper.py">def get_intf_id(intf_name):
    x, y = re.findall('\d+', intf_name)
    return int(x) + (int(y) * 16)
</code></pre>

<h2>Connecting Nodes to each other</h2>

<p>To create multi-access topologies we would need to maintain an internal mapping between Node&rsquo;s interface and the network it&rsquo;s attached to. However, if we assume that all links are point-to-point, we can not only simplify our implementation but also enable REST client to ignore the notion of a network all together.  We&rsquo;ll simply assume that when device A connects to B our implementation will create a network called <code>A_B</code> in the background and connect both devices to it. This method will perform two separate REST calls and thus will return both responses in a tuple:</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">class UnlNode(object):
    ...

def connect_node(self, local_intf, other_node, other_intf):
    net = self.lab.create_net(name='_'.join([self.device.name, other_node.device.name]))
    resp1 = self.connect_interface(local_intf, net)
    resp2 = other_node.connect_interface(other_intf, net)
    return resp1, resp2
</code></pre>

<p>Assuming all links are point-to-point certainly decreases visibility of created networks and we would not be able to perform selective changes on them in the future. However it is a safe assumption to make for 99% of the networks that I&rsquo;m dealing with.</p>

<h2>Node Start, Stop and Delete</h2>

<p>These simple actions can easily be coded using TDD. I will omit the actual implementation and simply provide unit tests for readers to exercise their TDD skills again.</p>

<pre><code class="python /rest-blog-unl-client/tests/test_unl.py">class AdvancedUnlNodeTest(UnlTests):

    def setUp(self):
        super(AdvancedUnlNodeTest, self).setUp()
        self.device_one = Router('R1')
        self.device_two = Router('R2')
        self.lab = self.unl.create_lab(LAB_NAME)
        self.node_one = self.lab.create_node(self.device_one)
        self.node_two = self.lab.create_node(self.device_two)

    def tearDown(self):
        self.unl.delete_lab(LAB_NAME)
        super(AdvancedUnlNodeTest, self).tearDown()

    def test_start_nodes(self):
        self.lab.stop_all_nodes()
        resp = self.lab.start_all_nodes()
        self.assertEqual(200, resp.status_code)

    def test_stop_nodes(self):
        self.lab.start_all_nodes()
        resp = self.lab.stop_all_nodes()
        self.assertEqual(200, resp.status_code)

    def test_delete_node(self):
        resp = self.lab.delete_node(self.node_one.id)
        self.assertEqual(200, resp.status_code)

    def test_del_all_nodes(self):
        self.lab.del_all_nodes()
        resp = self.lab.get_nodes()
        self.assertEqual(0, len(resp_2.json()['data']))

    def test_lab_cleanup(self):
        resp_1 = self.lab.stop_all_nodes()
        self.lab.del_all_nodes()
        resp_2 = self.lab.get_nodes()
        self.assertEqual(200, resp_1.status_code)
        self.assertEqual(0, len(resp_2.json()['data']))
</code></pre>

<p>The final, <code>lab_cleanup()</code> method is simply a shortcut to <code>stop_nodes()</code> followed by <code>del_all_nodes()</code>.<br/>
As always, link to full code is available at the end of this post.</p>

<h2>Pushing configuration to Nodes</h2>

<p>At this point of time UnetLab does not support configuration import so we&rsquo;re stuck with the only access method available - telnet. To push configuration into the Node we&rsquo;re gonna have to establish a telnet session to Node&rsquo;s URI (which we&rsquo;ve extracted earlier) and write all configuration into that session.</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">class UnlNode(object):
    ...

    def configure(self, text):
        return self.device.send_config(wrap_conf(text))
</code></pre>

<p>Another helper function <code>wrap_conf()</code> prepends <code>enable</code> and appends <code>end</code> to make configuration suitable for pasting into the new IOU device.</p>

<pre><code class="python /rest-blog-unl-client/restunl/device.py">class Router(Device):
    ...

    def send_config(self, config):
        session = telnetlib.Telnet(self.url_ip, self.url_port)
        send_and_wait(session, '\r\n')
        result = send_and_wait(session, config)
        session.close()
        return result
</code></pre>

<p>The biggest problem is that Nodes, when started, take some time to boot before we can access the CLI prompt. To overcome that I had to implement a dirty hack in a form of <code>send_and_wait()</code> helper function that simulates pressing the <code>Enter</code> button every 0.1 second until it sees a CLI prompt (either <code>&gt;</code> or <code>#</code>).</p>

<pre><code class="python /rest-blog-unl-client/restunl/helper.py">
def send_and_wait(session, text):
        session.read_very_eager()
        result = ''
        session.write(text)
        while not any(stop_char in result[-3:] for stop_char in ['&gt;', '#']):
            session.write('\r\n')
            result += session.read_very_eager()
            time.sleep(0.1)
        return result
</code></pre>

<p>Let&rsquo;s hope that UNL team will implement config import soon so that we can get rid of this kludgy workaround.</p>

<h2>Extending our sample app</h2>

<p>At this stage we&rsquo;ve got all the code to finish our sample app. The goal is to create and configure the following 3-node topology:</p>

<p><img class="centre" src="/images/rest-sample-app.png" title="REST SDK SAMPLE TOPO" ></p>

<p>We&rsquo;ll assume that all configs will be stored as text files under the <code>./config</code> directory and will have device names as their filename. A helper function <code>read_file</code> will read the contents of a configuration text file into a Python string.</p>

<pre><code class="python">
TOPOLOGY = {('R1', 'Ethernet0/0'): ('R2', 'Ethernet0/0'),
            ('R2', 'Ethernet0/1'): ('R3', 'Ethernet0/0'),
            ('R1', 'Ethernet0/1'): ('R3', 'Ethernet0/1')}

def app_1():
    ...
    try:
        # Creating topology in UnetLab
        nodes = dict()
        for (a_name, a_intf), (b_name, b_intf) in TOPOLOGY.iteritems():
            # Create a mapping between a Node's name and an object
            if not a_name in nodes:
                nodes[a_name] = lab.create_node(Router(a_name))
                print("*** NODE {} CREATED".format(a_name))
            if not b_name in nodes:
                nodes[b_name] = lab.create_node(Router(b_name))
                print("*** NODE {} CREATED".format(b_name))
            # Extract Node objects using their names and connect them
            node_a = nodes[a_name]
            node_b = nodes[b_name]
            node_a.connect_node(a_intf, node_b, b_intf)
            print("*** NODES {0} and {1} ARE CONNECTED".format(a_name, b_name))
        print("*** TOPOLOGY IS BUILT")
        lab.start_all_nodes()
        print("*** NODES STARTED")
        # Reading and pushing configuration
        for node_name in nodes:
            conf = read_file('..\\config\\{}.txt'.format(node_name))
            nodes[node_name].configure(conf)
            print("*** NODE {} CONFIGURED".format(node_name))
        raw_input('PRESS ANY KEY TO STOP THE LAB')
    except Exception as e:
        print("*** APP FAILED : {}".format(e))
    finally:
        print("*** CLEANING UP THE LAB")
        lab.cleanup()
        unl.delete_lab(LAB_NAME)
</code></pre>

<p>When you run this app for the first time, the lab with 3 nodes will be spun up and configured. When you get to the <code>PRESS ANY KEY</code> prompt you can login into Web GUI and navigate to lab <code>test_1</code> and validate that all configs have been pushed and devices can ping each other&rsquo;s loopbacks.</p>

<h2>Source code</h2>

<p>All code from this post can be found in my <a href="https://github.com/networkop/rest-blog-unl-client/tree/2e847b8a809a1c9c4c0962b61c1c72325a405090">public repository on Github</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[REST for Network Engineers Part 2 - Basic Operations With UnetLab]]></title>
    <link href="http://networkop.github.io/blog/2016/01/06/rest-basic-operations/"/>
    <updated>2016-01-06T00:00:00-08:00</updated>
    <id>http://networkop.github.io/blog/2016/01/06/rest-basic-operations</id>
    <content type="html"><![CDATA[<p>In this post I&rsquo;ll show how to build REST SDK to authenticate, create labs and nodes in <a href="http://www.unetlab.com/">UnetLab</a>. I&rsquo;ll briefly cover the difference between composition and inheritance design patterns and demonstrate how to use test-driven development.</p>

<!--more-->


<h2>REST SDK Design</h2>

<p>As it is with networks, design is a very crucial part of programming. I won&rsquo;t pretend to be an expert in that field and merely present the way I&rsquo;ve built REST SDK. Fortunately, a lot of design will mimic the objects and their relationship on the server side. I&rsquo;ll slightly enhance it to improve code re-use and portability. Here are the basic objects:</p>

<ol>
<li>RestServer - implements basic application-agnostic HTTP CRUD logic</li>
<li>UnlServer - an extension of a RestServer with specific authentication method (cookie-based) and several additional methods</li>
<li>Device - an instance of a network device with specific attributes like type, image name, number of CPUs</li>
<li>UnlLab - a lab instance existing inside a UnlServer</li>
<li>UnlNode - a node instance existing inside a UnlLab</li>
<li>UnlNet - a network instance also existing inside a UnlLab object</li>
</ol>


<p>All these objects and their relationships are depicted on the following simplified <abbr title="Unified Modeling Language">UML</abbr> diagram. If you&rsquo;re interested in what different connections mean you can read <a href="http://www.codeproject.com/Articles/618/OOP-and-UML">this guide</a>.</p>

<p><img class="centre" src="/images/rest-oop-design.png" title="REST SDK UML Diagram" ></p>

<p>Here I&rsquo;ve used inheritance to <em>extend</em> RestServer functionality to make a UnlServer. This makes sense because UnlServer object will re-use a lot of the methods from the RestServer. I could have combined them in a single object but I&rsquo;ve decided to split the application-agnostic bit into a separate component to allow it to be re-used by other RESTful clients in the future.</p>

<p>The other objects are aggregated and interact through code composition, where Lab holds a pointer to the UnlServer where it was created, Nodes and Nets point to the Lab in which they live. Composition creates loose coupling between objects, while still allowing method delegation and code re-use.</p>

<p>For additional information about Composition vs Inheritance you can go <a href="http://learnpythonthehardway.org/book/ex44.html">here</a>, <a href="http://lgiordani.com/blog/2014/08/20/python-3-oop-part-3-delegation-composition-and-inheritance/">here</a> or <a href="http://python-textbok.readthedocs.org/en/latest/Object_Oriented_Programming.html#avoiding-inheritance">here</a>.</p>

<h2>REST SDK Implementation</h2>

<blockquote><p>Throughout this post I&rsquo;ll be omitting a lot of the non-important code. For full working code refer to the link at the end of this post.</p></blockquote>

<h3>RestServer implementation</h3>

<p>When RestServer object is created, <code>__init__()</code> function takes the server IP address and constructs a <code>base_url</code>, a common prefix for all API calls. The 4 CRUD actions are encoded into names of the methods implementing them, for example to send an Update one would need to call <code>.update_object()</code>. This convention will make the implementation of UnlServer a lot more readable. Each of the 4 CRUD methods call <code>_send_request()</code> with correct HTTP verb preset (the leading underscore means that this method is private and should only be called from within the RestServer class).</p>

<pre><code class="python /rest-blog-unl-client/restunl/server.py">class RestServer(object):

    def __init__(self, address):
        self.cookies = None
        self.base_url = '/'.join(['http:/', address, 'api'])

    def _send_request(self, method, path, data=None):
        response = None
        url = self.base_url + path
        try:
            response = requests.request(method, url,  json=data, cookies=self.cookies)
        except requests.exceptions.RequestException as e:
            print('*** Error calling %s: %s', url, e.message)
        return response

    def get_object(self, api_call, data=None):
        return self._send_request('GET', api_call, data)

    def add_object(self, api_call, data=None):
        return self._send_request('POST', api_call, data)

    def update_object(self, api_call, data=None):
        return self._send_request('PUT', api_call, data)

    def del_object(self, api_call, data=None):
        return self._send_request('DELETE', api_call, data)

    def set_cookies(self, cookie):
        self.cookies = cookie
</code></pre>

<p>At this stage RestServer does very simple exception and no HTTP response error handling. I&rsquo;ll show how to extend it to do authentication error handling in the future posts.</p>

<h3>UnlServer implementation</h3>

<p>At the very top of the <code>unetlab.py</code> file we have a <code>REST_SCHEMA</code> global variable providing mapping between actions and their respective <a href="http://www.unetlab.com/2015/09/using-unetlab-apis/">API calls</a>. This improves code readability (at least to me) and makes future upgrades to API easier to implement.<br/>
UnlServer class is extending the functionality of a RestServer by implementing UNetLab-specific methods. For example, <code>login()</code> sends username and password using the <code>add_object()</code> method of the parent class and sets the cookies extracted from the response to allow all subsequent methods to be authenticated.</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">REST_SCHEMA = {
    'login': '/auth/login',
    'logout': '/auth/logout',
    'status': '/status',
    'list_templates': '/list/templates/'
}

class UnlServer(RestServer):

    def __init__(self, address):
        super(UnlServer, self).__init__(address)

    def login(self, user, pwd):
        api_call = REST_SCHEMA['login']
        payload = {
            "username": user,
            "password": pwd
        }
        resp = self.add_object(api_call, data=payload)
        self.set_cookies(resp.cookies)
        return resp

    def logout(self):
        api_call = REST_SCHEMA['logout']
        resp = self.get_object(api_call)
        return resp

    def get_status(self):
        api_call = REST_SCHEMA['status']
        resp = self.get_object(api_call)
        return resp

    def get_templates(self):
        api_call = REST_SCHEMA['list_templates']
        resp = self.get_object(api_call)
        return resp
</code></pre>

<p>As you can see all methods follow the same pattern:</p>

<ol>
<li>Extract an API url from <code>REST_SCHEMA</code> global variable</li>
<li>Send a request using one of the 4 CRUD methods of the parent RestServer class</li>
<li>Return the response</li>
</ol>


<p>Now let&rsquo;s see how we can use TDD approach to build out the rest of the code.</p>

<h2>Test-driven development</h2>

<p>The easiest way to test RESTful application is by observing the status code of the returned HTTP response. If it is 200 or 201 then it can be considered successful. The biggest challenge is to make sure each test case is independent from one another. One option is to include all the code required by a test case inside the function that implements it. This, however, may lead to long and unwieldy spaghetti-code and breaks the <abbr title="Do Not Repeat Yourself">DRY</abbr> principle.<br/>
To help avoid that, TDD frameworks often have <code>fixtures</code> - functions that are run before and after every test case, designed to setup and cleanup the test environment. In our case we can use fixtures to login before each test case is run and logoff after it&rsquo;s finished. Let&rsquo;s see how we can use Python&rsquo;s built-in <a href="https://docs.python.org/2/library/unittest.html">unittest</a> framework to drive the REST SDK development process.<br/>
First let&rsquo;s define our base class <code>UnlTests</code> who&rsquo;s sole purpose will be to implement authentication fixtures. All the test cases will go into child classes that can either reuse and extend these fixtures. This is how test cases for the already existing code look like:</p>

<pre><code class="python /rest-blog-unl-client/tests/test_unl.py">
class UnlTests(unittest.TestCase):

    def setUp(self):
        self.unl = UnlServer(UNETLAB_ADDRESS)
        resp = self.unl.login(USERNAME, PASSWORD)
        self.assertEqual(200, resp.status_code)

    def tearDown(self):
        resp = self.unl.logout()
        self.assertEqual(200, resp.status_code)

class BasicUnlTests(UnlTests):

    def test_status(self):
        resp = self.unl.get_status()
        self.assertEqual(200, resp.status_code)

    def test_templates(self):
        resp = self.unl.get_templates()
        self.assertEqual(200, resp.status_code)
</code></pre>

<p>At this point if you add all the necessary import statements and populate global variables with correct IP addresses and credentials all tests should pass. Now let&rsquo;s add another test case to retrieve <a href="http://www.unetlab.com/2015/09/using-unetlab-apis/#toc2">user information</a> from UNL:</p>

<pre><code class="python /rest-blog-unl-client/tests/test_unl.py">
class BasicUnlTests(UnlTests):
    ...

    def test_user_info(self):
        resp = self.unl.get_user_info()
        self.assertEqual(200, resp.status_code)
</code></pre>

<p>Rerun the tests and watch the last one fail saying <code>'UnlServer' object has no attribute 'get_user_info'</code>. Now let&rsquo;s go back to our UNL SDK code and add that attribute:</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">REST_SCHEMA = {
    ... ,
    'get_user_info': '/auth'
}

class UnetLab(RestServer):
    ...

    def get_user_info(self):
        api_call = REST_SCHEMA['get_user_info']
        resp = self.get_object(api_call)
        return resp
</code></pre>

<p>Rerun the <code>test_unl.py</code> now and watch all tests succeed again. The same iterative approach can be used to add any number of new methods at the same time making sure none of the existing functionality is affected.<br/>
Note that these are very simple tests and they only verify the response code and not its contents. The better approach would be to look inside the payload and verify, for example, that username is <code>admin</code>.</p>

<h3>UnlLab and UnlNode implementation</h3>

<p>Now let&rsquo;s revert back to normal coding style for a second and create classes for Labs and Nodes. As per the design, these should be separate objects but they should contain a pointer to the context in which they exist. Therefore, it makes sense to instantiate a Lab inside a UnlServer, a Node inside a Lab and pass in the <code>self</code> (UnlServer or Lab) as an argument. For example, here is how a lab will be created:</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">REST_SCHEMA = {
    ... ,
    'create_lab': '/labs'
}

class UnlServer(RestServer):
    ...

    def create_lab(self, name):
        return UnlLab(self, name)

class UnlLab(object):

    def __init__(self, unl, name):
        api_call = REST_SCHEMA['create_lab']
        payload = {
           "path": "/",
           "name": name,
           "version": "1"
        }
        self.name = name
        self.unl = unl
        self.resp = self.unl.add_object(api_call, data=payload)
</code></pre>

<p>So to create a Lab we need to issue a <code>.create_lab()</code> call on UnlServer object and give it a labname. That function will return a new Lab object with the following attributes preset:</p>

<ul>
<li>Lab name - <code>self.name</code></li>
<li>UnlServer that created it - <code>self.unl</code></li>
<li>HTTP response returned by the server after the Create CRUD action - <code>self.resp</code></li>
</ul>


<p>The latter can be used to check if the creation was successful (and potentially throw an error if it wasn&rsquo;t). The structure of the payload can be found in <a href="http://www.unetlab.com/2015/09/using-unetlab-apis/#toc30">API docs</a>.</p>

<p>Nodes will be created in a similar way with a little exception. Apart from the name, Node also needs to know about the particulars of the device it will represent (like device type, image name etc.). That&rsquo;s where Device class comes in. The implementation details are very easy and can be found on <a href="https://github.com/networkop/rest-blog-unl-client/blob/c72f7bdc11427ac5efe9ec18401f0d63c57221ba/restunl/device.py">Github</a> so I won&rsquo;t provide them here. The only function of a Device at this stage is to create a dictionary that can be used as a payload in <code>create_node</code> <a href="http://www.unetlab.com/2015/09/using-unetlab-apis/#toc34">API request</a>.</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">REST_SCHEMA = {
    ... ,
    'create_node': '/labs/{lab_name}/nodes',
}

class UnlLab(object):
    ...

    def create_node(self, device):
        return UnlNode(self, device)

class UnlNode(object):

    def __init__(self, lab, device):
        self.unl = lab.unl
        self.lab = lab
        self.device = device
        api_call = REST_SCHEMA['create_node']
        api_url = api_call.format(api_call, lab_name=append_unl(self.lab.name))
        payload = self.device.to_json()
        self.resp = self.unl.add_object(api_url, data=payload)
</code></pre>

<p>Take a quick look at how the <code>api_url</code> is created. We&rsquo;re using <code>.format()</code> method (built-into <code>string</code> module) to substitute a named variable <code>{format}</code> with the actual name of the lab (<code>self.lab.name</code>). That labname gets appended with an extension by a helper function <code>append_unl</code>. That helper function, along with the others we&rsquo;ll define in the future, can also be found on <a href="https://github.com/networkop/rest-blog-unl-client/blob/c72f7bdc11427ac5efe9ec18401f0d63c57221ba/restunl/helper.py">Github</a>.</p>

<h2>Back to TDD</h2>

<p>Let&rsquo;s use TDD again to add the last two actions we&rsquo;ll cover in this post.</p>

<ul>
<li>Get list of all Nodes</li>
<li>Delete a lab</li>
</ul>


<pre><code class="python /rest-blog-unl-client/tests/test_unl.py">class BasicUnlLabTest(UnlTests):

    def test_create_lab(self):     
        self.unl.delete_lab(LAB_NAME)
        resp = self.unl.create_lab(LAB_NAME).resp
        self.unl.delete_lab(LAB_NAME)
        self.assertEqual(200, resp.status_code)

    def test_delete_lab(self):
        self.unl.create_lab(LAB_NAME)
        resp = self.unl.delete_lab(LAB_NAME)
        self.assertEqual(200, resp.status_code)

    def test_get_nodes(self):
        lab = self.unl.create_lab(LAB_NAME)
        resp = lab.get_nodes()
        self.unl.delete_lab(LAB_NAME)
        self.assertEqual(200, resp.status_code)   
</code></pre>

<p>As a challenge, try implementing the SDK logic for the last two failing methods yourself using <a href="http://www.unetlab.com/2015/09/using-unetlab-apis/">UNL API</a> as a reference. You can always refer the the link at the end of the post if you run into any problems.</p>

<h2>Simple App</h2>

<p>So far we&rsquo;ve created and deleted objects with REST API but haven&rsquo;t seen the actual result. Let&rsquo;s start writing an app that we&rsquo;ll continue to expand in the next post. In this post we&rsquo;ll simply login and create a lab containing a single node.</p>

<pre><code class="python  /rest-blog-unl-client/samples/app-1.py">from restunl.unetlab import UnlServer
from restunl.device import Router

LAB_NAME = 'test_1'

def app_1():
    unl = UnlServer('192.168.247.20')
    unl.login('admin', 'unl')
    print ("*** CONNECTED TO UNL")
    lab = unl.create_lab(LAB_NAME)
    print ("*** CREATED LAB")
    node_1 = lab.create_node(Router('R1'))
    print ("*** CREATED NODE")

if __name__ == '__main__':
    app_1()
</code></pre>

<p>Run this once, then login the UNL web GUI and navigate to <code>test_1</code> lab. Examine how node <strong>R1</strong> is configured and compare it to the defaults set in a <a href="https://github.com/networkop/rest-blog-unl-client/blob/c72f7bdc11427ac5efe9ec18401f0d63c57221ba/restunl/device.py">Device module</a>.</p>

<h2>Source code</h2>

<p>All code from this post can be found in my <a href="https://github.com/networkop/rest-blog-unl-client/tree/c72f7bdc11427ac5efe9ec18401f0d63c57221ba">public repository on Github</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Development Environment Setup]]></title>
    <link href="http://networkop.github.io/blog/2015/06/17/dev-env-setup/"/>
    <updated>2015-06-17T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2015/06/17/dev-env-setup</id>
    <content type="html"><![CDATA[<p>Before we proceed with TDD framework build it is important to have the development environment setup. In our case it will consist of two major components:</p>

<ul>
<li>Network Simulation Environment</li>
<li>Ansible Development Environment</li>
</ul>


<p>To simplify things I will run both of these environments on the same Virtual Machine. For network simulation I will use <a href="http://www.unetlab.com/">UnetLab</a>, a wonderful product developed by Andrea Dainese. Currently, UnetLab is distributed as an OVA package and is available for free download on <a href="http://www.unetlab.com/download/">the website</a>. To simulate network devices I will run <abbr title="IOS on Unix">IOU</abbr> which will be interconnected to form a simple network. Finally, I will show how to setup development environment with Ansible, git and Python.</p>

<!--more-->


<h2>UnetLab setup</h2>

<p>UnetLab is a network simulation environment very similar to GNS3. The biggest advantage for me, personally, is that it runs as a single entity and doesn&rsquo;t require a separate front-end like GNS3. That being said, the only requirement for this project is for the test network to have remote connectivity to a machine running Ansible, so having UnetLab specifically is not required and any network simulator would do, including a real (non-virtual) lab. One of the side effects of choosing UnetLab is that all development will have to be done on Ubuntu which is the OS pre-installed in the OVA.<br/>
Here are the steps required to get the network environment setup:</p>

<ol>
<li><a href="http://www.unetlab.com/download/">Download</a> and import OVA file into the hypervisor of your choice.</li>
<li>Download and <a href="http://www.unetlab.com/2014/11/adding-cisco-iouiol-images/">import</a> Cisco L3 IOU file.</li>
<li>Create a simple 4-device network (<a href="http://www.unetlab.com/2014/11/create-the-first-lab/">example</a>) and <a href="http://www.unetlab.com/2014/11/using-cloud-devices/">connect it to the network of host machine</a>.</li>
<li><a href="http://www.unetlab.com/2015/03/url-telnet-ssh-vnc-integration-on-windows/">Configure</a> your favourite terminal program to work with UnetLab&rsquo;s web interface</li>
</ol>


<p>This is the topology I will be using for testing:
<img class="center" src="/images/lab-topo.png" title="&lsquo;Test Topology&rsquo;" ></p>

<p>Each device will have a Loopback interface in <code>10.0.0.0/24</code> subnet which I will statically point to <code>interface Eth0/2</code> of R1 on the host machine. Here&rsquo;s the example of R1&rsquo;s configuration:</p>

<pre><code class="text Sample Router Configuration - R1">! Configure hostname, domain and RSA key to enable SSH
hostname R1
ip domain name tdd.lab
crypto key generate rsa modulus 1024
! Point AAA to local database
aaa new-model
aaa authentication login default local
aaa authorization exec default local
username cisco privilege 15 secret cisco
! Enable remote ssh connections
line vty 0 4
 transport input ssh
! Configure interfaces
interface Loopback0
 ip address 10.0.0.1 255.255.255.255
!
interface Ethernet0/0
 ip address 12.12.12.1 255.255.255.0
!
interface Ethernet0/1
 ip address 14.14.14.1 255.255.255.0
!
interface Ethernet0/2
 description connection to host machine
 ip address 192.168.247.25 255.255.255.0
! Enable dynamic routing
router eigrp 100
 network 0.0.0.0
!
end
write 
</code></pre>

<p>All other devices will have similar configuration with the end goal of having connectivity between any pair of Loopback interfaces.</p>

<p>In order to to have connectivity to devices from a host machine we need to add a static route for <code>10.0.0.0/24</code> network:</p>

<pre><code class="bash Adding a static route to test topology">$ route add -net 10.0.0.0 netmask 255.255.255.0 gw 192.168.247.25
</code></pre>

<p>At this point host machine should be able to ping each one of those Loopbacks:</p>

<pre><code class="bash Testing connectivity to test devices">$ for i in {1..4}; do ping -c 1 10.0.0.$i; done | grep packets
1 packets transmitted, 1 received, 0% packet loss, time 0ms
1 packets transmitted, 1 received, 0% packet loss, time 0ms
1 packets transmitted, 1 received, 0% packet loss, time 0ms
1 packets transmitted, 1 received, 0% packet loss, time 0ms
</code></pre>

<h2>Dev environment setup</h2>

<p>Ansible is one of the most popular automation and orchestration tools in IT industry. Part of its popularity is due to the &ldquo;clientless&rdquo; architecture where
the only requirement to a managed system is to have ssh access and Python execution environment. The latter pretty much rules out the biggest part of common
networking infrastructure. However it is still possible to use Ansible in a &ldquo;raw&rdquo; mode and write modules of our own. That&rsquo;s exactly what we&rsquo;re gonna do in this exercise.
Due to the fact that Ansible is written in Python, it has better support for modules written in the same language, therefore all modules will be written in Python.<br/>
One important tool every developer uses is version control. It allows to track changes made to the code and enables collaboration between multiple
people working on the same project. For beginners it always makes sense to stick to the most popular tools, that&rsquo;s why I&rsquo;ll be using git for version control and store all my code on Github.</p>

<p>This is what&rsquo;s needed to setup the development environment:</p>

<pre><code class="bash 1. Install Python and git packages">$ sudo apt-get update &amp;&amp; sudo apt-get install python git-core
</code></pre>

<pre><code class="bash 2. Initialise global git settings">$ git config --global user.name "Network-oriented programming"
$ git config --global user.email "networkop@example.com"
</code></pre>

<pre><code class="bash 3. Install Ansible http://docs.ansible.com/intro_installation.html Ansible Installation">$ sudo apt-get install software-properties-common
$ sudo apt-add-repository ppa:ansible/ansible
$ sudo apt-get update
$ sudo apt-get install ansible
</code></pre>

<pre><code class="bash 4. Test Ansible connectivity to our network topology">$ sudo echo "R1 ansible_ssh_host=10.0.0.1" &gt;&gt; /etc/ansible/hosts
$ printf "[defaults]\nhost_key_checking=False\n" &gt;&gt; ansible.cfg
$ ansible R1 -u cisco --ask-pass -m "raw" -a "show version | include IOS"
SSH password:
R3 | success | rc=0 &gt;&gt;
Cisco IOS Software, Linux Software (I86BI_LINUX-ADVENTERPRISEK9-M), Version 15.4(1)T, DEVELOPMENT TEST SOFTWARE
Connection to 10.0.0.1 closed by remote host.
</code></pre>

<p>The above script first populates Ansible <code>inventory</code> file with an ip address of R1, then disables ssh key checking,
 and finally runs an <code>ad-hoc</code> command <code>show version | include IOS</code> which should prompt for a password and return a result of command execution on R1.
 I will explain about inventory and configuration files in a bit more detail in the next post. At this stage all what&rsquo;s required is a meaningful response from a Cisco router.</p>

<p><figure class='code'><figcaption><span>
5. Create a free Github account and setup a new repository
</span><a href='https://github.com/join'>Join Github</a></figcaption><div class="highlight">
</figure>
For my blog I will be using <code>networkop</code> as a Github username and <code>simple-cisco-tdd</code> as a repository name.
Once respository is created, Github will provide instructions to setup repository on a local machine which will be done in the next step.</p>

<pre><code class="bash 6. Setup a project directory and initialise git">$ mkdir ~/tdd_ansible &amp;&amp; cd ~/tdd_ansible
$ eacho "simple-cisco-tdd" &gt;&gt; README.md
$ git init
$ git add README.md
$ git commit -m "first commit"
$ git remote add origin https://github.com/networkop/simple-cisco-tdd.git
$ git push -u origin master
Username for https://github.com: networkop
Password for https://networkop@github.com:
Counting objects: 3, done.
Writing objects: 100% (3/3), 206 bytes | 0 bytes/s, done.
Total 3 (delta 0), reused 0 (delta 0)
To https://github.com/networkop/simple-cisco-tdd.git
 * [new branch]      master -&gt; master
Branch master set up to track remote branch master from origin.
</code></pre>

<p>The above result indicates that <code>README.md</code> file has been pushed to Github successfully. Needless to say that all pushed local files can be also viewed from Github&rsquo;s web page.</p>

<hr />

<p>This completes the initial environment setup. I highly recommend at this stage, hypervisor permitting, to take a snapshot of a current state of a virtual machine to avoid having to rebuild it every time something goes pear-shaped.
In the next post I will show how to setup Ansible to work with Cisco devices.</p>
]]></content>
  </entry>
  
</feed>
