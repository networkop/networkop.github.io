<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Unetlab | Network-oriented programming]]></title>
  <link href="http://networkop.github.io/blog/categories/unetlab/atom.xml" rel="self"/>
  <link href="http://networkop.github.io/"/>
  <updated>2016-10-26T14:23:05-07:00</updated>
  <id>http://networkop.github.io/</id>
  <author>
    <name><![CDATA[Michael Kashin]]></name>
    <email><![CDATA[mmkashin@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Type-2 and Type-5 EPVN on vQFX 10k in UnetLab]]></title>
    <link href="http://networkop.github.io/blog/2016/10/26/qfx-unl/"/>
    <updated>2016-10-26T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/10/26/qfx-unl</id>
    <content type="html"><![CDATA[<p>I was fortunate enough to be given a chance to test the new virtual QFX 10k image from Juniper. In this post I will show how to import this image into UnetLab and demonstrate the basic L2 and L3 EVPN services.</p>

<!--more-->


<h2>News about UnetLab</h2>

<p>Those who read my blog regularly know that I&rsquo;m a big fan of a network simulator called UnetLab. For the last two years I&rsquo;ve done all my labbing in UNL and was constantly surprised by how extensible and stable it has been. I believe that projects like this are very important to our networking community because they help train the new generation of network engineers and enable them to expand their horizons. Recently UnetLab team has decided take the next step and create a new version of UNL. This new project, called <a href="https://www.indiegogo.com/projects/eve-ng-emulated-virtual-environment-next-gen#/">EVE-NG</a>, will help users build labs of any size and run full replicas of their production networks, which is ideal for <a href="/blog/2016/02/19/network-ci-intro/">pre-deployment testing</a> of network changes. If you want to learn more, check out the <a href="https://www.indiegogo.com/projects/eve-ng-emulated-virtual-environment-next-gen#/">EVE-NG</a> page on indiegogo.</p>

<h2>Creating vQFX nodes in UnetLab</h2>

<p>Back to the business at hand, vQFX is not publically available yet but is expected to pop up at <a href="http://www.juniper.net/">Juniper.net</a> some time in the future. Similar to a recently released vMX, vQFX will consist of two virtual machines - one running the routing engine (RE) and second simulating the ASIC forwarding piplines (PFE). You can find more information about these images on Juniper&rsquo;s <a href="https://github.com/Juniper/vqfx10k-vagrant">Github page</a>. Images get distributed in multiple formats but in the context of this post we&rsquo;ll only deal with two VMDK files:</p>

<pre><code class="bash vQFX images">vqfx10k-re-15.1X53-D60.vmdk
vqfx10k-pfe-20160609-2.vmdk
</code></pre>

<p>To be able to use these images in UnetLab, we first need to convert them to <strong>qcow2</strong> format and copy them to the directory where UNL stores all its qemu images:</p>

<pre><code class="bash Importing VMDK images"> mkdir /opt/unetlab/addons/qemu/qfx_re-15d1X53
 mkdir /opt/unetlab/addons/qemu/qfx_pfe-20160609

/opt/qemu/bin/qemu-img convert -f vmdk -O qcow2 vqfx10k-pfe-20160609-2.vmdk /opt/unetlab/addons/qemu/qfx_pfe-20160609/hda.qcow2

/opt/qemu/bin/qemu-img convert -f vmdk -O qcow2 vqfx10k-re-15.1X53-D60.vmdk /opt/unetlab/addons/qemu/qfx_re-15d1X53/hda.qcow2
</code></pre>

<p>Next, we need to create new node definitions for RE and PFE VMs. The easiest way would be to clone the linux node type:</p>

<pre><code class="bash Creating vQFX node definitions">cd /opt/unetlab/html/templates
cp linux.php qfx_pfe.php
cp linux.php qfx_re.php

sed -i 's/2048/1024/; s/virtio-net-pci/e1000/; s/Server/Switch/' qfx_re.php
sed -i 's/2048/1536/; s/virtio-net-pci/e1000/; s/Server/Switch/' qfx_pfe.php

sed -i 's/Linux/QFX_RE/g; s/linux/qfx_re/g' qfx_re.php
sed -i 's/Linux/QFX_PFE/g; s/linux/qfx_pfe/g' qfx_pfe.php

sed -ri 's/(.*ethernet.*) = 1/\1 = 2/' qfx_pfe.php
sed -ri 's/(.*ethernet.*) = 1/\1 = 8/' qfx_re.php
</code></pre>

<p>Now let&rsquo;s add the QFX to the list of nodes by modifying the following file:</p>

<pre><code class="bash /opt/unetlab/html/includes/init.php">'openstack'             =&gt;      'Openstack',
'qfx_re'                =&gt;      'QFX10k-RE',
'qfx_pfe'               =&gt;      'QFX10k-PFE',
'mikrotik'              =&gt;      'MikroTik RouterOS',
</code></pre>

<p>Optionally, <code>/opt/unetlab/html/includes/__node.php</code> can be modified to change the default naming convention similar to the <code>vmx</code> node.</p>

<p>Once you&rsquo;ve done all the above changes, you should have a working vQFX 10k node available in UNL GUI. For the purpose of demonstration of EVPN features I&rsquo;ve created the following topology:</p>

<p><img class="center" src="/images/qfx-unl.png"></p>

<h2>EVPN L2 and L3 services</h2>

<p>EVPN standards define multiple routes types to distribute NLRI information across the network. The two most &ldquo;significant&rdquo; route types are 2 and 5. Type-2 NLRI was designed to carry the MAC (and optionally IP) address to VTEP IP binding information which is used to populate the dynamic MAC address table. This function, that was previously accomplished by a central SDN controller, is now performed in a scalable, standard-based, controller-independent fashion. <a href="http://www.juniper.net/documentation/en_US/junos16.1/topics/concept/evpn-route-type5-understanding.html">Type-5 NLRI</a> contains IP Prefix to VTEP IP mapping and is similar to the function of traditional L3 VPNs. In order to explore the capabilities of EVPN implementation on vQFX I&rsquo;ve created and artificial scenario with 3 virtual switches, 3 VLANs and 4 hosts.</p>

<p><img class="center" src="/images/qfx-lab.png"></p>

<p>VLAN10 (green) is present on all 3 switches, VLAN20 (purple) is only configured on switches 1 and 2 and VLAN88 (red) only exists on SW3. I&rsquo;ve provided configuration snippets below for reference purposes only and only for SW1. Remaining switches are configured similarly.</p>

<h3>Configuring Basic IP and BGP setup</h3>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">set </span>interfaces xe-0/0/0 unit <span class="m">0</span> family inet address 12.12.12.1/24
</span><span class='line'><span class="nb">set </span>interfaces xe-0/0/2 unit <span class="m">0</span> family inet address 13.13.13.1/24
</span><span class='line'><span class="nb">set </span>interfaces lo0 unit <span class="m">0</span> family inet address 99.99.99.1/32
</span><span class='line'><span class="nb">set </span>routing-options static route 99.99.99.2/32 next-hop 12.12.12.2
</span><span class='line'><span class="nb">set </span>routing-options static route 99.99.99.3/32 next-hop 13.13.13.3&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;set routing-options router-id 99.99.99.1
</span><span class='line'><span class="nb">set </span>routing-options autonomous-system 555&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;set routing-options autonomous-system 555
</span><span class='line'><span class="nb">set </span>protocols bgp group EVPN <span class="nb">type </span>internal
</span><span class='line'><span class="nb">set </span>protocols bgp group EVPN <span class="nb">local</span>-address 99.99.99.1
</span><span class='line'><span class="nb">set </span>protocols bgp group EVPN family evpn signaling
</span><span class='line'><span class="nb">set </span>protocols bgp group EVPN neighbor 99.99.99.2
</span><span class='line'><span class="nb">set </span>protocols bgp group EVPN neighbor 99.99.99.3
</span></code></pre></td></tr></table></div></figure></p>

<h3>Configuring End-host connectivity and IRB</h3>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">set </span>vlans BD5010 vlan-id 10
</span><span class='line'><span class="nb">set </span>vlans BD5010 l3-interface irb.10
</span><span class='line'><span class="nb">set </span>vlans BD5020 vlan-id 20
</span><span class='line'><span class="nb">set </span>vlans BD5020 l3-interface irb.20
</span><span class='line'><span class="nb">set </span>interfaces xe-0/0/1 unit <span class="m">0</span> family ethernet-switching vlan members 10
</span><span class='line'><span class="nb">set </span>interfaces xe-0/0/3 unit <span class="m">0</span> family ethernet-switching vlan members 20
</span><span class='line'><span class="nb">set </span>interfaces irb unit <span class="m">10</span> family inet address 10.0.0.254/24
</span><span class='line'><span class="nb">set </span>interfaces irb unit <span class="m">20</span> family inet address 20.0.0.254/24
</span></code></pre></td></tr></table></div></figure></p>

<h3>Configuring L2 EVPN services</h3>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">set </span>protocols evpn encapsulation vxlan
</span><span class='line'><span class="nb">set </span>protocols evpn extended-vni-list all
</span><span class='line'><span class="nb">set </span>protocols evpn multicast-mode ingress-replication&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;set switch-options vtep-source-interface lo0.0
</span><span class='line'><span class="nb">set </span>switch-options route-distinguisher 555:0
</span><span class='line'><span class="nb">set </span>switch-options vrf-target target:555:123&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;set vlans BD5010 vxlan vni 5010
</span><span class='line'><span class="nb">set </span>vlans BD5010 vxlan ingress-node-replication
</span><span class='line'><span class="nb">set </span>vlans BD5020 vxlan vni 5020
</span><span class='line'><span class="nb">set </span>vlans BD5020 vxlan ingress-node-replication
</span></code></pre></td></tr></table></div></figure></p>

<h3>Configuring L3 EVPN service</h3>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">set </span>routing-instances EVPN-VRF instance-type vrf
</span><span class='line'><span class="nb">set </span>routing-instances EVPN-VRF interface irb.10
</span><span class='line'><span class="nb">set </span>routing-instances EVPN-VRF interface irb.20
</span><span class='line'><span class="nb">set </span>routing-instances EVPN-VRF interface lo0.10
</span><span class='line'><span class="nb">set </span>routing-instances EVPN-VRF route-distinguisher 555:1
</span><span class='line'><span class="nb">set </span>routing-instances EVPN-VRF vrf-target target:123:5055
</span><span class='line'><span class="nb">set </span>routing-instances EVPN-VRF protocols evpn ip-prefix-routes advertise direct-nexthop
</span><span class='line'><span class="nb">set </span>routing-instances EVPN-VRF protocols evpn ip-prefix-routes encapsulation vxlan
</span><span class='line'><span class="nb">set </span>routing-instances EVPN-VRF protocols evpn ip-prefix-routes vni 5555
</span></code></pre></td></tr></table></div></figure></p>

<h2>Traffic flow overview</h2>

<p>Once all the nodes have been configured, we can have a closer look at the traffic flows, specifically at how packets are being forwarded and where the L2 and L3 lookups take place.</p>

<h3>L2 forwarding - H1 to H2 (00:50:79:66:68:06)</h3>

<p>Traffic from H1 to H2 will never leave its own broadcast domain. As soon as the packet hits the incoming interface of SW1, MAC address lookup occurs pointing to the remote VTEP interface of SW2.</p>

<pre><code class="bash SW1> show ethernet-switching table | match 00:50:79:66:68:06">   BD5010              00:50:79:66:68:06   D        vtep.32769             99.99.99.2
</code></pre>

<p>Once SW2 decapsulates the packet, the lookup in the MAC address table returns the locally connected interface, where it gets forwarded next.</p>

<pre><code class="bash SW2> show ethernet-switching table | match 00:50:79:66:68:06">   BD5010              00:50:79:66:68:06   D        xe-0/0/1.0
</code></pre>

<h3>L3 forwarding (symmetric) - H3 to H4</h3>

<p>The route to 8.8.8.0/24 is advertised by SW3 in type-5 NLRI</p>

<pre><code class="bash SW1> show route receive-protocol bgp 99.99.99.3 extensive">* 5:555:1::0::8.8.8.0::24/304 (1 entry, 1 announced)
     Import Accepted
     Route Distinguisher: 555:1
     Route Label: 5555
     Overlay gateway address: 0.0.0.0
     Nexthop: 99.99.99.3
     Localpref: 100
     AS path: I
     Communities: target:123:5055 encapsulation0:0:0:0:vxlan router-mac:02:05:86:71:72:00
</code></pre>

<p>This NLRI doesn&rsquo;t contain any overlay gateway address, however it does have a special &ldquo;router-mac&rdquo; community with a globally unique SW3&rsquo;s chassis MAC. This MAC is advertised as normal type-2 MAC address and points to the remote VTEP interface of SW3:</p>

<pre><code class="bash SW1> show ethernet-switching table | match 02:05:86:71:72:00">   BD5010              02:05:86:71:72:00   D        vtep.32770             99.99.99.3
</code></pre>

<p>The above two pieces of information are fed into our EVPN-VRF routing table to produce the entry with the following parameters:</p>

<pre><code class="bash SW1> show route table EVPN-VRF.inet.0 detail 8.8.8.8 | match "VTEP|VNI|MAC"">Encap VNI: 5555, Decap VNI: 5555
Source VTEP: 99.99.99.1, Destination VTEP: 99.99.99.3
SMAC: 02:05:86:71:3b:00, DMAC: 02:05:86:71:72:00
</code></pre>

<p>This is the example of how &ldquo;symmetric&rdquo; IRB routing is performed. Instead of routing the packet at the ingress and switching at the egress node, how it was done in the case of Neutron&rsquo;s <a href="/blog/2016/10/13/os-dvr/">DVR</a>, the routing is performed twice. First the packet is routed into a &ldquo;transit&rdquo; VNI 5555, which glues all the switches in the same EVI together from the L3 perspective. Once the packet reaches the destination node, it gets routed into the intended VNI (5088 in our case) and forwarded out the local interface. This way switches may have different sets of VLANs and IRBs and still be able route packets between VXLANs.</p>

<h3>L3 forwarding (asymmetric) - H1 to H4</h3>

<p>As you may have noticed, the green broadcast domain extends to all three switches, even though hosts are only attached to the first two. Let&rsquo;s see how it will affect the packet flows.
The flow from H1 to H4 will be similar to the one from H3 to H4 described above. However return packets will get routed on SW3 directly into VXLAN5010, since that switch has an IRB.10 interface and then switched all the way to H1.</p>

<pre><code class="bash SW3> show route forwarding-table destination 10.0.0.1">Routing table: EVPN-VRF.inet
Internet:
Destination        Type RtRef Next hop           Type Index    NhRef Netif
10.0.0.1/32        dest     0 0:50:79:66:68:5    ucst     1772     1 vtep.32770
</code></pre>

<p>This is the example of &ldquo;asymmetric&rdquo; routing, similar to the one exhibited by Neutron <a href="/blog/2016/10/13/os-dvr/">DVR</a>. You would see similar behaviour if you examined the flow between H3 and H2.</p>

<h2>Conclusion</h2>

<p>So why all the hassle configuring EPVN on data centre switches? For one, you can <a href="http://forums.juniper.net/t5/Data-Center-Technologists/MC-LAG-is-dead-Long-live-EVPN-Multi-homing/ba-p/298924">get rid</a> of MLAG in TOR switches and replace it with EVPN multihoming. However, the main benefit is that you can stretch L2 broadcast domains across your whole data centre without the need for an SDN controller. So, for example, we can now easily satisfy the requirement of having external floating IP network on all compute nodes introduced by <a href="/blog/2016/10/13/os-dvr/">Neutron DVR</a>. EVPN-enabled switches can also now perform functions similar to DC gateway routers (the likes of ASR, MX or SR) while giving you the benefits of horizontal scaling of Leaf/Spine networks. As <a href="https://docs.cumulusnetworks.com/display/DOCS/Ethernet+Virtual+Private+Network+-+EVPN">more</a> and <a href="https://eos.arista.com/forum/evpn-control-plane-support-for-vxlan/">more</a> vendors introduce EVPN support, it is poised to become the ultimate DC routing protocol, complementing the functions already performed by the host-based virtual switches, and with all the DC switches <a href="/blog/2016/09/09/os-lab-p2/">running BGP</a> already, introducing EVPN may be as easy as enabling a new address family.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building a Multi-node OpenStack Lab in UNetLab]]></title>
    <link href="http://networkop.github.io/blog/2016/04/18/os-unl-lab/"/>
    <updated>2016-04-18T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/04/18/os-unl-lab</id>
    <content type="html"><![CDATA[<p>In the <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous post</a> I&rsquo;ve demonstrated how to get a working instance of a single-node OpenStack inside <a href="http://www.unetlab.com/">UNetLab</a>. In this post we&rsquo;ll continue building on that by adding two new compute nodes and redesigning our network to resemble something you might actually see in a real life.</p>

<!--more-->


<h2>OpenStack network requirements</h2>

<p>Depending on the number of deployed <a href="https://www.openstack.org/software/project-navigator/">components</a>, OpenStack physical network requirements could be different. In our case we&rsquo;re not going to deploy any storage solution and simply use the <strong>ephemeral</strong> storage, i.e. hard disk that&rsquo;s a part of a virtual machine. However, even in minimal installations, there are a number of networks that should be considered individually due to different connectivity requirements:</p>

<ul>
<li><p>Server <abbr title=" Out-Of-Band">OOB</abbr> <strong>management</strong> network - this is usually a dedicated physical network used mainly for server bootstrapping and OS deployment. It is a Layer 3 network with DHCP relays configured at each edge L3 interface and access to Internet package repositories.</p></li>
<li><p><strong>API</strong> network - used for internal communication between various OpenStack services. This can be a routed network without Internet access. The only requirement is any-to-any reachability within a single OpenStack environment.</p></li>
<li><p><strong>External</strong> network - used for public access to internal OpenStack virtual machines. This is the <em>outside</em> of OpenStack, with a pool of IP addresses used to NAT the internal IPs of public-facing virtual machines. This network <strong>must</strong> be Layer 2 adjacent <strong>only</strong> with a network control node.</p></li>
<li><p><strong>Tenant</strong> network - used for communication between virtual machines within OpenStack environment. Thanks to the use of VXLAN overlay, this can be a simple routed network that has any-to-any reachability between all Compute and Network nodes.</p></li>
</ul>


<h2>Building a lab network</h2>

<p>For labbing purposes it&rsquo;s possible to relax some of the above network requirements without seriously affecting the outcomes of our simulation. For example, it&rsquo;s possible to combine some of the networks and still satisfy the requirements stated above. These are the networks that will be configured inside UNetLab:</p>

<ul>
<li><p><strong>Management</strong> - this network will combine the functions of OOB and API networks. To isolate it from our data centre underlay I&rsquo;ll be using separate interfaces on virtual machines and connect them directly to Workstation&rsquo;s NAT interface (192.168.91.0/24 in my case)  to give them direct access to Internet.</p></li>
<li><p><strong>External</strong> - this network will be connected to Workstation&rsquo;s host-only NIC (192.168.247.0/24) through Vlan300 configured on one of the leaf switches. Since it must be L2 adjacent with the network control node our leaf switch will not perform any routing for this subnet.</p></li>
<li><p><strong>Tenant</strong> - this will be a routed leaf/spine <a href="https://en.wikipedia.org/wiki/Clos_network">Clos</a> fabric comprised of 3 leaf and 2 spine switches running a single-area OSPF process on all their links. Each server will have its own unique tenant subnet (Vlan100) terminated on the leaf switch and subnet injected into OSPF. The subnet used for this Vlan is going to be <code>10.0.X.0/24</code>, where X is the number of the leaf switch terminating the vlan.</p>

<p>  The links between switches are all L3 point-to-point with addresses borrowed from 169.254.0.0/16 range specifically to emphasize the fact that the internal addressing does not need to be known or routed outside of the fabric. The <strong>sole function of the fabric</strong> is to provide multiple equal cost paths between any pair of leafs, thereby achieving maximum link utilisation. Here&rsquo;s an example of a traceroute between Vlan100&rsquo;s of Leaf #1 and Leaf #3.</p></li>
</ul>


<pre><code class="text Traceroute inside an ECMP routed Clos fabric">L3#traceroute 10.0.1.1 source 10.0.3.1
Type escape sequence to abort.
Tracing the route to 10.0.1.1
VRF info: (vrf in name/id, vrf out name/id)
  1 169.254.31.111 1 msec
    169.254.32.222 0 msec
    169.254.31.111 0 msec
  2 169.254.12.1 1 msec
    169.254.11.1 1 msec *
</code></pre>

<h2>Building lab servers</h2>

<p>Based on my experience a standard server would have at least 3 physical interfaces - one for OOB management and a pair of interfaces for application traffic. The two application interfaces will normally be combined in a single <abbr title=" Link Aggregation Group">LAG</abbr> and connected to a pair of MLAG-capable TOR switches. Multi-chassis LAG or <a href="http://blog.ipspace.net/2010/10/multi-chassis-link-aggregation-basics.html">MLAG</a> is a pretty old and well-understood technology so I&rsquo;m not going to try and simulate it in the lab. Instead I&rsquo;ll simply assume that a server will be connected to a TOR switch via a single physical link. That link will be setup as a dot1q trunk to allow for multiple subnets to share it.</p>

<h2>Physical lab topology</h2>

<p>All the above requirements and assumptions result in the following topology that we need to build inside UNetLab:</p>

<p><img class="center" src="/images/neutron-native.png"></p>

<p>For servers I&rsquo;ll be using OpenStack node type that I&rsquo;ve described in my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous post</a>. The two compute nodes do not need as much RAM as the control node, so I&rsquo;ll reduce it to just 2GB.</p>

<p>For switches I&rsquo;ll be using a Cisco&rsquo;s <a href="http://www.unetlab.com/2014/11/adding-cisco-iouiol-images/">L2 IOU</a> image for now, mainly due to the low resource requirements. In the future I&rsquo;ll try and swap it for something else. As you can see from the sample config below, fabric configuration is very basic and can be easily replaced by any other solution:</p>

<pre><code class="text Leaf 1 configuration">interface Ethernet0/0
 no switchport
 ip address 169.254.11.1 255.255.255.0
 ip ospf network point-to-point
 duplex auto
!
interface Ethernet0/1
 no switchport
 ip address 169.254.12.1 255.255.255.0
 ip ospf network point-to-point
 duplex auto
!
interface Ethernet0/2
 switchport trunk allowed vlan 100
 switchport trunk encapsulation dot1q
 switchport mode trunk
!
interface Vlan100
 ip address 10.0.1.1 255.255.255.0
!
router ospf 1
 network 0.0.0.0 255.255.255.255 area 0
</code></pre>

<h2>Server configuration and OpenStack installation</h2>

<p>Refer to my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous blogpost</a> for instructions on how to install OpenStack and follow the first 5 steps from &ldquo;Installing CentOS and Openstack&rdquo; section. Before doing the final step, we need to configure our VMs' new interfaces:</p>

<ul>
<li>Remove any IP configuration from <strong>eth1</strong> interface to make it look like this:</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/ifcfg-eth1">TYPE=Ethernet
BOOTPROTO=none
DEVICE=eth1
ONBOOT=yes
</code></pre>

<ul>
<li>Configure <strong>Tenant network</strong>:</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/ifcfg-eth1.100">VLAN=yes
DEVICE=eth1.100
BOOTPROTO=none
IPADDR=10.0.X.10
PREFIX=24
ONBOOT=yes
</code></pre>

<ul>
<li>Setup a <strong>static route</strong> to all other leaf nodes:</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/route-eth1.100">10.0.0.0/8 via 10.0.X.1
</code></pre>

<ul>
<li>On Control node setup <strong>External network</strong> interface</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/route-eth1.300">DEVICE=eth1.300
IPADDR=192.168.247.100
PREFIX=24
ONBOOT=yes
BOOTPROTO=none
VLAN=yes
</code></pre>

<p>Now we&rsquo;re ready to kick off OpenStack installation. This can be done with a single command that needs to be executed on the Control node. Note that <code>eth1.100</code> interface is spelled as <code>eth1_100</code> in the last line.</p>

<pre><code class="bash Controller">packstack --allinone \
    --os-cinder-install=n \
    --os-ceilometer-install=n \
    --os-trove-install=n \
    --os-ironic-install=n \
    --nagios-install=n \
    --os-swift-install=n \
    --os-gnocchi-install=n \
    --os-aodh-install=n \
    --os-neutron-ovs-bridge-mappings=extnet:br-ex \
    --os-neutron-ovs-bridge-interfaces=br-ex:eth1.300 \
    --os-neutron-ml2-type-drivers=vxlan,flat \
    --provision-demo=n \
    --os-compute-hosts=192.168.91.10,192.168.91.11,192.168.91.12 \
    --os-neutron-ovs-tunnel-if=eth1_100
</code></pre>

<h2>Creating a virtual network for a pair of VMs</h2>

<p>Once again, follow all steps from &ldquo;Configuring Openstack networking&rdquo; section of my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous blogpost</a>. Only this time when setting up a public subnet, update the subnet details to match our current environment:</p>

<pre><code class="bash Step 3 - Creating a public subnet">  neutron subnet-create --name public_subnet \
    --enable_dhcp=False \
    --allocation-pool=start=192.168.247.90,end=192.168.247.126 \
    --gateway=192.168.247.1 external_network 192.168.247.0/24
</code></pre>

<h2>Nova-scheduler and setting up host aggregates</h2>

<p>OpenStack&rsquo;s Nova project is responsible for managing virtual machines. Nova controller views all available compute nodes as a single pool of resources. When a new VM is to be instantiated, a special process called nova-scheduler examines all available compute nodes and selects the &ldquo;best&rdquo; one based on a special algorithm, which normally takes into account amount of RAM, CPU and other host capabilities.</p>

<p>To make our host selection a little bit more deterministic, we can define a group of compute servers via <strong>host aggregates</strong>, which will be used by nova-scheduler in its selection algorithm. Normally it could include all servers in a single rack or a row of racks. In our case we&rsquo;ll setup two host aggregates each with a single compute host. This way we&rsquo;ll be able to select exactly which compute host to use when instantiating a new virtual machine.</p>

<p>To setup it up, from Horizon&rsquo;s dashboard navigate to Admin -> System and create two host aggregates <strong>comp-1</strong> and <strong>comp-2</strong>, each including a single compute host.</p>

<h2>Creating workloads and final testing</h2>

<p>Using a process described in &ldquo;Spinning up a VM&rdquo; section of my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous blogpost</a>, create a couple of virtual machines assigning them to different host aggregates created earlier.</p>

<h2>Security and Remote access</h2>

<p>To access these virtual machines we need to give them a <a href="https://www.rdoproject.org/networking/floating-ip-range/">floating</a> ip address from the External subnet range. To do that navigate to Project -> Compute -> Instances and select <strong>Associate Floating IP</strong> from the Actions drop-down menu.</p>

<p>The final steps is to allow remote SSH access. Each new VM inherits ACLs from a default security group. So the easiest way to allow SSH is to go to Project -> Compute -> Access &amp; Security and add a rule to allow inbound SSH connections for the default security group.</p>

<h2>Verification</h2>

<p>At this stage you should be able to SSH into the floating IP addresses assigned to the two new VMs using the default credentials. Feel free to poke around and explore Horizon&rsquo;s interface a bit more. For example, try setting up an SSH key pair and re-build our two VMs to allow passwordless SSH access.</p>

<h2>What to expect next</h2>

<p>In the next post we&rsquo;ll explore some of the basic concepts of OpenStack&rsquo;s SDN. We&rsquo;ll peak inside the internal implementation of virtual networks and see what are some of their limitations and drawbacks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Openstack on UNetlab]]></title>
    <link href="http://networkop.github.io/blog/2016/04/04/openstack-unl/"/>
    <updated>2016-04-04T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/04/04/openstack-unl</id>
    <content type="html"><![CDATA[<p>In this post I&rsquo;m going to show how to get a running instance of Openstack inside a UNetLab virtual machine.</p>

<!--more-->


<p><img class="center" src="/images/unl-os.png"></p>

<h2>What the hell am I trying to do?</h2>

<p>I admit that running Openstack on anything other than baremetal is nonsense. So why would anyone want to run it with two layers of virtualisation underneath? My goal is to explore some of the new SDN/NFV technologies without leaving the confines on my home area network and/or racking up a triple-digit electricity bill. I also wanted to be able to swap underlay networks without spending hours trying to plumb together virtualized switches and servers from multiple vendors. That&rsquo;s why I&rsquo;ve decided to use UNetLab VM as a host for my Openstack lab. This would allow me to easily assemble any type of underlay, WAN or DCI network and with hardware virtualisation support I can afford to run Openstack double-nested inside Workstation and Qemu on my dual-core i7 without too much of a performance penalty. After all, <a href="https://www.ravellosystems.com/technology/hvx">some companies</a> even managed to turn similar things into a commercial product.</p>

<p>My interest in Openstack is strictly limited by networking, that&rsquo;s why a lot of the things you&rsquo;ll see in this and following posts will not be applicable to a real-life production environment. However, as far as networking is concerned, I&rsquo;ll try to stick as close to the official Openstack <a href="http://docs.openstack.org/openstack-ops/content/example_architecture.html">network design</a> as possible. I&rsquo;ll be using <a href="https://www.rdoproject.org">RDO</a> to deploy Openstack. The specific method will be Packstack which is a collection of Puppet modules used to deploy Openstack components.</p>

<p>Why have I not went the OpenDaylight/Mininet way if I wanted to play with SDN/NFV? Because I wanted something more realistic to play with, that wouldn&rsquo;t feel like vendor&rsquo;s powerpoint presentation. Plus there&rsquo;s plenty of resources on the &lsquo;net about it anyway.</p>

<p>So, without further ado, let&rsquo;s get cracking.</p>

<h2>Setting the scene</h2>

<p>On my Windows 8 laptop I&rsquo;ve got a UNL virtual machine running inside a VMWare Workstation.. I&rsquo;ve <a href="http://www.unetlab.com/download/">downloaded</a> and <a href="http://www.unetlab.com/2014/11/upgrade-unetlab-installation/">upgraded</a> a pre-built UNL VM image. I&rsquo;ve also downloaded a copy of the <a href="http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.iso">Centos 7 minimal ISO image</a> and copied it over to my UNL VM&rsquo;s home directory.</p>

<p>For network access I&rsquo;ll be using VMWware Workstation&rsquo;s NAT interface. It&rsquo;s currently configured with <code>192.168.91.0/24</code> subnet with DHCP range of <code>.128-.254</code>. Therefore I&rsquo;ll be using <code>.10-.126</code> to allocate IPs to my Openstack servers.</p>

<h2>Creating a custom node type in UNL</h2>

<p>Every node type inside UNL has its own unique settings. Some settings, like amount of RAM, CPU or number of network interfaces, can be changed during node instantiation, while some of them remain &ldquo;baked in&rdquo;. Say, for example, the default &ldquo;Linux&rdquo; template creates nodes with default <strong>Qemu Virtual CPU</strong> which doesn&rsquo;t support the hardware virtualisation (<strong>VT-X/AMD-V</strong>) <a href="http://docs.openstack.org/liberty/config-reference/content/kvm.html">required</a> by Openstack. In order to change that you can either edit the existing node template or follow these steps to create a new one:</p>

<ol>
<li><p>Add Openstack node definition to initialization file <code>/opt/unetlab/html/includes/init.php</code>.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='php'><span class='line'> <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">linux</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;</span>                 <span class="o">=&gt;</span>      <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">Linux</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;,</span>
</span><span class='line'> <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">openstack</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;</span>             <span class="o">=&gt;</span>      <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">Openstack</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;,</span>
</span><span class='line'> <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">mikrotik</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;</span>              <span class="o">=&gt;</span>      <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">MikroTik</span> <span class="nx">RouterOS</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;,</span>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a new Openstack node template based on existing linux node template.</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>cp /opt/unetlab/html/templates/linux.php /opt/unetlab/html/templates/openstack.php
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Edit the template file replacing all occurences of &lsquo;Linux&rsquo; with &lsquo;Openstack&rsquo;</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/Linux/Openstack/g<span class="p">;</span> s/linux/openstack/g<span class="err">&#39;</span> /opt/unetlab/html/templates/openstack.php
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Edit the template file to double the RAM and CPU and pass all host&rsquo;s CPU instructions to Openstack nodes</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/2048/4096/<span class="p">;</span> s/<span class="o">(</span>cpu.*<span class="o">)</span> <span class="o">=</span> 1/<span class="se">\1</span> <span class="o">=</span> 2/<span class="p">;</span> s/<span class="o">(</span><span class="nv">order</span><span class="o">=)</span>dc/<span class="se">\1</span>cd -cpu host/<span class="p">&amp;</span>lsquo<span class="p">;</span> /opt/unetlab/html/templates/openstack.php
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<p>At this point you should be able to navigate to UNL&rsquo;s web interface and find a new node of type Openstack. However you won&rsquo;t be able to create it until you have at least one image, which is what we&rsquo;re going to build next.</p>

<h2>Building a Linux VM inside UNetLab</h2>

<p>Now we need to create a CentOS image inside a UNL. One way to do it is build it inside a VMWare Workstation, copy it to UNL and convert the <strong>.vmdk</strong> to <strong>.qcow2</strong>. However, when I tried doing this I ran into a problem with CentOS not finding the correct disk partitions during bootup. The workaround was to boot into rescue mode and rebuild the initramfs. For those feeling adventurous, I would recommend checking out the following links [<a href="https://wiki.centos.org/TipsAndTricks/CreateNewInitrd">1</a>, <a href="http://advancelinux.blogspot.com.au/2013/06/how-to-rebuild-initrd-or-initramfs-in.html">2</a>, <a href="http://forums.fedoraforum.org/showthread.php?t=288020">3</a>] before trying this option.<br/>
The other option is to build CentOS inside UNL from scratch. This is how you can do it:</p>

<ol>
<li><p>Create a new directory for Openstack image</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>mkdir -p /opt/unetlab/addons/qemu/openstack-1
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a link to CentOS ISO boot image from our new directory</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>ln -s ~/CentOS-7-x86_64-Minimal-1511.iso /opt/unetlab/addons/qemu/openstack-1/cdrom.iso
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a blank 6Gb disk image</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>/opt/qemu/bin/qemu-img create -f qcow2 -o <span class="nv">preallocation</span><span class="o">=</span>metadata /opt/unetlab/addons/qemu/openstack-1/virtioa.qcow2 6G
</span></code></pre></td></tr></table></div></figure></p>

<p>If you want to create <strong>snapshots</strong> at any stage of the process you&rsquo;d need to use a copy of this file under /opt/unetlab/tmp/pod_id/lab_uuid/node_id/ directory</p></li>
</ol>


<p>Now you should be able to successfully create an Openstack node and connect it to Internet. Create a new network that would have Internet connectivity (in my case it&rsquo;s <strong>pnet0</strong>) and connect it to Openstack&rsquo;s <strong>eth0</strong>.  At this stage we have everything ready to start installing Openstack, but before we move on let me take a quick detour to tell you about my ordeals with VNC integration.</p>

<h2>Optional: Integrating TightVNC with UNL</h2>

<p>For some unknown reason UltraVNC does not work well on my laptop. My sessions would often crash or start minimised with the only option to close the window. That&rsquo;s not the only thing not working properly on my laptop thanks to the corporate policies with half of the sh*t locked down for <em>security</em> reasons.<br/>
So instead of mucking around with <strong>Ultra</strong> I decided to give me old pal <strong>Tight</strong>VNC a go. The setup process is very similar to the <a href="http://www.unetlab.com/2015/03/url-telnet-ssh-vnc-integration-on-windows/">official VNC integration guide</a> with the following exceptions:</p>

<ol>
<li><p>The wrapper file simply strips the leading &lsquo;vnc://&rsquo; and trailing &lsquo;/&rsquo; off the passed argument</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='winbatch'><span class='line'><span class="k">SET</span> <span class="nv">arg</span><span class="o">=</span><span class="nv">%1</span>
</span><span class='line'><span class="k">SET</span> <span class="nv">arg</span><span class="o">=</span><span class="nv">%arg:~6%</span>
</span><span class='line'><span class="k">SET</span> <span class="nv">arg</span><span class="o">=</span><span class="nv">%arg:~0</span><span class="p">,</span><span class="m">-1</span>%
</span><span class='line'>start &amp;ldquo;&amp;rdquo; &amp;ldquo;c:\Program Files\TightVNC\tvnviewer.exe&amp;rdquo; <span class="nv">%arg%</span>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>The registry entry now points to the TightVNC wrapper</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='registry'><span class='line'><span class="k">[</span><span class="nb">HKEY_CLASSES_ROOT</span><span class="k">\vnc\shell\open\command]</span>
</span><span class='line'><span class="na">@</span><span class="o">=</span><span class="s">&amp;ldquo;\&quot;c:\Program Files\TightVNC\wrapper.bat\&amp;rdquo; %1&quot;</span>
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<h2>Installing CentOS and Openstack</h2>

<p>Finally, we&rsquo;ve got all our ducks lined up in a row and we&rsquo;re ready to shoot. Fire up the Openstack node inside UNL and click on it to open a vnc session. Proceed to install CentOS with default options. You need to confirm which <strong>hard disk</strong> to use and setup the <strong>hostname</strong> and the <strong>root password</strong> during installation process.
As I mentioned earlier, we&rsquo;ll be using RDO&rsquo;s Packstack to deploy all the necessary Openstack components. The whole installation process will be quite simple and can be found on the RDO&rsquo;s <a href="https://www.rdoproject.org/install/quickstart/">quickstart page</a>. Here is my slightly modified version of installation process:</p>

<ol>
<li><p>Disable Network Manager and SELinux.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>service NetworkManager stop
</span><span class='line'> <span class="nv">$ </span>systemctl disable NetworkManager.service
</span><span class='line'> <span class="nv">$ </span>setenforce 0
</span><span class='line'> <span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/enforcing/permissive/<span class="p">&amp;</span>lsquo<span class="p">;</span> /etc/selinux/config
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Configure static IP address <code>192.168.91.10</code> on the network interface. <br/>
 Assuming your interface name is <code>eth0</code> make sure you /etc/sysconfig/network-scripts/ifcfg-eth0 looks something like this:</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">TYPE</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>Ethernet<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">BOOTPROTO</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>static<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">IPADDR</span><span class="o">=</span>192.168.91.10
</span><span class='line'> <span class="nv">PREFIX</span><span class="o">=</span>24
</span><span class='line'> <span class="nv">GATEWAY</span><span class="o">=</span>192.168.91.2
</span><span class='line'> <span class="nv">DNS1</span><span class="o">=</span>192.168.91.2
</span><span class='line'> <span class="nv">NAME</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>eth0<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">DEVICE</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>eth0<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">ONBOOT</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>yes<span class="p">&amp;</span>rdquo<span class="p">;</span>&lt;br/&gt;
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Issue a <code>service network restart</code> and reconnect to the new static IP address. Make sure that you still have access to Internet after making this change.</p></li>
<li><p>Setup RDO repositories</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>sudo yum install -y &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://rdoproject.org/repos/rdo-release.rpm&quot;</span>&gt;https://rdoproject.org/repos/rdo-release.rpm&lt;/a&gt;
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Update your current packages</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>sudo yum update -y
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Install Packstack</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>sudo yum install -y openstack-packstack
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> That&rsquo;s where it&rsquo;d make sense to take a snapshot with <code>qemu-img snapshot -c pre-install virtioa.qcow2</code> command</p></li>
<li><p>Deploy a single-node Openstack environment</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>packstack --allinone <span class="se">\</span>
</span><span class='line'> --os-cinder-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-ceilometer-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-trove-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-ironic-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --nagios-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-swift-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-neutron-ovs-bridge-mappings<span class="o">=</span>extnet:br-ex <span class="se">\</span>
</span><span class='line'> --os-neutron-ovs-bridge-interfaces<span class="o">=</span>br-ex:eth0 <span class="se">\</span>
</span><span class='line'> --os-neutron-ml2-type-drivers<span class="o">=</span>vxlan,flat <span class="se">\</span>
</span><span class='line'> --provision-demo<span class="o">=</span>n
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Here we&rsquo;re overriding some of the default Packstack options. We&rsquo;re not installing some of the components we&rsquo;re not going to use and setting up a name (<strong>extnet</strong>) for our external physical segment, which we&rsquo;ll use in the next section.</p></li>
</ol>


<p>At the end of these 4 steps you should be able to navigate to Horizon (Openstack&rsquo;s dashboard) by typing <code>http://192.168.91.10</code> in your browser. You can find login credentials in the <code>~/keystonerc_admin</code> file.</p>

<h2>Configuring Openstack networking</h2>

<p>At this stage we need to setup virtual networking infrastructure inside Openstack. This will be almost the same as described in RDO&rsquo;s external network <a href="https://www.rdoproject.org/networking/neutron-with-existing-external-network/">setup guide</a>. The only exceptions will be the <a href="https://www.rdoproject.org/networking/difference-between-floating-ip-and-private-ip/">floating IP range</a>, which will match our existing environment, and the fact that we&rsquo;re no going to setup any additional tenants yet. This is how our topology will look like:</p>

<p><img class="center" src="/images/os-net-1.png"></p>

<ol>
<li><p>Switch to Openstack&rsquo;s <code>admin</code> user
 <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span><span class="nb">source</span> ~/keystonerc_admin
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create external network
 <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron net-create external_network --provider:network_type flat <span class="se">\</span>
</span><span class='line'> --provider:physical_network extnet  <span class="se">\</span>
</span><span class='line'> --router:external <span class="se">\</span>
</span><span class='line'> --shared
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a public subnet</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron subnet-create --name public_subnet <span class="se">\</span>
</span><span class='line'> --enable_dhcp<span class="o">=</span>False <span class="se">\</span>
</span><span class='line'> --allocation-pool<span class="o">=</span><span class="nv">start</span><span class="o">=</span>192.168.91.90,end<span class="o">=</span>192.168.91.126 <span class="se">\</span>
</span><span class='line'> --gateway<span class="o">=</span>192.168.91.2 external_network 192.168.91.0/24
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Default gateway is VMware&rsquo;s NAT IP address</p></li>
<li><p>Create a private network and subnet</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron net-create private_network
</span><span class='line'> neutron subnet-create --name private_subnet private_network 10.0.0.0/24 <span class="se">\</span>
</span><span class='line'> --dns-nameserver 8.8.8.8
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p>  This network is not routable outside of Openstack and is used for inter-VM communication</p></li>
<li><p>Create a virtual router and attach it to both networks</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron router-create router
</span><span class='line'> neutron router-gateway-set router external_network
</span><span class='line'> neutron router-interface-add router private_subnet
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<p>Make sure to check out the visualisation of our newly created network topology in Horizon, it&rsquo;s amazing.</p>

<h2>Spinning up a VM</h2>

<p>There&rsquo;s no point in installing Openstack just for the sake of it. Our final step would be to create a working virtual machine that would be able to connect to Internet.</p>

<ol>
<li><p>Download a test linux image</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> curl &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img&quot;</span>&gt;http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img&lt;/a&gt; <span class="p">|</span> glance <span class="se">\</span>
</span><span class='line'> image-create --name<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>cirros image<span class="p">&amp;</span>rsquo<span class="p">;</span> <span class="se">\</span>
</span><span class='line'> --visibility<span class="o">=</span>public <span class="se">\</span>
</span><span class='line'> --container-format<span class="o">=</span>bare <span class="se">\</span>
</span><span class='line'> --disk-format<span class="o">=</span>qcow2
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>From Horizon&rsquo;s home page navigate to Project -> Compute -> Images.</p></li>
<li><p>Click on <code>Launch Instance</code> and give the new VM a name.</p></li>
<li><p>Make sure it&rsquo;s attached to <code>private_network</code> under the Networking tab.</p></li>
<li><p>Less then a minute later the status should change to <code>Active</code> and you can navigate to VM&rsquo;s console by clicking on its name and going to <code>Console</code> tab.</p></li>
<li><p>Login using the default credentials (<strong>cirros/cubswin:)</strong>) and verify Internet access by pinging google.com.</p></li>
</ol>


<p>Congratulations, we have successfully created a VM running inside a KVM inside a KVM inside a VMWare Workstation inside Windows!</p>

<h2>What to expect next</h2>

<p>Unlike my other post series, I don&rsquo;t have a clear goal at this stage so I guess I&rsquo;ll continue playing around with different underlays for multi-node Openstack and then move on to various SDN solutions available like OpenDayLight and OpenContrail. Unless I lose interest half way through, which happened in the past. But until that happens, stay tuned for more.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[REST for Network Engineers Part 3 - Advanced Operations With UnetLab]]></title>
    <link href="http://networkop.github.io/blog/2016/01/17/rest-unl-advanced/"/>
    <updated>2016-01-17T00:00:00-08:00</updated>
    <id>http://networkop.github.io/blog/2016/01/17/rest-unl-advanced</id>
    <content type="html"><![CDATA[<p>In this post we&rsquo;ll look at how to create arbitrary topologies and push configuration to Nodes in UNetlab via REST SDK. We&rsquo;ll conclude by extending our sample application to create and configure a 3-node topology and enable full connectivity between all nodes.</p>

<!--more-->


<h2>Extracting Node&rsquo;s UUID</h2>

<p>In the <a href="http://networkop.github.io/blog/2016/01/06/rest-basic-operations/">previous post</a> we have learned how to create a Node. To perform further actions on it we need to know it&rsquo;s UUID. According to HTTP specification <code>201 - Created</code> response SHOULD return a <code>Location</code> header with resource URI, which would contain resource UUID. However, UNetLab&rsquo;s implementation does not return a Location header so we need to extract that information ourselves. To do that we&rsquo;ll use the previously defined <code>.get_nodes()</code> method which returns all attributes of all configured Nodes in the following format:</p>

<p><img class="centre" src="/images/rest-unl-get-nodes.png" title="REST SDK GET ALL NODES" ></p>

<p>The best place to extract UUID would be when Node is being created. After the <code>Create</code> request has been sent to a server we&rsquo;ll send another <code>Read</code> request and extract all attributes of a Node based on its name.</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">class UnlNode(object):

    def __init__(self, lab, device):
        ...
        self.node = self._get_node()
        self.id = self.node['id']
        self.url = self.node['url']

    def _get_node(self):
        nodes = self.lab.get_nodes().json()['data']
        return get_obj_by_name(nodes, self.device.name)
</code></pre>

<p> To extract data from the payload we need to call <code>.json()</code> on the returned HTTP response and look for the <code>data</code> key inside that JSON object. The returned value will contain all attributes including the UUID and access URL which we&rsquo;ll use later. To help us find a Node object matching a name we&rsquo;ll use a helper function defined below:</p>

<pre><code class="python /rest-blog-unl-client/restunl/helper.py">def get_obj_by_name(objects, name):
    for obj_id in objects:
        if objects[obj_id]["name"] == name:
            return objects[obj_id]
    return None
</code></pre>

<p>Needless to say that we MUST have unique names for all nodes otherwise it won&rsquo;t be possible to do the matching. It&rsquo;s quite a safe assumption to make in most cases however no built-in error checking will be performed by the REST SDK to prevent you from doing it.</p>

<h2>UnlNet implementation</h2>

<p>Before we start connecting Nodes together we need to create a Network. As per the <a href="http://networkop.github.io/blog/2016/01/06/rest-basic-operations/">design</a>, UnlNet will be a class holding a pointer to the UnlLab object which created it. The structure of the class will be very similar to UnlNode.</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">REST_SCHEMA = { 
                ... ,
                'create_net': '/labs/{lab_name}/networks',
                'get_nets': '/labs/{lab_name}/networks'
            }

class UnlLab(object):
    ...
    def create_net(self, name):
        return UnlNet(self, name)

    def get_nets(self):
        api_call = REST_SCHEMA['get_nets']
        api_url = api_call.format(api_call, lab_name=append_unl(self.name))
        resp = self.unl.get_object(api_url)
        return resp


class UnlNet(object):

    def __init__(self, lab, name):
        api_call = REST_SCHEMA['create_net']
        self.unl, self.lab, self.name = lab.unl, lab, name
        payload = {'type': 'bridge', 'name': self.name}
        api_url = api_call.format(api_call, lab_name=append_unl(self.lab.name))
        self.resp = self.unl.add_object(api_url, data=payload)
        self.net = self._get_net()
        self.id = self.net['id']

    def _get_net(self):
        nets = self.lab.get_nets().json()['data']
        return get_obj_by_name(nets, self.name)
</code></pre>

<h2>Connecting Nodes to a network</h2>

<p>Official <a href="http://www.unetlab.com/2015/09/using-unetlab-apis/">Unetlab API guide</a> is still under development and doesn&rsquo;t specify how to connect a Node to a network. If you want to find out the syntax for this or any other unspecified API call you can always try that in a Web GUI while capturing traffic with Wireshark. That is how I&rsquo;ve discovered that to connect a Node to a network we need to send an Update request with payload containing mapping between Node&rsquo;s interface ID and Network ID.</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">REST_SCHEMA = { 
                ... ,
                'connect_interface': '/labs/{lab_name}/nodes/{node_id}/interfaces'
            }

class UnlNode(object):
    ...

    def connect_interface(self, intf_name, net):
        api_call = REST_SCHEMA['connect_interface']
        api_url = api_call.format(api_call, lab_name=append_unl(self.lab.name), node_id=self.id)
        payload = {get_intf_id(intf_name): net.id}
        resp = self.unl.update_object(api_url, data=payload)
        return resp
</code></pre>

<p>The ID of an interface &ldquo;Ethernet x/y of an IOU device can be easily calculated based on the formula <code>id = x + (y * 16)</code> as described <a href="http://evilrouters.net/2011/01/09/creating-a-netmap-file-for-iou/">here</a>. This will be accomplished with yet another helper function:</p>

<pre><code class="python /rest-blog-unl-client/restunl/helper.py">def get_intf_id(intf_name):
    x, y = re.findall('\d+', intf_name)
    return int(x) + (int(y) * 16)
</code></pre>

<h2>Connecting Nodes to each other</h2>

<p>To create multi-access topologies we would need to maintain an internal mapping between Node&rsquo;s interface and the network it&rsquo;s attached to. However, if we assume that all links are point-to-point, we can not only simplify our implementation but also enable REST client to ignore the notion of a network all together.  We&rsquo;ll simply assume that when device A connects to B our implementation will create a network called <code>A_B</code> in the background and connect both devices to it. This method will perform two separate REST calls and thus will return both responses in a tuple:</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">class UnlNode(object):
    ...

def connect_node(self, local_intf, other_node, other_intf):
    net = self.lab.create_net(name='_'.join([self.device.name, other_node.device.name]))
    resp1 = self.connect_interface(local_intf, net)
    resp2 = other_node.connect_interface(other_intf, net)
    return resp1, resp2
</code></pre>

<p>Assuming all links are point-to-point certainly decreases visibility of created networks and we would not be able to perform selective changes on them in the future. However it is a safe assumption to make for 99% of the networks that I&rsquo;m dealing with.</p>

<h2>Node Start, Stop and Delete</h2>

<p>These simple actions can easily be coded using TDD. I will omit the actual implementation and simply provide unit tests for readers to exercise their TDD skills again.</p>

<pre><code class="python /rest-blog-unl-client/tests/test_unl.py">class AdvancedUnlNodeTest(UnlTests):

    def setUp(self):
        super(AdvancedUnlNodeTest, self).setUp()
        self.device_one = Router('R1')
        self.device_two = Router('R2')
        self.lab = self.unl.create_lab(LAB_NAME)
        self.node_one = self.lab.create_node(self.device_one)
        self.node_two = self.lab.create_node(self.device_two)

    def tearDown(self):
        self.unl.delete_lab(LAB_NAME)
        super(AdvancedUnlNodeTest, self).tearDown()

    def test_start_nodes(self):
        self.lab.stop_all_nodes()
        resp = self.lab.start_all_nodes()
        self.assertEqual(200, resp.status_code)

    def test_stop_nodes(self):
        self.lab.start_all_nodes()
        resp = self.lab.stop_all_nodes()
        self.assertEqual(200, resp.status_code)

    def test_delete_node(self):
        resp = self.lab.delete_node(self.node_one.id)
        self.assertEqual(200, resp.status_code)

    def test_del_all_nodes(self):
        self.lab.del_all_nodes()
        resp = self.lab.get_nodes()
        self.assertEqual(0, len(resp_2.json()['data']))

    def test_lab_cleanup(self):
        resp_1 = self.lab.stop_all_nodes()
        self.lab.del_all_nodes()
        resp_2 = self.lab.get_nodes()
        self.assertEqual(200, resp_1.status_code)
        self.assertEqual(0, len(resp_2.json()['data']))
</code></pre>

<p>The final, <code>lab_cleanup()</code> method is simply a shortcut to <code>stop_nodes()</code> followed by <code>del_all_nodes()</code>.<br/>
As always, link to full code is available at the end of this post.</p>

<h2>Pushing configuration to Nodes</h2>

<p>At this point of time UnetLab does not support configuration import so we&rsquo;re stuck with the only access method available - telnet. To push configuration into the Node we&rsquo;re gonna have to establish a telnet session to Node&rsquo;s URI (which we&rsquo;ve extracted earlier) and write all configuration into that session.</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">class UnlNode(object):
    ...

    def configure(self, text):
        return self.device.send_config(wrap_conf(text))
</code></pre>

<p>Another helper function <code>wrap_conf()</code> prepends <code>enable</code> and appends <code>end</code> to make configuration suitable for pasting into the new IOU device.</p>

<pre><code class="python /rest-blog-unl-client/restunl/device.py">class Router(Device):
    ...

    def send_config(self, config):
        session = telnetlib.Telnet(self.url_ip, self.url_port)
        send_and_wait(session, '\r\n')
        result = send_and_wait(session, config)
        session.close()
        return result
</code></pre>

<p>The biggest problem is that Nodes, when started, take some time to boot before we can access the CLI prompt. To overcome that I had to implement a dirty hack in a form of <code>send_and_wait()</code> helper function that simulates pressing the <code>Enter</code> button every 0.1 second until it sees a CLI prompt (either <code>&gt;</code> or <code>#</code>).</p>

<pre><code class="python /rest-blog-unl-client/restunl/helper.py">
def send_and_wait(session, text):
        session.read_very_eager()
        result = ''
        session.write(text)
        while not any(stop_char in result[-3:] for stop_char in ['&gt;', '#']):
            session.write('\r\n')
            result += session.read_very_eager()
            time.sleep(0.1)
        return result
</code></pre>

<p>Let&rsquo;s hope that UNL team will implement config import soon so that we can get rid of this kludgy workaround.</p>

<h2>Extending our sample app</h2>

<p>At this stage we&rsquo;ve got all the code to finish our sample app. The goal is to create and configure the following 3-node topology:</p>

<p><img class="centre" src="/images/rest-sample-app.png" title="REST SDK SAMPLE TOPO" ></p>

<p>We&rsquo;ll assume that all configs will be stored as text files under the <code>./config</code> directory and will have device names as their filename. A helper function <code>read_file</code> will read the contents of a configuration text file into a Python string.</p>

<pre><code class="python">
TOPOLOGY = {('R1', 'Ethernet0/0'): ('R2', 'Ethernet0/0'),
            ('R2', 'Ethernet0/1'): ('R3', 'Ethernet0/0'),
            ('R1', 'Ethernet0/1'): ('R3', 'Ethernet0/1')}

def app_1():
    ...
    try:
        # Creating topology in UnetLab
        nodes = dict()
        for (a_name, a_intf), (b_name, b_intf) in TOPOLOGY.iteritems():
            # Create a mapping between a Node's name and an object
            if not a_name in nodes:
                nodes[a_name] = lab.create_node(Router(a_name))
                print("*** NODE {} CREATED".format(a_name))
            if not b_name in nodes:
                nodes[b_name] = lab.create_node(Router(b_name))
                print("*** NODE {} CREATED".format(b_name))
            # Extract Node objects using their names and connect them
            node_a = nodes[a_name]
            node_b = nodes[b_name]
            node_a.connect_node(a_intf, node_b, b_intf)
            print("*** NODES {0} and {1} ARE CONNECTED".format(a_name, b_name))
        print("*** TOPOLOGY IS BUILT")
        lab.start_all_nodes()
        print("*** NODES STARTED")
        # Reading and pushing configuration
        for node_name in nodes:
            conf = read_file('..\\config\\{}.txt'.format(node_name))
            nodes[node_name].configure(conf)
            print("*** NODE {} CONFIGURED".format(node_name))
        raw_input('PRESS ANY KEY TO STOP THE LAB')
    except Exception as e:
        print("*** APP FAILED : {}".format(e))
    finally:
        print("*** CLEANING UP THE LAB")
        lab.cleanup()
        unl.delete_lab(LAB_NAME)
</code></pre>

<p>When you run this app for the first time, the lab with 3 nodes will be spun up and configured. When you get to the <code>PRESS ANY KEY</code> prompt you can login into Web GUI and navigate to lab <code>test_1</code> and validate that all configs have been pushed and devices can ping each other&rsquo;s loopbacks.</p>

<h2>Source code</h2>

<p>All code from this post can be found in my <a href="https://github.com/networkop/rest-blog-unl-client/tree/2e847b8a809a1c9c4c0962b61c1c72325a405090">public repository on Github</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[REST for Network Engineers Part 2 - Basic Operations With UnetLab]]></title>
    <link href="http://networkop.github.io/blog/2016/01/06/rest-basic-operations/"/>
    <updated>2016-01-06T00:00:00-08:00</updated>
    <id>http://networkop.github.io/blog/2016/01/06/rest-basic-operations</id>
    <content type="html"><![CDATA[<p>In this post I&rsquo;ll show how to build REST SDK to authenticate, create labs and nodes in <a href="http://www.unetlab.com/">UnetLab</a>. I&rsquo;ll briefly cover the difference between composition and inheritance design patterns and demonstrate how to use test-driven development.</p>

<!--more-->


<h2>REST SDK Design</h2>

<p>As it is with networks, design is a very crucial part of programming. I won&rsquo;t pretend to be an expert in that field and merely present the way I&rsquo;ve built REST SDK. Fortunately, a lot of design will mimic the objects and their relationship on the server side. I&rsquo;ll slightly enhance it to improve code re-use and portability. Here are the basic objects:</p>

<ol>
<li>RestServer - implements basic application-agnostic HTTP CRUD logic</li>
<li>UnlServer - an extension of a RestServer with specific authentication method (cookie-based) and several additional methods</li>
<li>Device - an instance of a network device with specific attributes like type, image name, number of CPUs</li>
<li>UnlLab - a lab instance existing inside a UnlServer</li>
<li>UnlNode - a node instance existing inside a UnlLab</li>
<li>UnlNet - a network instance also existing inside a UnlLab object</li>
</ol>


<p>All these objects and their relationships are depicted on the following simplified <abbr title="Unified Modeling Language">UML</abbr> diagram. If you&rsquo;re interested in what different connections mean you can read <a href="http://www.codeproject.com/Articles/618/OOP-and-UML">this guide</a>.</p>

<p><img class="centre" src="/images/rest-oop-design.png" title="REST SDK UML Diagram" ></p>

<p>Here I&rsquo;ve used inheritance to <em>extend</em> RestServer functionality to make a UnlServer. This makes sense because UnlServer object will re-use a lot of the methods from the RestServer. I could have combined them in a single object but I&rsquo;ve decided to split the application-agnostic bit into a separate component to allow it to be re-used by other RESTful clients in the future.</p>

<p>The other objects are aggregated and interact through code composition, where Lab holds a pointer to the UnlServer where it was created, Nodes and Nets point to the Lab in which they live. Composition creates loose coupling between objects, while still allowing method delegation and code re-use.</p>

<p>For additional information about Composition vs Inheritance you can go <a href="http://learnpythonthehardway.org/book/ex44.html">here</a>, <a href="http://lgiordani.com/blog/2014/08/20/python-3-oop-part-3-delegation-composition-and-inheritance/">here</a> or <a href="http://python-textbok.readthedocs.org/en/latest/Object_Oriented_Programming.html#avoiding-inheritance">here</a>.</p>

<h2>REST SDK Implementation</h2>

<blockquote><p>Throughout this post I&rsquo;ll be omitting a lot of the non-important code. For full working code refer to the link at the end of this post.</p></blockquote>

<h3>RestServer implementation</h3>

<p>When RestServer object is created, <code>__init__()</code> function takes the server IP address and constructs a <code>base_url</code>, a common prefix for all API calls. The 4 CRUD actions are encoded into names of the methods implementing them, for example to send an Update one would need to call <code>.update_object()</code>. This convention will make the implementation of UnlServer a lot more readable. Each of the 4 CRUD methods call <code>_send_request()</code> with correct HTTP verb preset (the leading underscore means that this method is private and should only be called from within the RestServer class).</p>

<pre><code class="python /rest-blog-unl-client/restunl/server.py">class RestServer(object):

    def __init__(self, address):
        self.cookies = None
        self.base_url = '/'.join(['http:/', address, 'api'])

    def _send_request(self, method, path, data=None):
        response = None
        url = self.base_url + path
        try:
            response = requests.request(method, url,  json=data, cookies=self.cookies)
        except requests.exceptions.RequestException as e:
            print('*** Error calling %s: %s', url, e.message)
        return response

    def get_object(self, api_call, data=None):
        return self._send_request('GET', api_call, data)

    def add_object(self, api_call, data=None):
        return self._send_request('POST', api_call, data)

    def update_object(self, api_call, data=None):
        return self._send_request('PUT', api_call, data)

    def del_object(self, api_call, data=None):
        return self._send_request('DELETE', api_call, data)

    def set_cookies(self, cookie):
        self.cookies = cookie
</code></pre>

<p>At this stage RestServer does very simple exception and no HTTP response error handling. I&rsquo;ll show how to extend it to do authentication error handling in the future posts.</p>

<h3>UnlServer implementation</h3>

<p>At the very top of the <code>unetlab.py</code> file we have a <code>REST_SCHEMA</code> global variable providing mapping between actions and their respective <a href="http://www.unetlab.com/2015/09/using-unetlab-apis/">API calls</a>. This improves code readability (at least to me) and makes future upgrades to API easier to implement.<br/>
UnlServer class is extending the functionality of a RestServer by implementing UNetLab-specific methods. For example, <code>login()</code> sends username and password using the <code>add_object()</code> method of the parent class and sets the cookies extracted from the response to allow all subsequent methods to be authenticated.</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">REST_SCHEMA = {
    'login': '/auth/login',
    'logout': '/auth/logout',
    'status': '/status',
    'list_templates': '/list/templates/'
}

class UnlServer(RestServer):

    def __init__(self, address):
        super(UnlServer, self).__init__(address)

    def login(self, user, pwd):
        api_call = REST_SCHEMA['login']
        payload = {
            "username": user,
            "password": pwd
        }
        resp = self.add_object(api_call, data=payload)
        self.set_cookies(resp.cookies)
        return resp

    def logout(self):
        api_call = REST_SCHEMA['logout']
        resp = self.get_object(api_call)
        return resp

    def get_status(self):
        api_call = REST_SCHEMA['status']
        resp = self.get_object(api_call)
        return resp

    def get_templates(self):
        api_call = REST_SCHEMA['list_templates']
        resp = self.get_object(api_call)
        return resp
</code></pre>

<p>As you can see all methods follow the same pattern:</p>

<ol>
<li>Extract an API url from <code>REST_SCHEMA</code> global variable</li>
<li>Send a request using one of the 4 CRUD methods of the parent RestServer class</li>
<li>Return the response</li>
</ol>


<p>Now let&rsquo;s see how we can use TDD approach to build out the rest of the code.</p>

<h2>Test-driven development</h2>

<p>The easiest way to test RESTful application is by observing the status code of the returned HTTP response. If it is 200 or 201 then it can be considered successful. The biggest challenge is to make sure each test case is independent from one another. One option is to include all the code required by a test case inside the function that implements it. This, however, may lead to long and unwieldy spaghetti-code and breaks the <abbr title="Do Not Repeat Yourself">DRY</abbr> principle.<br/>
To help avoid that, TDD frameworks often have <code>fixtures</code> - functions that are run before and after every test case, designed to setup and cleanup the test environment. In our case we can use fixtures to login before each test case is run and logoff after it&rsquo;s finished. Let&rsquo;s see how we can use Python&rsquo;s built-in <a href="https://docs.python.org/2/library/unittest.html">unittest</a> framework to drive the REST SDK development process.<br/>
First let&rsquo;s define our base class <code>UnlTests</code> who&rsquo;s sole purpose will be to implement authentication fixtures. All the test cases will go into child classes that can either reuse and extend these fixtures. This is how test cases for the already existing code look like:</p>

<pre><code class="python /rest-blog-unl-client/tests/test_unl.py">
class UnlTests(unittest.TestCase):

    def setUp(self):
        self.unl = UnlServer(UNETLAB_ADDRESS)
        resp = self.unl.login(USERNAME, PASSWORD)
        self.assertEqual(200, resp.status_code)

    def tearDown(self):
        resp = self.unl.logout()
        self.assertEqual(200, resp.status_code)

class BasicUnlTests(UnlTests):

    def test_status(self):
        resp = self.unl.get_status()
        self.assertEqual(200, resp.status_code)

    def test_templates(self):
        resp = self.unl.get_templates()
        self.assertEqual(200, resp.status_code)
</code></pre>

<p>At this point if you add all the necessary import statements and populate global variables with correct IP addresses and credentials all tests should pass. Now let&rsquo;s add another test case to retrieve <a href="http://www.unetlab.com/2015/09/using-unetlab-apis/#toc2">user information</a> from UNL:</p>

<pre><code class="python /rest-blog-unl-client/tests/test_unl.py">
class BasicUnlTests(UnlTests):
    ...

    def test_user_info(self):
        resp = self.unl.get_user_info()
        self.assertEqual(200, resp.status_code)
</code></pre>

<p>Rerun the tests and watch the last one fail saying <code>'UnlServer' object has no attribute 'get_user_info'</code>. Now let&rsquo;s go back to our UNL SDK code and add that attribute:</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">REST_SCHEMA = {
    ... ,
    'get_user_info': '/auth'
}

class UnetLab(RestServer):
    ...

    def get_user_info(self):
        api_call = REST_SCHEMA['get_user_info']
        resp = self.get_object(api_call)
        return resp
</code></pre>

<p>Rerun the <code>test_unl.py</code> now and watch all tests succeed again. The same iterative approach can be used to add any number of new methods at the same time making sure none of the existing functionality is affected.<br/>
Note that these are very simple tests and they only verify the response code and not its contents. The better approach would be to look inside the payload and verify, for example, that username is <code>admin</code>.</p>

<h3>UnlLab and UnlNode implementation</h3>

<p>Now let&rsquo;s revert back to normal coding style for a second and create classes for Labs and Nodes. As per the design, these should be separate objects but they should contain a pointer to the context in which they exist. Therefore, it makes sense to instantiate a Lab inside a UnlServer, a Node inside a Lab and pass in the <code>self</code> (UnlServer or Lab) as an argument. For example, here is how a lab will be created:</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">REST_SCHEMA = {
    ... ,
    'create_lab': '/labs'
}

class UnlServer(RestServer):
    ...

    def create_lab(self, name):
        return UnlLab(self, name)

class UnlLab(object):

    def __init__(self, unl, name):
        api_call = REST_SCHEMA['create_lab']
        payload = {
           "path": "/",
           "name": name,
           "version": "1"
        }
        self.name = name
        self.unl = unl
        self.resp = self.unl.add_object(api_call, data=payload)
</code></pre>

<p>So to create a Lab we need to issue a <code>.create_lab()</code> call on UnlServer object and give it a labname. That function will return a new Lab object with the following attributes preset:</p>

<ul>
<li>Lab name - <code>self.name</code></li>
<li>UnlServer that created it - <code>self.unl</code></li>
<li>HTTP response returned by the server after the Create CRUD action - <code>self.resp</code></li>
</ul>


<p>The latter can be used to check if the creation was successful (and potentially throw an error if it wasn&rsquo;t). The structure of the payload can be found in <a href="http://www.unetlab.com/2015/09/using-unetlab-apis/#toc30">API docs</a>.</p>

<p>Nodes will be created in a similar way with a little exception. Apart from the name, Node also needs to know about the particulars of the device it will represent (like device type, image name etc.). That&rsquo;s where Device class comes in. The implementation details are very easy and can be found on <a href="https://github.com/networkop/rest-blog-unl-client/blob/c72f7bdc11427ac5efe9ec18401f0d63c57221ba/restunl/device.py">Github</a> so I won&rsquo;t provide them here. The only function of a Device at this stage is to create a dictionary that can be used as a payload in <code>create_node</code> <a href="http://www.unetlab.com/2015/09/using-unetlab-apis/#toc34">API request</a>.</p>

<pre><code class="python /rest-blog-unl-client/restunl/unetlab.py">REST_SCHEMA = {
    ... ,
    'create_node': '/labs/{lab_name}/nodes',
}

class UnlLab(object):
    ...

    def create_node(self, device):
        return UnlNode(self, device)

class UnlNode(object):

    def __init__(self, lab, device):
        self.unl = lab.unl
        self.lab = lab
        self.device = device
        api_call = REST_SCHEMA['create_node']
        api_url = api_call.format(api_call, lab_name=append_unl(self.lab.name))
        payload = self.device.to_json()
        self.resp = self.unl.add_object(api_url, data=payload)
</code></pre>

<p>Take a quick look at how the <code>api_url</code> is created. We&rsquo;re using <code>.format()</code> method (built-into <code>string</code> module) to substitute a named variable <code>{format}</code> with the actual name of the lab (<code>self.lab.name</code>). That labname gets appended with an extension by a helper function <code>append_unl</code>. That helper function, along with the others we&rsquo;ll define in the future, can also be found on <a href="https://github.com/networkop/rest-blog-unl-client/blob/c72f7bdc11427ac5efe9ec18401f0d63c57221ba/restunl/helper.py">Github</a>.</p>

<h2>Back to TDD</h2>

<p>Let&rsquo;s use TDD again to add the last two actions we&rsquo;ll cover in this post.</p>

<ul>
<li>Get list of all Nodes</li>
<li>Delete a lab</li>
</ul>


<pre><code class="python /rest-blog-unl-client/tests/test_unl.py">class BasicUnlLabTest(UnlTests):

    def test_create_lab(self):     
        self.unl.delete_lab(LAB_NAME)
        resp = self.unl.create_lab(LAB_NAME).resp
        self.unl.delete_lab(LAB_NAME)
        self.assertEqual(200, resp.status_code)

    def test_delete_lab(self):
        self.unl.create_lab(LAB_NAME)
        resp = self.unl.delete_lab(LAB_NAME)
        self.assertEqual(200, resp.status_code)

    def test_get_nodes(self):
        lab = self.unl.create_lab(LAB_NAME)
        resp = lab.get_nodes()
        self.unl.delete_lab(LAB_NAME)
        self.assertEqual(200, resp.status_code)   
</code></pre>

<p>As a challenge, try implementing the SDK logic for the last two failing methods yourself using <a href="http://www.unetlab.com/2015/09/using-unetlab-apis/">UNL API</a> as a reference. You can always refer the the link at the end of the post if you run into any problems.</p>

<h2>Simple App</h2>

<p>So far we&rsquo;ve created and deleted objects with REST API but haven&rsquo;t seen the actual result. Let&rsquo;s start writing an app that we&rsquo;ll continue to expand in the next post. In this post we&rsquo;ll simply login and create a lab containing a single node.</p>

<pre><code class="python  /rest-blog-unl-client/samples/app-1.py">from restunl.unetlab import UnlServer
from restunl.device import Router

LAB_NAME = 'test_1'

def app_1():
    unl = UnlServer('192.168.247.20')
    unl.login('admin', 'unl')
    print ("*** CONNECTED TO UNL")
    lab = unl.create_lab(LAB_NAME)
    print ("*** CREATED LAB")
    node_1 = lab.create_node(Router('R1'))
    print ("*** CREATED NODE")

if __name__ == '__main__':
    app_1()
</code></pre>

<p>Run this once, then login the UNL web GUI and navigate to <code>test_1</code> lab. Examine how node <strong>R1</strong> is configured and compare it to the defaults set in a <a href="https://github.com/networkop/rest-blog-unl-client/blob/c72f7bdc11427ac5efe9ec18401f0d63c57221ba/restunl/device.py">Device module</a>.</p>

<h2>Source code</h2>

<p>All code from this post can be found in my <a href="https://github.com/networkop/rest-blog-unl-client/tree/c72f7bdc11427ac5efe9ec18401f0d63c57221ba">public repository on Github</a></p>
]]></content>
  </entry>
  
</feed>
