<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Automation | Network-oriented programming]]></title>
  <link href="http://networkop.github.io/blog/categories/automation/atom.xml" rel="self"/>
  <link href="http://networkop.github.io/"/>
  <updated>2016-05-05T21:16:04-07:00</updated>
  <id>http://networkop.github.io/</id>
  <author>
    <name><![CDATA[Michael Kashin]]></name>
    <email><![CDATA[mmkashin@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Network-CI Part 3 - OSPF to BGP Migration in Active/Standby DC]]></title>
    <link href="http://networkop.github.io/blog/2016/03/23/network-ci-demo-large/"/>
    <updated>2016-03-23T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/03/23/network-ci-demo-large</id>
    <content type="html"><![CDATA[<p>The final post in a series demonstrates how to use the <strong>network-ci</strong> tools to safely replace a core routing protocol inside a small Active/Standby Data Centre.</p>

<!--more-->


<h2>Current network overview</h2>

<p>Let&rsquo;s start by taking a high-level look at our DC network routing topology. The core routing protocol is OSPF, it is responsible for distributing routing information between the Core and WAN layers of the network. WAN layer consists of two MPLS L3VPN services running BGP as PE-CE protocol and two DMVPN Hubs running EIGRP. All WAN layer devices perform mutual redistribution between the respective WAN protocol and OSPF.</p>

<p><img class="center" src="/images/network-ci-dc-before.png" title="Current network topology" ></p>

<h2>Target network overview</h2>

<p>The task is to replace OSPF with BGP as the core routing protocol inside the Data Centre. There are many advantages to using BGP inside a DC, in our case they are:</p>

<ul>
<li>Enhanced traffic routing and filtering policies</li>
<li>Reduced number of redistribution points</li>
<li>Because Ivan Pepelnjak <a href="http://blog.ipspace.net/2016/02/using-bgp-in-data-center-fabrics.html">said so</a></li>
</ul>


<p>We&rsquo;re not going getting rid of OSPF completely, but rather reduce its function to a simple distribution of <em>internal</em> DC prefixes. BGP will be running on top of OSPF and distribute all the DC and WAN <em>summary</em> prefixes.</p>

<p><img class="center" src="/images/network-ci-dc-after.png" title="Target network topology" ></p>

<h2>Physical topology overview</h2>

<p>Now let&rsquo;s take a closer look at the network that we&rsquo;re going to emulate. All green devices on the left-hand side constitute the <strong>Active</strong> Data Centre, that is where all the traffic will flow under normal conditions. All green devices have red <strong>Standby</strong> counterparts. These devices will pick up the function of traffic forwarding in case their green peer becomes unavailable.</p>

<p><img class="center" src="/images/network-ci-dc-full.png" title="Full demo topology" ></p>

<p>When simulating a real-life network it&rsquo;s often impossible to fit an exact replica inside a network emulator. That&rsquo;s why using <strong>mock</strong> devices is a crucial part in every simulation. The function of a mock is to approximate a set of network devices. There&rsquo;s a number of mock devices on our diagram colour-coded in purple. These devices simulate the remaining parts of the network. For example, <strong>Cloud</strong> devices may represent <abbr title=" Top-Of-the-Rack">TOR</abbr> switches, while <strong>MPLS/DMVPN</strong> devices represent remote WAN sites. Normally these devices will have some made-up configuration that best reflects real life, but not necessarily a copy-paste from an existing network device.</p>

<p>It&rsquo;s also important to pick the right amount of mock devices to strike the balance between accuracy and complexity. For example, for WAN sites it may suffice to create one site per unique combination of WAN links to make sure WAN failover works as expected.</p>

<h2>Traffic flow definition</h2>

<p>Let&rsquo;s define how we would expect the traffic to flow through our network. Let&rsquo;s assume that we should always try to use MPLS links when possible and only use DMVPN when both MPLS links are down. This translates to the following order of WAN links' precedence:</p>

<ol>
<li>Primary MPLS link</li>
<li>Standby MPLS link</li>
<li>Primary DMVPN link</li>
<li>Standby DMVPN link</li>
</ol>


<p>Based on that we can create the following traffic flows definition for network working under normal conditions.</p>

<pre><code class="text /network/tests/traffic_flows.txt">1 Failed None
  From FW to MPLS-DMVPN via Primary-WAN, Primary-MPLS
  From FW to DMVPN-ONLY via Primary-CORE-SW, Primary-DMVPN
  From FW to MPLS-ONLY via Primary-WAN, Primary-MPLS
  From Cloud-1 to FW Loopback0 via Primary-CORE-SW
  From Cloud-2 to MPLS-DMVPN via Primary-WAN, Primary-MPLS
</code></pre>

<p>We expect all traffic to flow through active devices even when the path may be suboptimal, like it&rsquo;s the case with traffic from Cloud-2. Similarly, we can create traffic flows definitions for different failure conditions. The complete <a href="https://github.com/networkop/network-ci/blob/master/acme-large/network/tests/traffic_flows.txt">traffic flows definition file</a> contains 2 additional failure scenarios covering the outage of the primary MPLS link and a complete outage of the primary core switch.</p>

<h2>Workflow example</h2>

<p>This is how you would approach a project like this.</p>

<ol>
<li>Get a copy of network-ci <a href="http://networkop.github.io/blog/2016/02/25/network-ci-dev-setup/">VM</a></li>
<li>Get a local copy of network-ci <a href="https://github.com/networkop/network-ci/tree/master/acme-large">tools</a></li>
<li>Copy configuration from real-life devices into the <a href="https://github.com/networkop/network-ci/tree/master/acme-large/config">config directory</a></li>
<li>Add configuration files for mock devices to the same directory</li>
<li>Review the <a href="https://github.com/networkop/network-ci/blob/master/acme-large/network/topology.py">topology definition file</a> to make sure it reflects our physical diagram</li>
<li>Review the UNL <a href="https://github.com/networkop/network-ci/blob/master/acme-large/network/unetlab.yml">configuration file</a> to make sure it points to the correct IP address assigned to your network-ci VM</li>
<li>Kick-off topology build inside UNL by running <code>./0_built_topo.py</code> script</li>
<li>Verify that traffic flows as expected with <code>2_test.py</code> script</li>
<li>Start the real-time monitoring with <code>1_monitor.py</code> script</li>
<li>Implement required changes on individual devices (all required changes can be found in <a href="https://github.com/networkop/network-ci/blob/master/acme-large/network/ospf-bgp.txt">ospf-bgp.txt</a> file)</li>
<li>Make sure that the network still behaves as before by running <code>2_test.py</code> script</li>
<li>Destroy the topology in UNL by running <code>3_destroy_topo.py</code></li>
</ol>


<h2>Continuous Integration</h2>

<p>In the <a href="http://networkop.github.io/blog/2016/03/03/network-ci-demo-small/">previous post</a> I&rsquo;ve showed how to use Jenkins to setup the CI environment for a small demo network. The same method can be applied to setup the job for our small Data Centre. It is simply a matter of changing the directory name from <strong>acme-small</strong> to <strong>acme-large</strong> in the first build step.</p>

<h2>Source code</h2>

<p>All code from this and previous posts is available on <a href="https://github.com/networkop/network-ci/tree/master/acme-large">Github</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network-CI Part 2 - Small Network Demo]]></title>
    <link href="http://networkop.github.io/blog/2016/03/03/network-ci-demo-small/"/>
    <updated>2016-03-03T00:00:00-08:00</updated>
    <id>http://networkop.github.io/blog/2016/03/03/network-ci-demo-small</id>
    <content type="html"><![CDATA[<p>In this post I&rsquo;ll demonstrate how to use the network-ci tools to automate the build, test and upgrade of a small 4-node network topology.</p>

<!--more-->


<h2>Demo network overview</h2>

<p>The network consists of 4 nodes interconnected via point-to-point links and running EIGRP as a routing protocol.</p>

<p><img class="center" src="/images/ci-acme-small.jpg" title="Small demo topology" ></p>

<p>To create a local development environment you can clone my <a href="https://github.com/networkop/network-ci/tree/master/acme-small">repository</a> and reset it to work with your own Github account using <code>git remote set-url origin https://github.com/USERNAME/OTHERREPOSITORY.git</code> command.<br/>
Local development environment contains the following files describing the modelled topology:</p>

<ol>
<li>Configuration files for each node under the <code>./config</code> directory</li>
<li>Network topology in <code>./network/topology.py</code> modelled as a list of pairs of interconnected devices</li>
<li>UNetLab configuration file containing the IP address and username/passwords to access the server</li>
<li>Traffic flow test definitions under <code>./network/test</code> directory</li>
</ol>


<h2>Test definitions</h2>

<p>Traffic flow test file contains an ordered set of test scenarios that will be performed automatically. The following is an example that illustrates main capabilities of our test tools:</p>

<pre><code class="text ./network/tests/traffic_flows.txt">## Normal operations
1 Failed None
  From R1 to R3 via R2 or R4, R3
  From R2 to R3 via R3
  From R2 to R4 via R1 or R3

# Failed link between R1 and R2
2 Failed R1 Gig1/1, R2 Gig0/0
  From R1 to R2 via not R2, R3
  From R2 to R4 via not R1, R4
</code></pre>

<p>Each scenario starts with a <strong>failure definition</strong>. It could be either <em>None</em>, which represents normal network conditions, or it could contain a list of interfaces that need to be failed. Following the failure definition are the actual tests. On each line we define source, destination and the path we expect the traffic to take. Path definition (everything following the &lsquo;via&rsquo; keyword) contains an ordered set of nodes and can use simple boolean operators like <strong>or</strong> or <strong>not</strong>.</p>

<p>Ping flow definition file is a lot shorter and simply contains a list of source/destination pairs to run a ping test between. All ping tests will execute concurrently and issue only 2 pings, therefore we&rsquo;ll only be able to detect connectivity loss if it lasts for more than 4 seconds.</p>

<h2>Jenkins setup</h2>

<p>In the previous post we&rsquo;ve automatically built a VM with Jenkins and UNetLab pre-installed. Before we can start using Jenkins we need to install a Github plugin, which can be done very easily from Jenkins GUI. Power up your <strong>vm-network-ci</strong> and open Jenkins home page at <code>http://VM_IP:8080</code>. From there navigate to <strong>Manage Jenkins -> Manage Plugins -> Available</strong>, search for and install the <strong>GitHub plugin</strong>.</p>

<h2>A quick Jenkins intro</h2>

<p>Inside Jenkins, <strong>a job</strong> represents a set of tasks that need to be automated for a particular project. Each job first waits for a trigger, which can be either a manual or an automatic event. When triggered, it connects to Github repository, downloads all the code to a local machine and executes a set of build steps defined in this job. A very simple workflow would looks like this:</p>

<ol>
<li>Developer commits and pushes a change to a Github repository</li>
<li>Github notifies Jenkins server by sending an HTTP POST request</li>
<li>Jenkins identifies the job that needs to be run and clones Github repo into a local directory</li>
<li>It then goes through and executes a set of build steps defined for this job</li>
<li>At the end you can optionally configure Jenkins to update the status of the build as described <a href="http://stackoverflow.com/questions/14274293/show-current-state-of-jenkins-build-on-github-repo">here</a></li>
</ol>


<h2>Using Jenkins for network testing</h2>

<ol>
<li>From Jenkins home page click on <strong>create new jobs</strong> and create a <em>Freestyle project</em> called <strong>acme-small</strong>.</li>
<li>Select the <strong>Github project</strong> option and enter a url of your Github repository (in my case its <a href="https://github.com/networkop/network-ci">https://github.com/networkop/network-ci</a>).</li>
<li>Under <strong>Source Code Management</strong> select <em>Git</em> and enter the same repository URL.</li>
<li>Under <strong>Additional Behaviours</strong> add a <em>Polling ignores commits in certain paths</em>. <br/>
Since I&rsquo;m keeping multiple subprojects in the same Github repo, I need to make sure that this job is only triggered if commit affected a particular directory.</li>
<li>Under <strong>Included Regions</strong> add <code>acme-small/.*</code> to only trigger builds for changes made to <strong>acme-small</strong> directory.</li>
<li>Optionally you can specify the build triggers to either build periodically or wait for changes to be pushed to Github.</li>
<li><p>Under <strong>Build</strong> add a new build step with the following shell commands:</p>

<pre><code class="`bash"> export UNL_IP="unl_ip_address"
 export PYTHONUNBUFFERED=1
 cd acme-small
 chmod +x 0_built_topo.py
 chmod +x 2_test.py
 chmod +x 3_destroy_topo.py
 ./0_built_topo.py
 ./2_test.py
 ./3_destroy_topo.py
</code></pre>

<p>The first two env variables setup the UNL&rsquo;s IP address of and disable IO buffering so that we can see the output produced by our scripts in real time. The remaining steps simply execute the build, test and destroy scripts in order.</p></li>
<li><p>Save the job and click on the <strong>Build Now</strong> to trigger the build manually.</p></li>
<li>In the <strong>Build History</strong> pane click on the latest build number (should be #1) and go to <em>Console Output</em>.</li>
<li>Observe how Jenkins builds, tests and destroys our demo topology</li>
</ol>


<h2>Network upgrade workflow</h2>

<p>Now imagine that a new requirements has come in to make sure that traffic from R1 to R3&rsquo;s Gig0/1 does not traverse R4 and goes via R2 instead, only falling back to R4 when R1-R2 link is down. In the following video I&rsquo;ll show how to use network-ci tools locally to implement and test this traffic engineering requirement.</p>

<p><div class="embed-video-container"><iframe src="//www.youtube.com/embed/GLOG9KZzP90" allowfullscreen></iframe></div></p>

<h2>Coming up</h2>

<p>In the next post I&rsquo;ll show how to apply the same workflow to automate the build, test and ugprade of a large 14-node topology.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network-CI Part 1 - Automatically Building a VM With UNetLab and Jenkins]]></title>
    <link href="http://networkop.github.io/blog/2016/02/25/network-ci-dev-setup/"/>
    <updated>2016-02-25T00:00:00-08:00</updated>
    <id>http://networkop.github.io/blog/2016/02/25/network-ci-dev-setup</id>
    <content type="html"><![CDATA[<p>Traditionally, the first post in the series describes how to setup a development environment. This time I&rsquo;ll do it DevOps-style. I&rsquo;ll show how to use Packer to automatically create and configure a VM with UNetLab and Jenkins pre-installed.</p>

<!--more-->


<p><img class="center" src="/images/packer-unl-jenkins.png" title="Packer-UNL-Jenkins" ></p>

<h2>Packer intro</h2>

<p><a href="https://www.packer.io/">Packer</a> is a tool that can automatically create virtual machines for different hypervisors and cloud platforms. The goal is to produce identically configured VMs for either VirtualBox, VMWare, Amazon or Google clouds based on a single template file. If you&rsquo;re familiar with <a href="https://www.vagrantup.com/docs/">Vagrant</a>, then you can also use Packer to create custom Vagrant boxes. In our case, however, we&rsquo;re only concerned about VMWare since it&rsquo;s the only <a href="https://en.wikipedia.org/wiki/Hypervisor">type-2 hypervisor</a> that supports nested hardware virtualisation (e.g. Intel VT-x), a feature required by UNetLab to run some of the emulated images.</p>

<p>Packer builds VMs using a special template file. At the very least, this file describes how to:</p>

<ul>
<li><p>Build a VM</p></li>
<li><p>Provision and configure apps on a VM</p></li>
</ul>


<p>These two actions correspond to the <code>builders</code> and <code>provisioners</code> sections of the template file.</p>

<p>The <code>builders</code> section contains a set of instructions for a particular hypervisor or platform on how to build a VM. For example, it might contain the amount of  RAM, CPU and disk sizes, number and type of interfaces, OS boot instructions and so on.</p>

<p>The <code>provisioners</code> section contains a set of instructions to configure a VM. This section may be as simple as a list of shell scripts or may include a reference to Ansible playbook which will be executed after the VM is built.</p>

<p>You can find my Packer templates along with Ubuntu preseed and provisioner scripts in my <a href="https://github.com/networkop/packer-unl-jenkins">Gihub repository</a>. For those looking for deeper insights about how to build a packer template I can recommend an official Packer <a href="https://www.packer.io/intro/index.html">introduction docs</a>.</p>

<h2>Building a VM with Packer</h2>

<p>As I&rsquo;ve mentioned previously, I&rsquo;m using Windows as my primary development environment and VMWare Workstation as my hypervisor. Before you begin you also need to have <a href="https://www.packer.io/intro/getting-started/setup.html">Packer</a> and <a href="https://git-scm.com/download/win">git</a> installed.</p>

<pre><code class="winbatch 1. Clone git repository">git clone https://github.com/networkop/packer-unl-jenkins
cd packer-unl-jenkins
</code></pre>

<pre><code class="winbatch 2. Start the build">packer build vmware.json
</code></pre>

<p>With a bit of luck, approximately 30 minutes later you should have a fully configured VM inside your VMWare Workstation waiting to be powered on. These are some of the features of this new VM:</p>

<ul>
<li>4 GB of RAM, 20GB of disk space, 2 dual-core vCPUs</li>
<li>1 Host-only and 1 NAT ethernet interfaces both using DHCP</li>
<li>Jenkins and UNetLab installed</li>
<li>Git and Python PIP packages installed</li>
<li>Username/password are <code>unl/admin</code></li>
</ul>


<p>Once powered on, you should be able to navigate to UNetLab&rsquo;s home page at <code>http://vm_ip:80</code> and Jenkins' home page and <code>http://vm_ip:8080</code>, where <code>vm_ip</code> is the IP of your new VM.</p>

<h2>IOU images</h2>

<p>Unfortunately IOU images are not publicly available so you&rsquo;re gonna have to find them yourself, which shouldn&rsquo;t be too hard. You&rsquo;ll also need to generate a license file for these images which, again, I&rsquo;m not going to discuss in this blog, but I can guarantee that you won&rsquo;t have to look farther than the 1st page of Google search to find all your answers. These are the remaining steps that you need to do:</p>

<ol>
<li>Obtain L2 and L3 IOU images</li>
<li>Generate a license file</li>
<li>Follow <a href="http://www.unetlab.com/2014/11/adding-cisco-iouiol-images/">these instructions</a> to install those images on the UNetLab server</li>
</ol>


<h2>non-DevOps way</h2>

<p>In case you&rsquo;re struggling with Packer here are the list of steps to setup a similar VM manually:</p>

<ol>
<li><a href="http://www.ubuntu.com/download/server">Download</a> your favourite Ubuntu Server image. Recommended release at the time of writing is 14.04.4.</li>
<li>Create a VM with at least 4GB of RAM, VT-x support and boot it off the Ubuntu ISO image.</li>
<li>Following instructions <a href="http://www.unetlab.com/2015/08/installing-unetlab-on-a-physical-server/">install Ubuntu and UNetLab</a>.</li>
<li>Install Jenkins as described on <a href="https://wiki.jenkins-ci.org/display/JENKINS/Installing+Jenkins+on+Ubuntu">their wiki website</a></li>
<li>Install additional packages like git and pip. Refer to my Packer <a href="https://github.com/networkop/packer-unl-jenkins/blob/master/scripts/packages.sh">packages script</a> for commands.</li>
</ol>


<h2>Coming up</h2>

<p>In the next post I&rsquo;ll show how to setup Jenkins to do automatic network testing and verification.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network Continuous Integration and Delivery]]></title>
    <link href="http://networkop.github.io/blog/2016/02/19/network-ci-intro/"/>
    <updated>2016-02-19T00:00:00-08:00</updated>
    <id>http://networkop.github.io/blog/2016/02/19/network-ci-intro</id>
    <content type="html"><![CDATA[<p>In this series of posts I&rsquo;ll introduce the tools I&rsquo;ve been using for continuous network development and how they can be used with automation servers like Jenkins for network continuous integration and delivery.</p>

<!--more-->


<h2>CI/CD vs ITIL</h2>

<p>How do you implement changes in your network? In today&rsquo;s world there&rsquo;s 95% chance that you have to write up an <abbr title=" Request For Change">RFC</abbr>, submit it at least a week before the planned implementation date, go through at least one <abbr title=" Change Admission Board">CAB</abbr> meeting and only then, assuming it got approved, can you implement it. But the most important question is &lsquo;how do you test&rsquo;? Do you simply content yourself with a few pings or do you make sure all main routes are in place? And how often do you get a call the next morning from a very nervous client asking you to &lsquo;please have a look at the network performance issues&rsquo;?</p>

<p>Software developers have solved most of these problems for themselves. DevOps movement has brought forth ideas of Continuous Integration and Delivery (CI/CD) to streamline the change process and &lsquo;embrace&rsquo; the change rather than protect against it. But how applicable are those ideas to the networks of today? Can we simply rip and replace our CAB meetings with a Jenkins server and live happily ever after?  As always, things are getting difficult as you move down from Layer 7.</p>

<h2>Problem #1 - How to test</h2>

<p>Ever since the dawn of networking, the only tools that engineers could use for testing were traceroutes and pings. It&rsquo;s not necessarily bad since, after all, networks are supposed to be a simple packet transports and shouldn&rsquo;t be endowed with application-layer knowledge. Note that I&rsquo;m talking about traditional or, in SDN-era terms, &lsquo;underlay&rsquo; networks. The biggest problem with network testability is not the lack of test tools but rather lack of automation. For every ping and every traceroute we had to login a device, carefully craft the command including source interface names, VRFs and other various options and then interpret the output.<br/>
I have already explored the idea of automated network testing in my previous blog posts - <a href="http://networkop.github.io/blog/2015/06/15/simple-tdd-framework/">Building Network TDD framework</a> and <a href="http://networkop.github.io/blog/2015/07/17/tdd-quickstart/">Network TDD quickstart</a>. I even got lucky enough to get invited to one of the <a href="http://blog.ipspace.net/2015/11/test-driven-network-development-with.html">greatest networking podcasts</a> hosted by Ivan Pepelnjak.</p>

<h2>Problem #2 - Where to test</h2>

<p>Another big problem is the lack of testable network software. We&rsquo;ve only had IOU, vSRX and vEOS for the past 3-4 years and even now a lot of the real-world functionality remains unvirtualizable. However having those images is a lot better than not, even though some of them tend to crash and behave unreliably from time to time.</p>

<h2>Network CI</h2>

<p>Here I&rsquo;ve come to the actual gist of my post. I want to demonstrate the tools that I&rsquo;ve built and how I use them to automate a lot of the repetitive tasks to prepare for network deployments and upgrades. This is what these tools can do:</p>

<ul>
<li>Create a replica of almost any real-world network topology inside a network emulation environment</li>
<li>Apply configuration to all built devices</li>
<li>Verify real-time connectivity between nodes</li>
<li>Verify traffic flows under various failure conditions against pre-defined set rules</li>
<li>Shutdown and delete the network topology</li>
</ul>


<p>All these steps can be done automatically without making a single click in a GUI or entering a single command in a CLI. This is a sneak peak of what to expect later in the series:</p>

<p><div class="embed-video-container"><iframe src="//www.youtube.com/embed/jiZs0969RWI" allowfullscreen></iframe></div></p>

<p>Please don&rsquo;t judge me too harshly, this is my first experience with screencasts.</p>

<h2>Coming up</h2>

<p>In the following posts I&rsquo;ll show how to:</p>

<ul>
<li><a href="http://networkop.github.io/blog/2016/02/25/network-ci-dev-setup/">Setup continuous delivery environment with Jenkins and UNetLab</a></li>
<li><a href="http://networkop.github.io/blog/2016/03/03/network-ci-demo-small/">Create test cases to enable automatic testing by Jenkins server</a></li>
<li><a href="http://networkop.github.io/blog/2016/03/03/network-ci-demo-large/">Safely replace a core routing protocol inside an Active/Standby DC</a></li>
</ul>


<p>If that sounds interesting to you - stay tuned.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[REST API for Network Engineers]]></title>
    <link href="http://networkop.github.io/blog/2016/01/01/rest-for-neteng/"/>
    <updated>2016-01-01T00:00:00-08:00</updated>
    <id>http://networkop.github.io/blog/2016/01/01/rest-for-neteng</id>
    <content type="html"><![CDATA[<p>This is the first, introductory, post in a series dedicated to REST APIs for Network Engineers. In this post we&rsquo;ll learn what REST API is, what are the most common tools and ways to consume it. Later in the series I&rsquo;ll show how to build a REST client to control <a href="http://www.unetlab.com/">UnetLab</a>, a very popular network emulation environment.</p>

<!--more-->


<h2>Management interface evolution</h2>

<p>Since the early dawn of networking, devices have been configured through <abbr title="Virtual Terminal User Interface">VTY</abbr>s. The transport has evolved from telnet to ssh but the underlying rule still maintained that network is configured manually, device-by-device by a human administrator. It&rsquo;s obvious that this approach does not scale and is prone to human error, however it still remains the most prevalent method of network device configuration. <br/>
The first attempt to tackle these issues has been made in 1988 with the introduction of SNMP. The basic idea was to monitor and manage network devices using a strictly-defined data structures called <abbr title="Management Information Base">MIB</abbr>s. Unfortunately, due to several architectural issues and poor vendor implementation, SNMP has ended up being used mainly for basic monitoring tasks.<br/>
The idea of programmatic management of network devices later resulted in the creation of NETCONF protocol. Despite being almost 10 years old, this protocol is still not widely used due to limited vendor support, however things may improve as the <abbr title="Yet Another Next Generation">YANG</abbr>-based network configuration gains greater adoption.<br/>
Finally, with the advent of SDN, REST has become a new de-facto standard for network provisioning. It is supported by most of the latest products of all the major vendors. Let&rsquo;s take a closer look at what REST is and how it works.</p>

<h2>REpresentational State Transfer overview</h2>

<p>Technically speaking, REST is not a protocol but a <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">software architectural pattern</a>. To put it in simple words, it describes <strong>a way</strong> to exchange information rather than the structure of that information. Think of it as another transport (like telnet or ssh for VTY) for payloads of any format. Most commonly payload will get encoded as <abbr title="JavaScript Object Notation">JSON</abbr>, however the structure of JSON is not defined and is different for each application. This rather <em>weak</em> definition opens up huge opportunities for potential use cases:</p>

<ol>
<li><p>The most basic use case is a Web application with client retrieving and updating information on a server. A good example in networking world would be the native interfaces of Cisco ACI or VMWare NSX. Each time you create an object (a network or a tenant) in a Web GUI, your client sends a request to server&rsquo;s REST interface.</p></li>
<li><p>The more advanced use case is for communication between distributed software components over a network environment. This is how all management and control plane communications are designed inside OpenStack. For example, every time a new VM is created, <abbr title="Compute controller">Nova</abbr> sends a request to <abbr title="Network controller">Neutron</abbr>&rsquo;s REST API interface to allocate an IP address and a port for that VM.</p></li>
</ol>


<h2>REST under the hood</h2>

<p>REST is an API that allows clients to perform read/write operations on data stored on the server. REST uses HTTP to perform a set of actions commonly known as <strong>CRUD</strong>:</p>

<ul>
<li>Create</li>
<li>Read</li>
<li>Update</li>
<li>Delete</li>
</ul>


<p>Assuming we want to manipulate a &lsquo;device&rsquo; object on a server we can send an <code>HTTP GET</code> request to <code>/api/devices</code> and get a response with a payload containing a full list of known devices.<br/>
If we want to add a new device, we need to construct a payload with device attributes (e.g. IP address, Hostname) and send it attached to an <code>HTTP POST</code> request.<br/>
To update a device we need to send the full updated payload with <code>HTTP PUT</code> method.</p>

<p><img class="centre" src="/images/rest-crud.png" title="Basic REST actions" ></p>

<p>Note that both Update and Delete API calls refer to a specific number in url <code>/api/devices/{ID}</code>. That is a <abbr title="Universally Unique IDentifier">UUID</abbr> that server assigns to every new object and is returned in <code>Location</code> header of <code>201 Created</code> message sent in response to Create request.</p>

<h2>REST API Constraints</h2>

<p>For API to be considered <strong>RESTful</strong> it needs comply with a number of <a href="http://www.restapitutorial.com/lessons/whatisrest.html">formal constraints</a>. One of the most important constraint of all is the stateless nature of the interface. That means that no cookie or session information should be stored on the server, and every request is treated independently from every other request. The implication here is that all state needs to be maintained on the client and information needs to be cacheable in order to improve interface responsiveness.</p>

<p>It&rsquo;s worth noting that an API can still be considered RESTful even if a server <em>does</em> store a session cookie, however all requests must still be treated independently. The last point means there&rsquo;s no <em>configuration modes</em> like we&rsquo;ve seen in traditional CLIs where all commands issued in &ldquo;interface configuration mode&rdquo; will be applied to that specific interface. With RESTful API each request from a client must reference a full path to an object including its UUID within the system.</p>

<h2>Using REST</h2>

<h3>cURL</h3>

<p>cURL is a light-weight Linux command-line tool for transferring data. It&rsquo;s very simple to get started and very frequently used to make HTTP requests. For example, this is how you would issue a Read request to get a list of devices configured on an imaginary SDN controller:</p>

<pre><code class="bash  Get all devices using cURL">$ curl -X GET http://sdn-controller.org:8181/api/devices
</code></pre>

<p>Obviously this tool is very good for fast prototyping and troubleshooting but becomes too cumbersome for anything involving more than a few commands.</p>

<h3>Browser plugins</h3>

<p>There&rsquo;s plenty of GUI-based clients that can be added-on to most popular browsers. <a href="https://www.getpostman.com/">Postman</a> and <a href="https://addons.mozilla.org/en-us/firefox/addon/restclient/">RESTClient</a> are amongst the most popular ones. In addition to basic functionality, these plugins allow you to store and issue sequence of requests which can be very handy for live demonstrations. Unfortunately that&rsquo;s where most of the people stop learning about REST API and assume this is the intended way to interact with a server.</p>

<h3><abbr title="Software Development Kit">SDK</abbr></h3>

<p>SDK is a collection of tools and libraries that allow users to build a full-blown applications with complex internal logic. Instead of caring about the exact syntax of API calls, SDK libraries provide a simple programming interface. For example, this is how you would run the same command we did earlier with cURL:</p>

<pre><code class="python Get all devices using SDK">controller = sdk.Controller('sdn-controller.org', 8181)
response = controller.get_devices()
</code></pre>

<p>We first create an instance of a controller with specific hostname and port number and then issue a <code>.get_devices()</code> call on it to obtain the list of all known devices. SDK library will do all the dirty work constructing HTTP request in the background and return the parsed information from HTTP response. We can then use that information to perform some complex logic, like start only Router devices (but not switches and firewalls):</p>

<pre><code class="python Start all Routers using SDK">routers_started = [device.start() for device in response.get_payload() if device.type == 'Router']
</code></pre>

<p>As you can see, we can use all capabilities of a programming language to write a succinct and powerful code, something that would be extremely difficult to do with cURL or Postman. That&rsquo;s why the focus of this series of posts will be on the SDK, how to build one from scratch and how to use it.</p>

<h2>UNetLab REST SDK</h2>

<p>In the upcoming posts I&rsquo;ll show how to develop a REST SDK to control UnetLab. My final goal would be to be able to create and fully configure an arbitrary network topology. The whole series will be broken up into several posts going over:</p>

<ol>
<li><a href="http://networkop.github.io/blog/2016/01/03/dev-env-setup-rest/">UnetLab SDK development environment setup</a>.
I&rsquo;ll show how to setup PyCharm to work with UNL running in a hypervisor on a Windows machine.</li>
<li><a href="http://networkop.github.io/blog/2016/01/06/rest-basic-operations/">First steps with REST API for UNetLab</a>.
In this post I&rsquo;ll show how to work with HTTP and perform basic operations like login/logoff and object creation in UNL.</li>
<li><a href="http://networkop.github.io/blog/2016/01/17/rest-unl-advanced/">Advanced actions with UNetLab SDK</a>.
The final post will demonstrate how to push configuration to devices inside UNL. To wrap up I&rsquo;ll write a sample app to spin up and provision a three-node topology without making a single GUI click.</li>
</ol>

]]></content>
  </entry>
  
</feed>
