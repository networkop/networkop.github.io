<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Automation | Network-oriented programming]]></title>
  <link href="http://networkop.github.io/blog/categories/automation/atom.xml" rel="self"/>
  <link href="http://networkop.github.io/"/>
  <updated>2016-10-13T03:03:05-07:00</updated>
  <id>http://networkop.github.io/</id>
  <author>
    <name><![CDATA[Michael Kashin]]></name>
    <email><![CDATA[mmkashin@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Automating the Build of OpenStack Lab (Part 2)]]></title>
    <link href="http://networkop.github.io/blog/2016/09/09/os-lab-p2/"/>
    <updated>2016-09-09T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/09/09/os-lab-p2</id>
    <content type="html"><![CDATA[<p>In this post we&rsquo;ll use Chef, unnumbered BGP and Cumulus VX to build a massively scalable &ldquo;Lapukhov&rdquo; Leaf-Spine data centre.</p>

<!--more-->


<hr />

<p>In the <a href="http://networkop.github.io/blog/2016/08/26/os-lab-p1/">last post</a> we&rsquo;ve seen how to use Chef to automate the build of a 3-node OpenStack cloud. The only thing remaining is to build an underlay network supporting communication between the nodes, which is what we&rsquo;re going to do next. The build process will, again, be relatively simple and will include only a few manual steps, but before we get there let me go over some of the decisions and assumptions I&rsquo;ve made in my network design.</p>

<h2>High-level design</h2>

<p>The need to provide more bandwidth for East-West traffic has made the Clos Leaf-Spine architecture a de facto standard in any data centre network design. The use of virtual overlay networks has obviated the requirement to have a strict VLAN and IP numbering schemes in the underlay. The only requirement for the compute nodes now is to have any-to-any layer 3 connectivity. This is how the underlay network design has converged to a Layer 3 Leaf-Spine architecture.<br/>
The choice of a routing protocol is not so straight-forward. My fellow countryman Petr Lapukhov and co-authors of <a href="https://tools.ietf.org/html/draft-ietf-rtgwg-bgp-routi3ng-large-dc-11">RFC draft</a> claim that having a single routing protocol in your WAN and DC reduces complexity and makes interoperability and operations a lot easier. This draft presents some of the design principles that can be used to build a L3 data centre with BGP as the only routing protocol. In our lab we&rsquo;re going to implement a single &ldquo;cluster&rdquo; of the multi-tier topology proposed in that RFC.</p>

<p><img class="center" src="/images/os-lab-chef-full.png"></p>

<p>In order to help us build this in an automated and scalable way, we&rsquo;re going to use a relatively new feature called <strong>unnumbered BGP</strong>.</p>

<h2>Unnumbered BGP as a replacement for IGP</h2>

<p>As we all know, one of the main advantages of interior gateway protocols is the automatic discovery of adjacent routers which is accomplished with the help of link-local multicasts. On the other hand, BGP traditionally required you to explicitly define neighbor&rsquo;s IP address in order to establish a peering relationship with it. This is where IPv6 comes to the rescue. With the help of neighbor discovery protocol and router advertisement messages, it becomes possible to accurately determine the address of the peer BGP router on an intra-fabric link. The only question is how we would exchange IPv4 information over and IPv6-only BGP network.<br/>
<a href="https://tools.ietf.org/html/rfc5549">RFC 5549</a>, described an &ldquo;extended nexthop encoding capability&rdquo; which allows BGP to exchange routing updates with nexthops that don&rsquo;t belong to the address family of the advertised prefix. In plain English it means that BGP is now capable of advertising an IPV4 prefix with an IPv6 nexthop. This makes it possible to configure all transit links inside the Clos fabric with IPv6 link-local addresses and still maintain reachability between the edge IPv4 host networks. Since nexthop IPs will get updated at every hop, there is no need for an underlying IGP to distribute them between all BGP routers. What we see is, effectively, BGP <strong>absorbing</strong> the functions of an IGP protocol inside the data centre.</p>

<h2>Configuration example on Cumulus VX</h2>

<p>In order to implement BGP unnumbered on Cumulus Linux all you need to is:</p>

<ol>
<li>Enable IPv6 router advertisements on all transit links</li>
<li>Enable BGP on the same interfaces</li>
</ol>


<p>Example Quagga configuration snippet will look like this:</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>interface swp1
</span><span class='line'>  ipv6 nd ra-interval 5
</span><span class='line'>  no ipv6 nd suppress-ra&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;rouer bgp &lt;ASN&gt;
</span><span class='line'>  neighbor swp1 interface
</span><span class='line'>  neighbor swp1 external
</span></code></pre></td></tr></table></div></figure></p>

<p>As you can see, Cumulus simplifies it even more by allowing you to only specify the BGP peering type (external/internal) and learning the value of peer BGP AS dynamically from a neighbor.</p>

<h2>Design assumptions and caveats</h2>

<p>With all the above in mind, this is the list of decisions I&rsquo;ve made while building the fabric configuration:</p>

<ul>
<li>All switches inside the fabric will be running BGP peerings using <strong>IPv6 link-local</strong> addresses</li>
<li><strong>eBGP</strong> will be used throughout to simplify configuration automation (all peers will be external)</li>
<li>Each Leaf/Spine switch will have a <strong>unique IPv4 loopback</strong> address assigned for management purposes (ICMP, SSH)</li>
<li>On each Leaf switch <strong>all directly connected IPv4</strong> prefixes will get redistributed into BGP</li>
<li>BGP multipath rule will be &ldquo;relaxed&rdquo; to allow for different AS-PATHs. This is not used in our current topology but is required in an HA Leaf switch design (same IPv4 prefix will be advertised from two Leaf switches with different ASN)</li>
<li>Loop prevention on Leaf switches will also be &ldquo;relaxed&rdquo;. This, again, is not used in our single &ldquo;cluster&rdquo; topology, however it will allow same Leaf ASNs to be reused in a different cluster.</li>
</ul>


<h2>Implementation steps</h2>

<p>Picking up where we left off after the OpenStack node provisioning described in the <a href="http://networkop.github.io/blog/2016/08/26/os-lab-p1/">previous post</a></p>

<ol>
<li><p>Get the latest <a href="https://github.com/networkop/chef-unl-os">OpenStack lab cookbooks</a></p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> git clone &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://github.com/networkop/chef-unl-os.git&quot;</span>&gt;https://github.com/networkop/chef-unl-os.git&lt;/a&gt;
</span><span class='line'> <span class="nb">cd </span>chef-unl-os
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p><a href="https://cumulusnetworks.com/cumulus-vx/">Download</a> and import Cumulus VX image similar to how it&rsquo;s described <a href="http://www.unetlab.com/2015/06/adding-cisco-asav-images/">here</a>.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> /opt/unetlab/addons/qemu/cumulus-vx/hda.qcow2
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Build the topology inside UNL. Make sure that Node IDs inside UNL match the ones in <strong>chef-unl-os/environment/lab.rb</strong> file and that interfaces are connected as shown in the diagram below</p>

<p> <img class="center" src="/images/os-lab-unl.png"></p></li>
<li><p>Re-run UNL self-provisioning cookbook to create a <a href="https://github.com/networkop/chef-unl-os/blob/master/cookbooks/pxe/templates/ztp.erb">zero touch provisioning</a> file and update DHCP server configuration with static entries for the switches.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> chef-client -z -E lab -o pxe
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Cumulus <a href="https://docs.cumulusnetworks.com/display/DOCS/Zero+Touch+Provisioning+-+ZTP">ZTP</a> allows you to run a predefined script on the first boot of the operating system. In our case we inject a UNL VM&rsquo;s public key and enable passwordless <strong>sudo</strong> for cumulus user.</p></li>
<li><p>Kickoff Chef provisioning to bootstrap and configure the DC fabric.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> chef-client -z -E lab fabric.rb
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> This command instructs Chef provisioning to connect to each switch, download and install the Chef client and run a simple recipe to create quagga configuration file from a template.</p></li>
</ol>


<p>At the end of step 5 we should have a fully functional BGP-only fabric and all 3 compute nodes should be able to reach each other in at most 4 hops.</p>

<p><code>bash [root@controller-1 ~]# traceroute 10.0.0.4
traceroute to 10.0.0.4 (10.0.0.4), 30 hops max, 60 byte packets
 1  10.0.0.1 (10.0.0.1)  0.609 ms  0.589 ms  0.836 ms
 2  10.255.255.7 (10.255.255.7)  0.875 ms  2.957 ms  3.083 ms
 3  10.255.255.6 (10.255.255.6)  3.473 ms  5.486 ms  3.147 ms
 4  10.0.0.4 (10.0.0.4)  4.231 ms  4.159 ms  4.115 ms
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Automating the Build of OpenStack Lab (Part 1)]]></title>
    <link href="http://networkop.github.io/blog/2016/08/26/os-lab-p1/"/>
    <updated>2016-08-26T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/08/26/os-lab-p1</id>
    <content type="html"><![CDATA[<p>In this post we will explore what&rsquo;s required to perform a zero-touch deployment of an OpenStack cloud. We&rsquo;ll get a 3-node lab up and running inside UNetLab with just a few commands.</p>

<!--more-->


<hr />

<p>Now that I&rsquo;m finally beginning to settle down at my new place of residence I can start spending more time on research and blogging. I have left off right before I was about to start exploring the native OpenStack distributed virtual routing function. However as I&rsquo;d started rebuilding my OpenStack lab from scratch I realised that I was doing a lot of repetitive tasks which can be easily automated. Couple that with the fact that I needed to learn Chef for my new work and you&rsquo;ve got this blogpost describing a few Chef <a href="https://github.com/networkop/chef-unl-os.git">cookbooks</a> (similar to Ansible&rsquo;s playbook) automating all those manual steps described in my earlier blogposts <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">1</a> and <a href="http://networkop.github.io/blog/2016/04/18/os-unl-lab/">2</a>.<br/>
In addition to that in this post I&rsquo;ll show how to build a very simple OpenStack baremetal provisioner and installer. Some examples of production-grade baremetal provisioners are <a href="https://wiki.openstack.org/wiki/Ironic">Ironic</a>, <a href="http://crowbar.github.io/">Crowbar</a> and <a href="http://maas.io/">MAAS</a>. In our case we&rsquo;ll turn UNetLab VM into an <strong>undercloud</strong>, a server used to provision and deploy our OpenStack lab, an <strong>overcloud</strong>. To do that we&rsquo;ll first install and configure DHCP, TFTP and Apache servers to PXE-boot our UNL OpenStack nodes. Once all the nodes are bootstrapped, we&rsquo;ll use Chef to configure the server networking and kickoff the packstack OpenStack installer.</p>

<p><img class="center" src="/images/os-lab-chef.png"></p>

<p>In this post I&rsquo;ll try to use Chef recipes that I&rsquo;ve written as much as possible, therefore you won&rsquo;t see the actual configuration commands, e.g. how to configure Apache or DHCP servers. However I will try to describe everything that happens at each step and hopefully that will provide enough incentive for the curious to look into the Chef code and see how it&rsquo;s done. To help with the Chef code understanding let me start with a brief overview of what to look for in a cookbook.</p>

<h2>How to read a Chef cookbook (Optional)</h2>

<p>A cookbook directory (<strong>/cookbooks/[cookbook_name]</strong>) contains all its configuration scripts in <strong>/recipes</strong>. Each file inside a recipe contains a list of steps to be performed on a server. Each step is an operation (add/delete/update) on a <strong>resource</strong>. Here are some of the common Chef resources:</p>

<ul>
<li>Package - allows you to add, remove or update a package</li>
<li>Template - creates a file from an <strong>erb</strong>-formatted template</li>
<li>Execute - runs an ad-hoc CLI command</li>
</ul>


<p>Just these three basic resources allow you to do 95% of administrative tasks on any server. Most importantly they do it in platform-independent (any flavour of Linux) and idempotent (only make changes if current state is different from a desired state) way. Other directories you might want to explore are:</p>

<ul>
<li>/templates - contains all the <strong>erb</strong>-formatted templates</li>
<li>/attributes - contains recipe variables (file paths, urls etc.)</li>
<li>/files - contains the non-template files, i.e. files with static content</li>
</ul>


<h2>Bootstrapping the OpenStack nodes</h2>

<ol>
<li><p>If you haven&rsquo;t done it yet, download a copy of the <strong>UNetLab VM</strong> from the <a href="http://www.unetlab.com/">official website</a>. Set it up inside your hypervisor so that you can access Internet through the first interface <strong>pnet0</strong> (i.e. connect the first NIC of the VM to hypervisor&rsquo;s NAT interface). Make sure the VM has got at least 6GB of RAM and VT-x support enabled for nested virtualization.</p></li>
<li><p>Follow the official <a href="https://downloads.chef.io/chef-dk/">installation instructions</a> to <strong>install Chef Development Kit</strong> inside UNetLab VM.</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>wget &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://packages.chef.io/stable/ubuntu/12.04/chefdk_0.16.28-1_amd64.deb&quot;</span>&gt;https://packages.chef.io/stable/ubuntu/12.04/chefdk_0.16.28-1_amd64.deb&lt;/a&gt;
</span><span class='line'>dpkg -i chefdk_0.16.28-1_amd64.deb
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p><strong>Install git</strong> and clone <a href="https://github.com/networkop/chef-unl-os.git">chef cookbooks</a>.</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>apt-get -y update
</span><span class='line'>apt-get -y install git
</span><span class='line'>git clone &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://github.com/networkop/chef-unl-os.git&quot;</span>&gt;https://github.com/networkop/chef-unl-os.git&lt;/a&gt;
</span><span class='line'><span class="nb">cd </span>chef-unl-os
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Examine the lab <strong>environment settings</strong> to see what values are going to be used. You can modify that file to your liking.</p>

<blockquote><p>Note that the OpenStack node IDs (keys of <em>os_lab</em> hash) MUST have one to one correspondence with the UNL node IDs which will be created at step 5</p></blockquote>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>cat environment/lab.rb
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Run Chef against a local server to setup the <strong>baremetal provisioner</strong>. This step installs and configures DHCP, TFTP and Apache servers. It also creates all the necessary PXE-boot and kickstart files based on our environment settings. Note that a part of the process is the download of a 700MB CentOS image so it might take a while to complete.</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>chef-client -z -E lab -o pxe
</span></code></pre></td></tr></table></div></figure></p>

<p>At the start of the PXE-boot process, DCHP server sends an OFFER which, along with the standard IP information, includes the name of the PXE boot image and the IP address of TFTP server where to get it from. A server loads this image and then searches the TFTP server for the boot configuration file which tells it what kernel to load and where to get a kickstart file. Both kickstart and the actual installation files are accessed via HTTP and served by the same Apache server that runs UNL GUI.</p></li>
<li><p>From <strong>UNL GUI</strong> create a new lab, add 3 OpenStack nodes and connect them all to <strong>pnet10</strong> interface as described in <a href="http://www.unetlab.com/2014/11/using-cloud-devices/">this guide</a>. Note that the <strong>pnet10</strong> interface has already been created by Chef so you don&rsquo;t have to re-create it again.</p>

<blockquote><p>Make sure that the UNL node IDs match the ones defined in the environment setting file</p></blockquote></li>
<li><p>Fire-up the nodes and watch them being bootstrapped by our UNL VM.</p></li>
</ol>


<h2>Server provisioning</h2>

<p>Next step is to configure the server networking and kickoff the OpenStack installer. These steps will also be done with a single command:</p>

<p>   <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>   chef-client -z -E lab lab.rb
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>
The first part of this script will connect to each prospective OpenStack node and setup its network interfaces and hostnames. The second part of this script will generate a packstack answer file and modify its settings to exclude some of the components we&rsquo;re not going to use (like Nagios, Ceph and Ceilometer). Have a look at <strong>cookbooks/packstack/recipe/default.rb</strong> for the list of modifications. The final step is a command to kickoff the packstack installer which will use another configuration management system, Puppet, to install and configure OpenStack according to the provided answer file.</p>

<p>At the end of these steps you should have a fully functional 3-node OpenStack environment.</p>

<h2>To be continued&hellip;</h2>

<p>This is a part of a 2-post series. In the next post we&rsquo;ll look into how to use the same tools to perform the baremetal provisioning of our physical underlay network.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network-CI Part 3 - OSPF to BGP Migration in Active/Standby DC]]></title>
    <link href="http://networkop.github.io/blog/2016/03/23/network-ci-demo-large/"/>
    <updated>2016-03-23T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/03/23/network-ci-demo-large</id>
    <content type="html"><![CDATA[<p>The final post in a series demonstrates how to use the <strong>network-ci</strong> tools to safely replace a core routing protocol inside a small Active/Standby Data Centre.</p>

<!--more-->


<h2>Current network overview</h2>

<p>Let&rsquo;s start by taking a high-level look at our DC network routing topology. The core routing protocol is OSPF, it is responsible for distributing routing information between the Core and WAN layers of the network. WAN layer consists of two MPLS L3VPN services running BGP as PE-CE protocol and two DMVPN Hubs running EIGRP. All WAN layer devices perform mutual redistribution between the respective WAN protocol and OSPF.</p>

<p><img class="center" src="/images/network-ci-dc-before.png" title="Current network topology" ></p>

<h2>Target network overview</h2>

<p>The task is to replace OSPF with BGP as the core routing protocol inside the Data Centre. There are many advantages to using BGP inside a DC, in our case they are:</p>

<ul>
<li>Enhanced traffic routing and filtering policies</li>
<li>Reduced number of redistribution points</li>
<li>Because Ivan Pepelnjak <a href="http://blog.ipspace.net/2016/02/using-bgp-in-data-center-fabrics.html">said so</a></li>
</ul>


<p>We&rsquo;re not going getting rid of OSPF completely, but rather reduce its function to a simple distribution of <em>internal</em> DC prefixes. BGP will be running on top of OSPF and distribute all the DC and WAN <em>summary</em> prefixes.</p>

<p><img class="center" src="/images/network-ci-dc-after.png" title="Target network topology" ></p>

<h2>Physical topology overview</h2>

<p>Now let&rsquo;s take a closer look at the network that we&rsquo;re going to emulate. All green devices on the left-hand side constitute the <strong>Active</strong> Data Centre, that is where all the traffic will flow under normal conditions. All green devices have red <strong>Standby</strong> counterparts. These devices will pick up the function of traffic forwarding in case their green peer becomes unavailable.</p>

<p><img class="center" src="/images/network-ci-dc-full.png" title="Full demo topology" ></p>

<p>When simulating a real-life network it&rsquo;s often impossible to fit an exact replica inside a network emulator. That&rsquo;s why using <strong>mock</strong> devices is a crucial part in every simulation. The function of a mock is to approximate a set of network devices. There&rsquo;s a number of mock devices on our diagram colour-coded in purple. These devices simulate the remaining parts of the network. For example, <strong>Cloud</strong> devices may represent <abbr title=" Top-Of-the-Rack">TOR</abbr> switches, while <strong>MPLS/DMVPN</strong> devices represent remote WAN sites. Normally these devices will have some made-up configuration that best reflects real life, but not necessarily a copy-paste from an existing network device.</p>

<p>It&rsquo;s also important to pick the right amount of mock devices to strike the balance between accuracy and complexity. For example, for WAN sites it may suffice to create one site per unique combination of WAN links to make sure WAN failover works as expected.</p>

<h2>Traffic flow definition</h2>

<p>Let&rsquo;s define how we would expect the traffic to flow through our network. Let&rsquo;s assume that we should always try to use MPLS links when possible and only use DMVPN when both MPLS links are down. This translates to the following order of WAN links' precedence:</p>

<ol>
<li>Primary MPLS link</li>
<li>Standby MPLS link</li>
<li>Primary DMVPN link</li>
<li>Standby DMVPN link</li>
</ol>


<p>Based on that we can create the following traffic flows definition for network working under normal conditions.</p>

<pre><code class="text /network/tests/traffic_flows.txt">1 Failed None
  From FW to MPLS-DMVPN via Primary-WAN, Primary-MPLS
  From FW to DMVPN-ONLY via Primary-CORE-SW, Primary-DMVPN
  From FW to MPLS-ONLY via Primary-WAN, Primary-MPLS
  From Cloud-1 to FW Loopback0 via Primary-CORE-SW
  From Cloud-2 to MPLS-DMVPN via Primary-WAN, Primary-MPLS
</code></pre>

<p>We expect all traffic to flow through active devices even when the path may be suboptimal, like it&rsquo;s the case with traffic from Cloud-2. Similarly, we can create traffic flows definitions for different failure conditions. The complete <a href="https://github.com/networkop/network-ci/blob/master/acme-large/network/tests/traffic_flows.txt">traffic flows definition file</a> contains 2 additional failure scenarios covering the outage of the primary MPLS link and a complete outage of the primary core switch.</p>

<h2>Workflow example</h2>

<p>This is how you would approach a project like this.</p>

<ol>
<li>Get a copy of network-ci <a href="http://networkop.github.io/blog/2016/02/25/network-ci-dev-setup/">VM</a></li>
<li>Get a local copy of network-ci <a href="https://github.com/networkop/network-ci/tree/master/acme-large">tools</a></li>
<li>Copy configuration from real-life devices into the <a href="https://github.com/networkop/network-ci/tree/master/acme-large/config">config directory</a></li>
<li>Add configuration files for mock devices to the same directory</li>
<li>Review the <a href="https://github.com/networkop/network-ci/blob/master/acme-large/network/topology.py">topology definition file</a> to make sure it reflects our physical diagram</li>
<li>Review the UNL <a href="https://github.com/networkop/network-ci/blob/master/acme-large/network/unetlab.yml">configuration file</a> to make sure it points to the correct IP address assigned to your network-ci VM</li>
<li>Kick-off topology build inside UNL by running <code>./0_built_topo.py</code> script</li>
<li>Verify that traffic flows as expected with <code>2_test.py</code> script</li>
<li>Start the real-time monitoring with <code>1_monitor.py</code> script</li>
<li>Implement required changes on individual devices (all required changes can be found in <a href="https://github.com/networkop/network-ci/blob/master/acme-large/network/ospf-bgp.txt">ospf-bgp.txt</a> file)</li>
<li>Make sure that the network still behaves as before by running <code>2_test.py</code> script</li>
<li>Destroy the topology in UNL by running <code>3_destroy_topo.py</code></li>
</ol>


<h2>Continuous Integration</h2>

<p>In the <a href="http://networkop.github.io/blog/2016/03/03/network-ci-demo-small/">previous post</a> I&rsquo;ve showed how to use Jenkins to setup the CI environment for a small demo network. The same method can be applied to setup the job for our small Data Centre. It is simply a matter of changing the directory name from <strong>acme-small</strong> to <strong>acme-large</strong> in the first build step.</p>

<h2>Source code</h2>

<p>All code from this and previous posts is available on <a href="https://github.com/networkop/network-ci/tree/master/acme-large">Github</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network-CI Part 2 - Small Network Demo]]></title>
    <link href="http://networkop.github.io/blog/2016/03/03/network-ci-demo-small/"/>
    <updated>2016-03-03T00:00:00-08:00</updated>
    <id>http://networkop.github.io/blog/2016/03/03/network-ci-demo-small</id>
    <content type="html"><![CDATA[<p>In this post I&rsquo;ll demonstrate how to use the network-ci tools to automate the build, test and upgrade of a small 4-node network topology.</p>

<!--more-->


<h2>Demo network overview</h2>

<p>The network consists of 4 nodes interconnected via point-to-point links and running EIGRP as a routing protocol.</p>

<p><img class="center" src="/images/ci-acme-small.jpg" title="Small demo topology" ></p>

<p>To create a local development environment you can clone my <a href="https://github.com/networkop/network-ci/tree/master/acme-small">repository</a> and reset it to work with your own Github account using <code>git remote set-url origin https://github.com/USERNAME/OTHERREPOSITORY.git</code> command.<br/>
Local development environment contains the following files describing the modelled topology:</p>

<ol>
<li>Configuration files for each node under the <code>./config</code> directory</li>
<li>Network topology in <code>./network/topology.py</code> modelled as a list of pairs of interconnected devices</li>
<li>UNetLab configuration file containing the IP address and username/passwords to access the server</li>
<li>Traffic flow test definitions under <code>./network/test</code> directory</li>
</ol>


<h2>Test definitions</h2>

<p>Traffic flow test file contains an ordered set of test scenarios that will be performed automatically. The following is an example that illustrates main capabilities of our test tools:</p>

<pre><code class="text ./network/tests/traffic_flows.txt">## Normal operations
1 Failed None
  From R1 to R3 via R2 or R4, R3
  From R2 to R3 via R3
  From R2 to R4 via R1 or R3

# Failed link between R1 and R2
2 Failed R1 Gig1/1, R2 Gig0/0
  From R1 to R2 via not R2, R3
  From R2 to R4 via not R1, R4
</code></pre>

<p>Each scenario starts with a <strong>failure definition</strong>. It could be either <em>None</em>, which represents normal network conditions, or it could contain a list of interfaces that need to be failed. Following the failure definition are the actual tests. On each line we define source, destination and the path we expect the traffic to take. Path definition (everything following the &lsquo;via&rsquo; keyword) contains an ordered set of nodes and can use simple boolean operators like <strong>or</strong> or <strong>not</strong>.</p>

<p>Ping flow definition file is a lot shorter and simply contains a list of source/destination pairs to run a ping test between. All ping tests will execute concurrently and issue only 2 pings, therefore we&rsquo;ll only be able to detect connectivity loss if it lasts for more than 4 seconds.</p>

<h2>Jenkins setup</h2>

<p>In the previous post we&rsquo;ve automatically built a VM with Jenkins and UNetLab pre-installed. Before we can start using Jenkins we need to install a Github plugin, which can be done very easily from Jenkins GUI. Power up your <strong>vm-network-ci</strong> and open Jenkins home page at <code>http://VM_IP:8080</code>. From there navigate to <strong>Manage Jenkins -> Manage Plugins -> Available</strong>, search for and install the <strong>GitHub plugin</strong>.</p>

<h2>A quick Jenkins intro</h2>

<p>Inside Jenkins, <strong>a job</strong> represents a set of tasks that need to be automated for a particular project. Each job first waits for a trigger, which can be either a manual or an automatic event. When triggered, it connects to Github repository, downloads all the code to a local machine and executes a set of build steps defined in this job. A very simple workflow would looks like this:</p>

<ol>
<li>Developer commits and pushes a change to a Github repository</li>
<li>Github notifies Jenkins server by sending an HTTP POST request</li>
<li>Jenkins identifies the job that needs to be run and clones Github repo into a local directory</li>
<li>It then goes through and executes a set of build steps defined for this job</li>
<li>At the end you can optionally configure Jenkins to update the status of the build as described <a href="http://stackoverflow.com/questions/14274293/show-current-state-of-jenkins-build-on-github-repo">here</a></li>
</ol>


<h2>Using Jenkins for network testing</h2>

<ol>
<li>From Jenkins home page click on <strong>create new jobs</strong> and create a <em>Freestyle project</em> called <strong>acme-small</strong>.</li>
<li>Select the <strong>Github project</strong> option and enter a url of your Github repository (in my case its <a href="https://github.com/networkop/network-ci">https://github.com/networkop/network-ci</a>).</li>
<li>Under <strong>Source Code Management</strong> select <em>Git</em> and enter the same repository URL.</li>
<li>Under <strong>Additional Behaviours</strong> add a <em>Polling ignores commits in certain paths</em>. <br/>
Since I&rsquo;m keeping multiple subprojects in the same Github repo, I need to make sure that this job is only triggered if commit affected a particular directory.</li>
<li>Under <strong>Included Regions</strong> add <code>acme-small/.*</code> to only trigger builds for changes made to <strong>acme-small</strong> directory.</li>
<li>Optionally you can specify the build triggers to either build periodically or wait for changes to be pushed to Github.</li>
<li><p>Under <strong>Build</strong> add a new build step with the following shell commands:</p>

<pre><code class="`bash"> export UNL_IP="unl_ip_address"
 export PYTHONUNBUFFERED=1
 cd acme-small
 chmod +x 0_built_topo.py
 chmod +x 2_test.py
 chmod +x 3_destroy_topo.py
 ./0_built_topo.py
 ./2_test.py
 ./3_destroy_topo.py
</code></pre>

<p>The first two env variables setup the UNL&rsquo;s IP address of and disable IO buffering so that we can see the output produced by our scripts in real time. The remaining steps simply execute the build, test and destroy scripts in order.</p></li>
<li><p>Save the job and click on the <strong>Build Now</strong> to trigger the build manually.</p></li>
<li>In the <strong>Build History</strong> pane click on the latest build number (should be #1) and go to <em>Console Output</em>.</li>
<li>Observe how Jenkins builds, tests and destroys our demo topology</li>
</ol>


<h2>Network upgrade workflow</h2>

<p>Now imagine that a new requirements has come in to make sure that traffic from R1 to R3&rsquo;s Gig0/1 does not traverse R4 and goes via R2 instead, only falling back to R4 when R1-R2 link is down. In the following video I&rsquo;ll show how to use network-ci tools locally to implement and test this traffic engineering requirement.</p>

<p><div class="embed-video-container"><iframe src="//www.youtube.com/embed/GLOG9KZzP90" allowfullscreen></iframe></div></p>

<h2>Coming up</h2>

<p>In the next post I&rsquo;ll show how to apply the same workflow to automate the build, test and ugprade of a large 14-node topology.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network-CI Part 1 - Automatically Building a VM With UNetLab and Jenkins]]></title>
    <link href="http://networkop.github.io/blog/2016/02/25/network-ci-dev-setup/"/>
    <updated>2016-02-25T00:00:00-08:00</updated>
    <id>http://networkop.github.io/blog/2016/02/25/network-ci-dev-setup</id>
    <content type="html"><![CDATA[<p>Traditionally, the first post in the series describes how to setup a development environment. This time I&rsquo;ll do it DevOps-style. I&rsquo;ll show how to use Packer to automatically create and configure a VM with UNetLab and Jenkins pre-installed.</p>

<!--more-->


<p><img class="center" src="/images/packer-unl-jenkins.png" title="Packer-UNL-Jenkins" ></p>

<h2>Packer intro</h2>

<p><a href="https://www.packer.io/">Packer</a> is a tool that can automatically create virtual machines for different hypervisors and cloud platforms. The goal is to produce identically configured VMs for either VirtualBox, VMWare, Amazon or Google clouds based on a single template file. If you&rsquo;re familiar with <a href="https://www.vagrantup.com/docs/">Vagrant</a>, then you can also use Packer to create custom Vagrant boxes. In our case, however, we&rsquo;re only concerned about VMWare since it&rsquo;s the only <a href="https://en.wikipedia.org/wiki/Hypervisor">type-2 hypervisor</a> that supports nested hardware virtualisation (e.g. Intel VT-x), a feature required by UNetLab to run some of the emulated images.</p>

<p>Packer builds VMs using a special template file. At the very least, this file describes how to:</p>

<ul>
<li><p>Build a VM</p></li>
<li><p>Provision and configure apps on a VM</p></li>
</ul>


<p>These two actions correspond to the <code>builders</code> and <code>provisioners</code> sections of the template file.</p>

<p>The <code>builders</code> section contains a set of instructions for a particular hypervisor or platform on how to build a VM. For example, it might contain the amount of  RAM, CPU and disk sizes, number and type of interfaces, OS boot instructions and so on.</p>

<p>The <code>provisioners</code> section contains a set of instructions to configure a VM. This section may be as simple as a list of shell scripts or may include a reference to Ansible playbook which will be executed after the VM is built.</p>

<p>You can find my Packer templates along with Ubuntu preseed and provisioner scripts in my <a href="https://github.com/networkop/packer-unl-jenkins">Gihub repository</a>. For those looking for deeper insights about how to build a packer template I can recommend an official Packer <a href="https://www.packer.io/intro/index.html">introduction docs</a>.</p>

<h2>Building a VM with Packer</h2>

<p>As I&rsquo;ve mentioned previously, I&rsquo;m using Windows as my primary development environment and VMWare Workstation as my hypervisor. Before you begin you also need to have <a href="https://www.packer.io/intro/getting-started/setup.html">Packer</a> and <a href="https://git-scm.com/download/win">git</a> installed.</p>

<pre><code class="winbatch 1. Clone git repository">git clone https://github.com/networkop/packer-unl-jenkins
cd packer-unl-jenkins
</code></pre>

<pre><code class="winbatch 2. Start the build">packer build vmware.json
</code></pre>

<p>With a bit of luck, approximately 30 minutes later you should have a fully configured VM inside your VMWare Workstation waiting to be powered on. These are some of the features of this new VM:</p>

<ul>
<li>4 GB of RAM, 20GB of disk space, 2 dual-core vCPUs</li>
<li>1 Host-only and 1 NAT ethernet interfaces both using DHCP</li>
<li>Jenkins and UNetLab installed</li>
<li>Git and Python PIP packages installed</li>
<li>Username/password are <code>unl/admin</code></li>
</ul>


<p>Once powered on, you should be able to navigate to UNetLab&rsquo;s home page at <code>http://vm_ip:80</code> and Jenkins' home page and <code>http://vm_ip:8080</code>, where <code>vm_ip</code> is the IP of your new VM.</p>

<h2>IOU images</h2>

<p>Unfortunately IOU images are not publicly available so you&rsquo;re gonna have to find them yourself, which shouldn&rsquo;t be too hard. You&rsquo;ll also need to generate a license file for these images which, again, I&rsquo;m not going to discuss in this blog, but I can guarantee that you won&rsquo;t have to look farther than the 1st page of Google search to find all your answers. These are the remaining steps that you need to do:</p>

<ol>
<li>Obtain L2 and L3 IOU images</li>
<li>Generate a license file</li>
<li>Follow <a href="http://www.unetlab.com/2014/11/adding-cisco-iouiol-images/">these instructions</a> to install those images on the UNetLab server</li>
</ol>


<h2>non-DevOps way</h2>

<p>In case you&rsquo;re struggling with Packer here are the list of steps to setup a similar VM manually:</p>

<ol>
<li><a href="http://www.ubuntu.com/download/server">Download</a> your favourite Ubuntu Server image. Recommended release at the time of writing is 14.04.4.</li>
<li>Create a VM with at least 4GB of RAM, VT-x support and boot it off the Ubuntu ISO image.</li>
<li>Following instructions <a href="http://www.unetlab.com/2015/08/installing-unetlab-on-a-physical-server/">install Ubuntu and UNetLab</a>.</li>
<li>Install Jenkins as described on <a href="https://wiki.jenkins-ci.org/display/JENKINS/Installing+Jenkins+on+Ubuntu">their wiki website</a></li>
<li>Install additional packages like git and pip. Refer to my Packer <a href="https://github.com/networkop/packer-unl-jenkins/blob/master/scripts/packages.sh">packages script</a> for commands.</li>
</ol>


<h2>Coming up</h2>

<p>In the next post I&rsquo;ll show how to setup Jenkins to do automatic network testing and verification.</p>
]]></content>
  </entry>
  
</feed>
