<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Openstack | Network-oriented programming]]></title>
  <link href="http://networkop.github.io/blog/categories/openstack/atom.xml" rel="self"/>
  <link href="http://networkop.github.io/"/>
  <updated>2016-04-17T18:37:16-07:00</updated>
  <id>http://networkop.github.io/</id>
  <author>
    <name><![CDATA[Michael Kashin]]></name>
    <email><![CDATA[mmkashin@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Building a Multi-node OpenStack Lab in UNetLab]]></title>
    <link href="http://networkop.github.io/blog/2016/04/18/os-unl-lab/"/>
    <updated>2016-04-18T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/04/18/os-unl-lab</id>
    <content type="html"><![CDATA[<p>In the <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous post</a> I&rsquo;ve demonstrated how to get a working instance of a single-node OpenStack inside <a href="http://www.unetlab.com/">UNetLab</a>. In this post we&rsquo;ll continue building on that by adding two new compute nodes and redesigning our network to resemble something you might actually see in a real life.</p>

<!--more-->


<h2>OpenStack network requirements</h2>

<p>Depending on the number of deployed <a href="https://www.openstack.org/software/project-navigator/">components</a>, OpenStack physical network requirements could be different. In our case we&rsquo;re not going to deploy any storage solution and simply use the <strong>ephemeral</strong> storage, i.e. hard disk that&rsquo;s a part of a virtual machine. However, even in minimal installations, there are a number of networks that should be considered individually due to different connectivity requirements:</p>

<ul>
<li><p>Server <abbr title=" Out-Of-Band">OOB</abbr> <strong>management</strong> network - this is usually a dedicated physical network used mainly for server bootstrapping and OS deployment. It is a Layer 3 network with DHCP relays configured at each edge L3 interface and access to Internet package repositories.</p></li>
<li><p><strong>API</strong> network - used for internal communication between various OpenStack services. This can be a routed network without Internet access. The only requirement is any-to-any reachability within a single OpenStack environment.</p></li>
<li><p><strong>External</strong> network - used for public access to internal OpenStack virtual machines. This is the <em>outside</em> of OpenStack, with a pool of IP addresses used to NAT the internal IPs of public-facing virtual machines. This network <strong>must</strong> be Layer 2 adjacent <strong>only</strong> with a network control node.</p></li>
<li><p><strong>Tenant</strong> network - used for communication between virtual machines within OpenStack environment. Thanks to the use of VXLAN overlay, this can be a simple routed network that has any-to-any reachability between all Compute and Network nodes.</p></li>
</ul>


<h2>Building a lab network</h2>

<p>For labbing purposes it&rsquo;s possible to relax some of the above network requirements without seriously affecting the outcomes of our simulation. For example, it&rsquo;s possible to combine some of the networks and still satisfy the requirements stated above. These are the networks that will be configured inside UNetLab:</p>

<ul>
<li><p><strong>Management</strong> - this network will combine the functions of OOB and API networks. To isolate it from our data centre underlay I&rsquo;ll be using separate interfaces on virtual machines and connect them directly to Workstation&rsquo;s NAT interface (192.168.91.0/24 in my case)  to give them direct access to Internet.</p></li>
<li><p><strong>External</strong> - this network will be connected to Workstation&rsquo;s host-only NIC (192.168.247.0/24) through Vlan300 configured on one of the leaf switches. Since it must be L2 adjacent with the network control node our leaf switch will not perform any routing for this subnet.</p></li>
<li><p><strong>Tenant</strong> - this will be a routed leaf/spine <a href="https://en.wikipedia.org/wiki/Clos_network">Clos</a> fabric comprised of 3 leaf and 2 spine switches running a single-area OSPF process on all their links. Each server will have its own unique tenant subnet (Vlan100) terminated on the leaf switch and subnet injected into OSPF. The subnet used for this Vlan is going to be <code>10.0.X.0/24</code>, where X is the number of the leaf switch terminating the vlan.</p>

<p>  The links between switches are all L3 point-to-point with addresses borrowed from 169.254.0.0/16 range specifically to emphasize the fact that the internal addressing does not need to be known or routed outside of the fabric. The <strong>sole function of the fabric</strong> is to provide multiple equal cost paths between any pair of leafs, thereby achieving maximum link utilisation. Here&rsquo;s an example of a traceroute between Vlan100&rsquo;s of Leaf #1 and Leaf #3.</p></li>
</ul>


<pre><code class="text Traceroute inside an ECMP routed Clos fabric">L3#traceroute 10.0.1.1 source 10.0.3.1
Type escape sequence to abort.
Tracing the route to 10.0.1.1
VRF info: (vrf in name/id, vrf out name/id)
  1 169.254.31.111 1 msec
    169.254.32.222 0 msec
    169.254.31.111 0 msec
  2 169.254.12.1 1 msec
    169.254.11.1 1 msec *
</code></pre>

<h2>Building lab servers</h2>

<p>Based on my experience a standard server would have at least 3 physical interfaces - one for OOB management and a pair of interfaces for application traffic. The two application interfaces will normally be combined in a single <abbr title=" Link Aggregation Group">LAG</abbr> and connected to a pair of MLAG-capable TOR switches. Multi-chassis LAG or <a href="http://blog.ipspace.net/2010/10/multi-chassis-link-aggregation-basics.html">MLAG</a> is a pretty old and well-understood technology so I&rsquo;m not going to try and simulate it in the lab. Instead I&rsquo;ll simply assume that a server will be connected to a TOR switch via a single physical link. That link will be setup as a dot1q trunk to allow for multiple subnets to share it.</p>

<h2>Physical lab topology</h2>

<p>All the above requirements and assumptions result in the following topology that we need to build inside UNetLab:</p>

<p><img class="center" src="/images/neutron-native.png"></p>

<p>For servers I&rsquo;ll be using OpenStack node type that I&rsquo;ve described in my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous post</a>. The two compute nodes do not need as much RAM as the control node, so I&rsquo;ll reduce it to just 2GB.</p>

<p>For switches I&rsquo;ll be using a Cisco&rsquo;s <a href="http://www.unetlab.com/2014/11/adding-cisco-iouiol-images/">L2 IOU</a> image for now, mainly due to the low resource requirements. In the future I&rsquo;ll try and swap it for something else. As you can see from the sample config below, fabric configuration is very basic and can be easily replaced by any other solution:</p>

<pre><code class="text Leaf 1 configuration">interface Ethernet0/0
 no switchport
 ip address 169.254.11.1 255.255.255.0
 ip ospf network point-to-point
 duplex auto
!
interface Ethernet0/1
 no switchport
 ip address 169.254.12.1 255.255.255.0
 ip ospf network point-to-point
 duplex auto
!
interface Ethernet0/2
 switchport trunk allowed vlan 100
 switchport trunk encapsulation dot1q
 switchport mode trunk
!
interface Vlan100
 ip address 10.0.1.1 255.255.255.0
!
router ospf 1
 network 0.0.0.0 255.255.255.255 area 0
</code></pre>

<h2>Server configuration and OpenStack installation</h2>

<p>Refer to my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous blogpost</a> for instructions on how to install OpenStack and follow the first 5 steps from &ldquo;Installing CentOS and Openstack&rdquo; section. Before doing the final step, we need to configure our VMs' new interfaces:</p>

<ul>
<li>Remove any IP configuration from <strong>eth1</strong> interface to make it look like this:</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/ifcfg-eth1">TYPE=Ethernet
BOOTPROTO=none
DEVICE=eth1
ONBOOT=yes
</code></pre>

<ul>
<li>Configure <strong>Tenant network</strong>:</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/ifcfg-eth1.100">VLAN=yes
DEVICE=eth1.100
BOOTPROTO=none
IPADDR=10.0.X.10
PREFIX=24
ONBOOT=yes
</code></pre>

<ul>
<li>Setup a <strong>static route</strong> to all other leaf nodes:</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/route-eth1.100">10.0.0.0/8 via 10.0.X.1
</code></pre>

<ul>
<li>On Control node setup <strong>External network</strong> interface</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/route-eth1.300">DEVICE=eth1.300
IPADDR=192.168.247.100
PREFIX=24
ONBOOT=yes
BOOTPROTO=none
VLAN=yes
</code></pre>

<p>Now we&rsquo;re ready to kick off OpenStack installation. This can be done with a single command that needs to be executed on the Control node. Note that <code>eth1.100</code> interface is spelled as <code>eth1_100</code> in the last line.</p>

<pre><code class="bash Controller">packstack --allinone \
    --os-cinder-install=n \
    --os-ceilometer-install=n \
    --os-trove-install=n \
    --os-ironic-install=n \
    --nagios-install=n \
    --os-swift-install=n \
    --os-gnocchi-install=n \
    --os-aodh-install=n \
    --os-neutron-ovs-bridge-mappings=extnet:br-ex \
    --os-neutron-ovs-bridge-interfaces=br-ex:eth1.300 \
    --os-neutron-ml2-type-drivers=vxlan,flat \
    --provision-demo=n \
    --os-compute-hosts=192.168.91.10,192.168.91.11,192.168.91.12 \
    --os-neutron-ovs-tunnel-if=eth1_100
</code></pre>

<h2>Creating a virtual network for a pair of VMs</h2>

<p>Once again, follow all steps from &ldquo;Configuring Openstack networking&rdquo; section of my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous blogpost</a>. Only this time when setting up a public subnet, update the subnet details to match our current environment:</p>

<pre><code class="bash Step 3 - Creating a public subnet">  neutron subnet-create --name public_subnet \
    --enable_dhcp=False \
    --allocation-pool=start=192.168.247.90,end=192.168.247.126 \
    --gateway=192.168.247.1 external_network 192.168.247.0/24
</code></pre>

<h2>Nova-scheduler and setting up host aggregates</h2>

<p>OpenStack&rsquo;s Nova project is responsible for managing virtual machines. Nova controller views all available compute nodes as a single pool of resources. When a new VM is to be instantiated, a special process called nova-scheduler examines all available compute nodes and selects the &ldquo;best&rdquo; one based on a special algorithm, which normally takes into account amount of RAM, CPU and other host capabilities.</p>

<p>To make our host selection a little bit more deterministic, we can define a group of compute servers via <strong>host aggregates</strong>, which will be used by nova-scheduler in its selection algorithm. Normally it could include all servers in a single rack or a row of racks. In our case we&rsquo;ll setup two host aggregates each with a single compute host. This way we&rsquo;ll be able to select exactly which compute host to use when instantiating a new virtual machine.</p>

<p>To setup it up, from Horizon&rsquo;s dashboard navigate to Admin -> System and create two host aggregates <strong>comp-1</strong> and <strong>comp-2</strong>, each including a single compute host.</p>

<h2>Creating workloads and final testing</h2>

<p>Using a process described in &ldquo;Spinning up a VM&rdquo; section of my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous blogpost</a>, create a couple of virtual machines assigning them to different host aggregates created earlier.</p>

<h2>Security and Remote access</h2>

<p>To access these virtual machines we need to give them a <a href="https://www.rdoproject.org/networking/floating-ip-range/">floating</a> ip address from the External subnet range. To do that navigate to Project -> Compute -> Instances and select <strong>Associate Floating IP</strong> from the Actions drop-down menu.</p>

<p>The final steps is to allow remote SSH access. Each new VM inherits ACLs from a default security group. So the easiest way to allow SSH is to go to Project -> Compute -> Access &amp; Security and add a rule to allow inbound SSH connections for the default security group.</p>

<h2>Verification</h2>

<p>At this stage you should be able to SSH into the floating IP addresses assigned to the two new VMs using the default credentials. Feel free to poke around and explore Horizon&rsquo;s interface a bit more. For example, try setting up an SSH key pair and re-build our two VMs to allow passwordless SSH access.</p>

<h2>What to expect next</h2>

<p>In the next post we&rsquo;ll explore some of the basic concepts of OpenStack&rsquo;s SDN. We&rsquo;ll peak inside the internal implementation of virtual networks and see what are some of their limitations and drawbacks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Openstack on UNetlab]]></title>
    <link href="http://networkop.github.io/blog/2016/04/04/openstack-unl/"/>
    <updated>2016-04-04T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/04/04/openstack-unl</id>
    <content type="html"><![CDATA[<p>In this post I&rsquo;m going to show how to get a running instance of Openstack inside a UNetLab virtual machine.</p>

<!--more-->


<p><img class="center" src="/images/unl-os.png"></p>

<h2>What the hell am I trying to do?</h2>

<p>I admit that running Openstack on anything other than baremetal is nonsense. So why would anyone want to run it with two layers of virtualisation underneath? My goal is to explore some of the new SDN/NFV technologies without leaving the confines on my home area network and/or racking up a triple-digit electricity bill. I also wanted to be able to swap underlay networks without spending hours trying to plumb together virtualized switches and servers from multiple vendors. That&rsquo;s why I&rsquo;ve decided to use UNetLab VM as a host for my Openstack lab. This would allow me to easily assemble any type of underlay, WAN or DCI network and with hardware virtualisation support I can afford to run Openstack double-nested inside Workstation and Qemu on my dual-core i7 without too much of a performance penalty. After all, <a href="https://www.ravellosystems.com/technology/hvx">some companies</a> even managed to turn similar things into a commercial product.</p>

<p>My interest in Openstack is strictly limited by networking, that&rsquo;s why a lot of the things you&rsquo;ll see in this and following posts will not be applicable to a real-life production environment. However, as far as networking is concerned, I&rsquo;ll try to stick as close to the official Openstack <a href="http://docs.openstack.org/openstack-ops/content/example_architecture.html">network design</a> as possible. I&rsquo;ll be using <a href="https://www.rdoproject.org">RDO</a> to deploy Openstack. The specific method will be Packstack which is a collection of Puppet modules used to deploy Openstack components.</p>

<p>Why have I not went the OpenDaylight/Mininet way if I wanted to play with SDN/NFV? Because I wanted something more realistic to play with, that wouldn&rsquo;t feel like vendor&rsquo;s powerpoint presentation. Plus there&rsquo;s plenty of resources on the &lsquo;net about it anyway.</p>

<p>So, without further ado, let&rsquo;s get cracking.</p>

<h2>Setting the scene</h2>

<p>On my Windows 8 laptop I&rsquo;ve got a UNL virtual machine running inside a VMWare Workstation.. I&rsquo;ve <a href="http://www.unetlab.com/download/">downloaded</a> and <a href="http://www.unetlab.com/2014/11/upgrade-unetlab-installation/">upgraded</a> a pre-built UNL VM image. I&rsquo;ve also downloaded a copy of the <a href="http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.iso">Centos 7 minimal ISO image</a> and copied it over to my UNL VM&rsquo;s home directory.</p>

<p>For network access I&rsquo;ll be using VMWware Workstation&rsquo;s NAT interface. It&rsquo;s currently configured with <code>192.168.91.0/24</code> subnet with DHCP range of <code>.128-.254</code>. Therefore I&rsquo;ll be using <code>.10-.126</code> to allocate IPs to my Openstack servers.</p>

<h2>Creating a custom node type in UNL</h2>

<p>Every node type inside UNL has its own unique settings. Some settings, like amount of RAM, CPU or number of network interfaces, can be changed during node instantiation, while some of them remain &ldquo;baked in&rdquo;. Say, for example, the default &ldquo;Linux&rdquo; template creates nodes with default <strong>Qemu Virtual CPU</strong> which doesn&rsquo;t support the hardware virtualisation (<strong>VT-X/AMD-V</strong>) <a href="http://docs.openstack.org/liberty/config-reference/content/kvm.html">required</a> by Openstack. In order to change that you can either edit the existing node template or follow these steps to create a new one:</p>

<ol>
<li><p>Add Openstack node definition to initialization file <code>/opt/unetlab/html/includes/init.php</code>.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='php'><span class='line'> <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">linux</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;</span>                 <span class="o">=&gt;</span>      <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">Linux</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;,</span>
</span><span class='line'> <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">openstack</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;</span>             <span class="o">=&gt;</span>      <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">Openstack</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;,</span>
</span><span class='line'> <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">mikrotik</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;</span>              <span class="o">=&gt;</span>      <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">MikroTik</span> <span class="nx">RouterOS</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;,</span>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a new Openstack node template based on existing linux node template.</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>cp /opt/unetlab/html/templates/linux.php /opt/unetlab/html/templates/openstack.php
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Edit the template file replacing all occurences of &lsquo;Linux&rsquo; with &lsquo;Openstack&rsquo;</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/Linux/Openstack/g<span class="p">;</span> s/linux/openstack/g<span class="err">&#39;</span> /opt/unetlab/html/templates/openstack.php
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Edit the template file to double the RAM and CPU and pass all host&rsquo;s CPU instructions to Openstack nodes</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/2048/4096/<span class="p">;</span> s/<span class="o">(</span>cpu.*<span class="o">)</span> <span class="o">=</span> 1/<span class="se">\1</span> <span class="o">=</span> 2/<span class="p">;</span> s/<span class="o">(</span><span class="nv">order</span><span class="o">=)</span>dc/<span class="se">\1</span>cd -cpu host/<span class="p">&amp;</span>lsquo<span class="p">;</span> /opt/unetlab/html/templates/openstack.php
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<p>At this point you should be able to navigate to UNL&rsquo;s web interface and find a new node of type Openstack. However you won&rsquo;t be able to create it until you have at least one image, which is what we&rsquo;re going to build next.</p>

<h2>Building a Linux VM inside UNetLab</h2>

<p>Now we need to create a CentOS image inside a UNL. One way to do it is build it inside a VMWare Workstation, copy it to UNL and convert the <strong>.vmdk</strong> to <strong>.qcow2</strong>. However, when I tried doing this I ran into a problem with CentOS not finding the correct disk partitions during bootup. The workaround was to boot into rescue mode and rebuild the initramfs. For those feeling adventurous, I would recommend checking out the following links [<a href="https://wiki.centos.org/TipsAndTricks/CreateNewInitrd">1</a>, <a href="http://advancelinux.blogspot.com.au/2013/06/how-to-rebuild-initrd-or-initramfs-in.html">2</a>, <a href="http://forums.fedoraforum.org/showthread.php?t=288020">3</a>] before trying this option.<br/>
The other option is to build CentOS inside UNL from scratch. This is how you can do it:</p>

<ol>
<li><p>Create a new directory for Openstack image</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>mkdir -p /opt/unetlab/addons/qemu/openstack-1
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a link to CentOS ISO boot image from our new directory</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>ln -s ~/CentOS-7-x86_64-Minimal-1511.iso /opt/unetlab/addons/qemu/openstack-1/cdrom.iso
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a blank 6Gb disk image</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>/opt/qemu/bin/qemu-img create -f qcow2 -o <span class="nv">preallocation</span><span class="o">=</span>metadata /opt/unetlab/addons/qemu/openstack-1/virtioa.qcow2 6G
</span></code></pre></td></tr></table></div></figure></p>

<p>If you want to create <strong>snapshots</strong> at any stage of the process you&rsquo;d need to use a copy of this file under /opt/unetlab/tmp/pod_id/lab_uuid/node_id/ directory</p></li>
</ol>


<p>Now you should be able to successfully create an Openstack node and connect it to Internet. Create a new network that would have Internet connectivity (in my case it&rsquo;s <strong>pnet0</strong>) and connect it to Openstack&rsquo;s <strong>eth0</strong>.  At this stage we have everything ready to start installing Openstack, but before we move on let me take a quick detour to tell you about my ordeals with VNC integration.</p>

<h2>Optional: Integrating TightVNC with UNL</h2>

<p>For some unknown reason UltraVNC does not work well on my laptop. My sessions would often crash or start minimised with the only option to close the window. That&rsquo;s not the only thing not working properly on my laptop thanks to the corporate policies with half of the sh*t locked down for <em>security</em> reasons.<br/>
So instead of mucking around with <strong>Ultra</strong> I decided to give me old pal <strong>Tight</strong>VNC a go. The setup process is very similar to the <a href="http://www.unetlab.com/2015/03/url-telnet-ssh-vnc-integration-on-windows/">official VNC integration guide</a> with the following exceptions:</p>

<ol>
<li><p>The wrapper file simply strips the leading &lsquo;vnc://&rsquo; and trailing &lsquo;/&rsquo; off the passed argument</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='winbatch'><span class='line'><span class="k">SET</span> <span class="nv">arg</span><span class="o">=</span><span class="nv">%1</span>
</span><span class='line'><span class="k">SET</span> <span class="nv">arg</span><span class="o">=</span><span class="nv">%arg:~6%</span>
</span><span class='line'><span class="k">SET</span> <span class="nv">arg</span><span class="o">=</span><span class="nv">%arg:~0</span><span class="p">,</span><span class="m">-1</span>%
</span><span class='line'>start &amp;ldquo;&amp;rdquo; &amp;ldquo;c:\Program Files\TightVNC\tvnviewer.exe&amp;rdquo; <span class="nv">%arg%</span>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>The registry entry now points to the TightVNC wrapper</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='registry'><span class='line'><span class="k">[</span><span class="nb">HKEY_CLASSES_ROOT</span><span class="k">\vnc\shell\open\command]</span>
</span><span class='line'><span class="na">@</span><span class="o">=</span><span class="s">&amp;ldquo;\&quot;c:\Program Files\TightVNC\wrapper.bat\&amp;rdquo; %1&quot;</span>
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<h2>Installing CentOS and Openstack</h2>

<p>Finally, we&rsquo;ve got all our ducks lined up in a row and we&rsquo;re ready to shoot. Fire up the Openstack node inside UNL and click on it to open a vnc session. Proceed to install CentOS with default options. You need to confirm which <strong>hard disk</strong> to use and setup the <strong>hostname</strong> and the <strong>root password</strong> during installation process.
As I mentioned earlier, we&rsquo;ll be using RDO&rsquo;s Packstack to deploy all the necessary Openstack components. The whole installation process will be quite simple and can be found on the RDO&rsquo;s <a href="https://www.rdoproject.org/install/quickstart/">quickstart page</a>. Here is my slightly modified version of installation process:</p>

<ol>
<li><p>Disable Network Manager and SELinux.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>service NetworkManager stop
</span><span class='line'> <span class="nv">$ </span>systemctl disable NetworkManager.service
</span><span class='line'> <span class="nv">$ </span>setenforce 0
</span><span class='line'> <span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/enforcing/permissive/<span class="p">&amp;</span>lsquo<span class="p">;</span> /etc/selinux/config
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Configure static IP address <code>192.168.91.10</code> on the network interface. <br/>
 Assuming your interface name is <code>eth0</code> make sure you /etc/sysconfig/network-scripts/ifcfg-eth0 looks something like this:</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">TYPE</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>Ethernet<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">BOOTPROTO</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>static<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">IPADDR</span><span class="o">=</span>192.168.91.10
</span><span class='line'> <span class="nv">PREFIX</span><span class="o">=</span>24
</span><span class='line'> <span class="nv">GATEWAY</span><span class="o">=</span>192.168.91.2
</span><span class='line'> <span class="nv">DNS1</span><span class="o">=</span>192.168.91.2
</span><span class='line'> <span class="nv">NAME</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>eth0<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">DEVICE</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>eth0<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">ONBOOT</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>yes<span class="p">&amp;</span>rdquo<span class="p">;</span>&lt;br/&gt;
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Issue a <code>service network restart</code> and reconnect to the new static IP address. Make sure that you still have access to Internet after making this change.</p></li>
<li><p>Setup RDO repositories</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>sudo yum install -y &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://rdoproject.org/repos/rdo-release.rpm&quot;</span>&gt;https://rdoproject.org/repos/rdo-release.rpm&lt;/a&gt;
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Update your current packages</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>sudo yum update -y
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Install Packstack</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>sudo yum install -y openstack-packstack
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> That&rsquo;s where it&rsquo;d make sense to take a snapshot with <code>qemu-img snapshot -c pre-install virtioa.qcow2</code> command</p></li>
<li><p>Deploy a single-node Openstack environment</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>packstack --allinone <span class="se">\</span>
</span><span class='line'> --os-cinder-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-ceilometer-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-trove-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-ironic-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --nagios-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-swift-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-neutron-ovs-bridge-mappings<span class="o">=</span>extnet:br-ex <span class="se">\</span>
</span><span class='line'> --os-neutron-ovs-bridge-interfaces<span class="o">=</span>br-ex:eth0 <span class="se">\</span>
</span><span class='line'> --os-neutron-ml2-type-drivers<span class="o">=</span>vxlan,flat <span class="se">\</span>
</span><span class='line'> --provision-demo<span class="o">=</span>n
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Here we&rsquo;re overriding some of the default Packstack options. We&rsquo;re not installing some of the components we&rsquo;re not going to use and setting up a name (<strong>extnet</strong>) for our external physical segment, which we&rsquo;ll use in the next section.</p></li>
</ol>


<p>At the end of these 4 steps you should be able to navigate to Horizon (Openstack&rsquo;s dashboard) by typing <code>http://192.168.91.10</code> in your browser. You can find login credentials in the <code>~/keystonerc_admin</code> file.</p>

<h2>Configuring Openstack networking</h2>

<p>At this stage we need to setup virtual networking infrastructure inside Openstack. This will be almost the same as described in RDO&rsquo;s external network <a href="https://www.rdoproject.org/networking/neutron-with-existing-external-network/">setup guide</a>. The only exceptions will be the <a href="https://www.rdoproject.org/networking/difference-between-floating-ip-and-private-ip/">floating IP range</a>, which will match our existing environment, and the fact that we&rsquo;re no going to setup any additional tenants yet. This is how our topology will look like:</p>

<p><img class="center" src="/images/os-net-1.png"></p>

<ol>
<li><p>Switch to Openstack&rsquo;s <code>admin</code> user
 <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span><span class="nb">source</span> ~/keystonerc_admin
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create external network
 <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron net-create external_network --provider:network_type flat <span class="se">\</span>
</span><span class='line'> --provider:physical_network extnet  <span class="se">\</span>
</span><span class='line'> --router:external <span class="se">\</span>
</span><span class='line'> --shared
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a public subnet</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron subnet-create --name public_subnet <span class="se">\</span>
</span><span class='line'> --enable_dhcp<span class="o">=</span>False <span class="se">\</span>
</span><span class='line'> --allocation-pool<span class="o">=</span><span class="nv">start</span><span class="o">=</span>192.168.91.90,end<span class="o">=</span>192.168.91.126 <span class="se">\</span>
</span><span class='line'> --gateway<span class="o">=</span>192.168.91.2 external_network 192.168.91.0/24
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Default gateway is VMware&rsquo;s NAT IP address</p></li>
<li><p>Create a private network and subnet</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron net-create private_network
</span><span class='line'> neutron subnet-create --name private_subnet private_network 10.0.0.0/24 <span class="se">\</span>
</span><span class='line'> --dns-nameserver 8.8.8.8
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p>  This network is not routable outside of Openstack and is used for inter-VM communication</p></li>
<li><p>Create a virtual router and attach it to both networks</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron router-create router
</span><span class='line'> neutron router-gateway-set router external_network
</span><span class='line'> neutron router-interface-add router private_subnet
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<p>Make sure to check out the visualisation of our newly created network topology in Horizon, it&rsquo;s amazing.</p>

<h2>Spinning up a VM</h2>

<p>There&rsquo;s no point in installing Openstack just for the sake of it. Our final step would be to create a working virtual machine that would be able to connect to Internet.</p>

<ol>
<li><p>Download a test linux image</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> curl &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img&quot;</span>&gt;http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img&lt;/a&gt; <span class="p">|</span> glance <span class="se">\</span>
</span><span class='line'> image-create --name<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>cirros image<span class="p">&amp;</span>rsquo<span class="p">;</span> <span class="se">\</span>
</span><span class='line'> --visibility<span class="o">=</span>public <span class="se">\</span>
</span><span class='line'> --container-format<span class="o">=</span>bare <span class="se">\</span>
</span><span class='line'> --disk-format<span class="o">=</span>qcow2
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>From Horizon&rsquo;s home page navigate to Project -> Compute -> Images.</p></li>
<li><p>Click on <code>Launch Instance</code> and give the new VM a name.</p></li>
<li><p>Make sure it&rsquo;s attached to <code>private_network</code> under the Networking tab.</p></li>
<li><p>Less then a minute later the status should change to <code>Active</code> and you can navigate to VM&rsquo;s console by clicking on its name and going to <code>Console</code> tab.</p></li>
<li><p>Login using the default credentials (<strong>cirros/cubswin:)</strong>) and verify Internet access by pinging google.com.</p></li>
</ol>


<p>Congratulations, we have successfully created a VM running inside a KVM inside a KVM inside a VMWare Workstation inside Windows!</p>

<h2>What to expect next</h2>

<p>Unlike my other post series, I don&rsquo;t have a clear goal at this stage so I guess I&rsquo;ll continue playing around with different underlays for multi-node Openstack and then move on to various SDN solutions available like OpenDayLight and OpenContrail. Unless I lose interest half way through, which happened in the past. But until that happens, stay tuned for more.</p>
]]></content>
  </entry>
  
</feed>
