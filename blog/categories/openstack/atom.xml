<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Openstack | Network-oriented programming]]></title>
  <link href="http://networkop.github.io/blog/categories/openstack/atom.xml" rel="self"/>
  <link href="http://networkop.github.io/"/>
  <updated>2017-01-24T21:16:34+00:00</updated>
  <id>http://networkop.github.io/</id>
  <author>
    <name><![CDATA[Michael Kashin]]></name>
    <email><![CDATA[mmkashin@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[OpenStack SDN With OVN (Part 2) - Network Engineering Analysis]]></title>
    <link href="http://networkop.github.io/blog/2016/12/10/ovn-part2/"/>
    <updated>2016-12-10T00:00:00+00:00</updated>
    <id>http://networkop.github.io/blog/2016/12/10/ovn-part2</id>
    <content type="html"><![CDATA[<p>In this post we will see how OVN implements virtual networks for OpenStack. The structure of this post is such that starting from the highest level of networking abstraction we will delve deeper into implementation details with each subsequent section. The biggest emphasis will be on how networking data model gets transformed into a set of logical flows, which eventually become OpenFlow flows. The final section will introduce a new overlay protocol GENEVE and explain why VXLAN no longer satisfies the needs of an overlay protocol.</p>

<!--more-->


<h2>OpenStack - virtual network topology</h2>

<p>In the <a href="/blog/2016/11/27/ovn-part1/">previous post</a> we have installed OpenStack and created a simple virtual topology as shown below. In OpenStack&rsquo;s data model this topology consists of the following elements:</p>

<ul>
<li><strong>Network</strong> defines a virtual L2 broadcast domain</li>
<li><strong>Subnet</strong> attached to the network, defines an IP subnet within the network</li>
<li><strong>Router</strong> provides connectivity between all directly connected subnets</li>
<li><strong>Port</strong> VM&rsquo;s point of attachment to the subnet</li>
</ul>


<p><img class="center" src="/images/ovn-zoom1.png"></p>

<p>So far nothing unusual, this is a simple Neutron data model, all that information is stored in Neutron&rsquo;s database and can be queried with <code>neutron</code> CLI commands.</p>

<h2>OVN Northbound DB - logical network topology</h2>

<p>Every call to implement an element for the above data model is forwarded to OVN ML2 driver as defined by the <code>mechanism driver</code> setting of the ML2 plugin. This driver is responsible for the creation of an appropriate data model inside the OVN Northbound DB. The main elements of this data model are:</p>

<ul>
<li><strong>Switch</strong> equivalent of a Neutron&rsquo;s Subnet, enables L2 forwarding for all attached ports</li>
<li><strong>Distributed Router</strong> provides distributed routing between directly connected subnets</li>
<li><strong>Gateway Router</strong> provides connectivity between external networks and distributed routers, implements NAT and Load Balancing</li>
<li><strong>Port</strong> of a logical switch, attaches VM to the switch</li>
</ul>


<p>This is a visual representation of our network topology inside OVN&rsquo;s Northbound DB, built based on the output of <code>ovn-nbctl show</code> command:</p>

<p><img class="center" src="/images/ovn-zoom2.png"></p>

<p>This topology is pretty similar to Neutron&rsquo;s native data model with the exception of a gateway router. In OVN, a gateway router is a special non-distributed router which performs functions that are very hard or impossible to distribute amongst all nodes, like NAT and Load Balancing. This router only exists on a single compute node which is selected by the scheduler based on the <code>ovn_l3_scheduler</code> setting of the ML2 plugin. It is attached to a distributed router via a point-to-point /30 subnet defined in the <code>ovn_l3_admin_net_cidr</code> setting of the ML2 plugin.</p>

<p>Apart from the logical network topology, Northbound database keeps track of all QoS, NAT and ACL settings and their parent objects. The detailed description of all tables and properties of this database can be found in the official <a href="http://openvswitch.org/support/dist-docs/ovn-nb.5.html">Northbound DB documentation</a>.</p>

<h2>OVN Southbound DB - logical flows</h2>

<p>OVN <a href="http://openvswitch.org/support/dist-docs/ovn-northd.8.html">northd</a> process running on the controller node translates the above logical topology into a set of tables stored in Southbound DB. Each row in those tables is a logical flow and together they form a <strong>forwarding pipeline</strong> by stringing together multiple actions to be performed on a packet. These actions range from packet drop through packet header modification to packet output. The stringing is implemented with a special <code>next</code> action which moves the packet one step down the pipeline starting from table 0. Let&rsquo;s have a look at the <strong>simplified</strong> versions of L2 and L3 forwarding pipelines using examples from our virtual topology.</p>

<h3>L2 datapath</h3>

<p>In the first example we&rsquo;ll explore the L2 datapath between VM1 and VM3. Both VMs are attached to the ports of the same logical switch. The full datapath of a logical switch consists of two parts - ingress and egress datapath (the direction is from the perspective of a logical switch). The ultimate goal of an ingress datapath is to determine the output port or ports (in case of multicast) and pass the packet to the egress datapath. The egress datapath does a few security checks before sending the packet out to its destination. Two things are worth noting at this stage:</p>

<ol>
<li>The two datapaths can be located either on the same or on two different hypervisor nodes. In the latter case, the packet is passed between the two nodes in an overlay tunnel.</li>
<li>The egress datapath does not have a destination lookup step which means that all information about the output port MUST be supplied by the ingress datapath. This means that destination lookup does not have to be done twice and it also has some interesting implications on the choice of encapsulation protocol as we&rsquo;ll see in the next section.</li>
</ol>


<p><img class="center" src="/images/ovn-zoom3-l2.png"></p>

<p>Let&rsquo;s have a closer look at each of the stages of the forwarding pipeline. I&rsquo;ll include snippets of logical flows demonstrating the most interesting behaviour at each stage. Full logical datapath is quite long and can be viewed with <code>ovn-sbctl lflow-list [DATAPATH]</code> command. Here is some useful information, collected from the Northbound database, that will be used in the examples below:</p>

<hr />

<table>
<thead>
<tr>
<th> VM# </th>
<th> IP </th>
<th> MAC </th>
<th> Port UUID </th>
</tr>
</thead>
<tbody>
<tr>
<td> VM1 </td>
<td> 10.0.0.2 </td>
<td> fa:16:3e:4f:2f:b8 </td>
<td> 26c23a54-6a91-48fd-a019-3bd8a7e118de </td>
</tr>
<tr>
<td> VM3 </td>
<td> 10.0.0.5 </td>
<td> fa:16:3e:2a:60:32 </td>
<td> 5c62cfbe-0b2f-4c2a-98c3-7ee76c9d2879 </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li><strong>Port security</strong> - makes sure that incoming packet has the correct source MAC and IP addresses.</li>
</ul>


<pre><code>table=0 (ls_in_port_sec_l2), priority=50, match=(inport == "26c23a54-6a91-48fd-a019-3bd8a7e118de"
  &amp;&amp; eth.src == {fa:16:3e:4f:2f:b8}), action=(next;)
table=1 (ls_in_port_sec_ip), priority=90, match=(inport == "26c23a54-6a91-48fd-a019-3bd8a7e118de"
  &amp;&amp; eth.src == fa:16:3e:4f:2f:b8 &amp;&amp; ip4.src == {10.0.0.2}), action=(next;)
</code></pre>

<ul>
<li><strong>Egress ACL</strong> - set of tables that implement Neutron&rsquo;s Egress Port Security functionality. Default rules allow all egress traffic from a VM. The first flow below matches all new connections coming from VM1 and marks them for connection tracking with <code>reg0[1] = 1</code>. The next table catches these marked packets and commits them to the connection tracker. Special <code>ct_label=0/1</code> action ensures return traffic is allowed which is a standard behaviour of all stateful firewalls.</li>
</ul>


<pre><code>table=6 (ls_in_acl), priority=2002 , match=(((ct.new &amp;&amp; !ct.est) ||
  (!ct.new &amp;&amp; ct.est &amp;&amp; !ct.rpl &amp;&amp; ct_label.blocked == 1)) &amp;&amp;
  (inport == "26c23a54-6a91-48fd-a019-3bd8a7e118de" &amp;&amp; ip4)),
  action=(reg0[1] = 1; next;)
table=9 (ls_in_stateful), priority=100  , match=(reg0[1] == 1),
  action=(ct_commit(ct_label=0/1); next;)
</code></pre>

<ul>
<li><strong>ARP Responder</strong> - matches an incoming ARP/ND request and generates an appropriate ARP/ND response. The way it is accomplished is similar to Neutron&rsquo;s native <a href="/blog/2016/05/06/neutron-l2pop/">ARP responder</a> feature. Effectively an ARP request gets transformed into an ARP response by swapping source and destination fields.</li>
</ul>


<pre><code>table=10(ls_in_arp_rsp), priority=50, match=(arp.tpa == 10.0.0.5 &amp;&amp; arp.op == 1),
  action=(eth.dst = eth.src; eth.src = fa:16:3e:2a:60:32; arp.op = 2; /* ARP reply */
  arp.tha = arp.sha; arp.sha = fa:16:3e:2a:60:32; arp.tpa = arp.spa; arp.spa = 10.0.0.5;
  outport = inport; flags.loopback = 1; output;)
</code></pre>

<ul>
<li><strong>DHCP Processing</strong> - set of tables that implement the DHCP server functionality using the approach similar to the ARP responder described above.</li>
</ul>


<pre><code>table=12(ls_in_dhcp_response), priority=100, match=(inport == "26c23a54-6a91-48fd-a019-3bd8a7e118de"
  &amp;&amp; eth.src == fa:16:3e:4f:2f:b8 &amp;&amp; ip4.src == 0.0.0.0 &amp;&amp; ip4.dst == 255.255.255.255
  &amp;&amp; udp.src == 68 &amp;&amp; udp.dst == 67 &amp;&amp; reg0[3]),
  action=(eth.dst = eth.src; eth.src = fa:16:3e:94:b6:bc; ip4.dst = 10.0.0.2;
  ip4.src = 10.0.0.1; udp.src = 67; udp.dst = 68; outport = inport; flags.loopback = 1; output;)
</code></pre>

<ul>
<li><strong>Destination Lookup</strong> - implements L2 forwarding based on the destination MAC address of a frame. At this stage the <strong>outport</strong> variable is set to the VM3&rsquo;s port UUID.</li>
</ul>


<pre><code>table=13(ls_in_l2_lkup), priority=50, match=(eth.dst == fa:16:3e:2a:60:32),
  action=(outport = "5c62cfbe-0b2f-4c2a-98c3-7ee76c9d2879"; output;)
</code></pre>

<ul>
<li><strong>Ingress ACL</strong> - set of tables that implement Neutron&rsquo;s Ingress Port security. For the sake of argument let&rsquo;s assume that we have enabled inbound SSH connections. The principle is same as before - the packet gets matched in one table and submitted to connection tracking in another table.</li>
</ul>


<pre><code>table=4 (ls_out_acl), priority=2002 , match=(((ct.new &amp;&amp; !ct.est)
  || (!ct.new &amp;&amp; ct.est &amp;&amp; !ct.rpl &amp;&amp; ct_label.blocked == 1))
  &amp;&amp; (outport == "26c23a54-6a91-48fd-a019-3bd8a7e118de" &amp;&amp; ip4
  &amp;&amp; ip4.src == 0.0.0.0/0 &amp;&amp; tcp &amp;&amp; tcp.dst == 22)),
  action=(reg0[1] = 1; next;
table=6 (ls_out_stateful), priority=100  , match=(reg0[1] == 1),
  action=(ct_commit(ct_label=0/1); next;)
</code></pre>

<ul>
<li><strong>Port Security</strong> - implements inbound port security for destination VM by checking the sanity of destination MAC and IP addresses.</li>
</ul>


<pre><code>table=7 (ls_out_port_sec_ip), priority=90, match=(outport == "5c62cfbe-0b2f-4c2a-98c3-7ee76c9d2879"
  &amp;&amp; eth.dst == fa:16:3e:2a:60:32 &amp;&amp; ip4.dst == {255.255.255.255, 224.0.0.0/4, 10.0.0.5}),
  action=(next;)
table=8 (ls_out_port_sec_l2), priority=50, match=(outport == "5c62cfbe-0b2f-4c2a-98c3-7ee76c9d2879"
  &amp;&amp; eth.dst == {fa:16:3e:2a:60:32}),
  action=(output;)
</code></pre>

<h3>L3 datapath</h3>

<p>Similar to a logical switch pipeline, L3 datapath is split into ingress and egress parts. In this example we&rsquo;ll concentrate on  the Gateway router datapath. This router is connected to a distributed logical router via a transit subnet (SWtr) and to an external network via an external bridge (SWex) and performs NAT translation for all VM traffic.</p>

<p><img class="center" src="/images/ovn-zoom3-l3.png"></p>

<p>Here is some useful information about router interfaces and ports that will be used in the examples below.</p>

<hr />

<table>
<thead>
<tr>
<th> SW function </th>
<th> IP </th>
<th> MAC </th>
<th> Port UUID </th>
</tr>
</thead>
<tbody>
<tr>
<td> External </td>
<td> 169.254.0.54/24 </td>
<td> fa:16:3e:39:c8:d8 </td>
<td> lrp-dc1ae9e3-d8fd-4451-aed8-3d6ddc5d095b </td>
</tr>
<tr>
<td> DVR-GW transit </td>
<td> 169.254.128.2/30 </td>
<td> fa:16:3e:7e:96:e7 </td>
<td> lrp-gtsp-186d8754-cc4b-40fd-9e5d-b0d26fc063bd </td>
</tr>
</tbody>
</table>


<hr />

<ul>
<li><strong>Port security</strong> - implements sanity check for all incoming packets.</li>
</ul>


<pre><code>table=0 (lr_in_admission), priority=50, match=((eth.mcast || eth.dst == fa:16:3e:7e:96:e7)
 &amp;&amp; inport == "lrp-gtsp-186d8754-cc4b-40fd-9e5d-b0d26fc063bd"), action=(next;)
</code></pre>

<ul>
<li><p><strong>IP Input</strong> - performs additional L3 sanity checks and implements typical IP services of a router (e.g. ICMP/ARP reply)
<code>
table=1 (lr_in_ip_input), priority=100, match=(ip4.src == {169.254.128.2, 169.254.128.3}),
action=(drop;)
table=1 (lr_in_ip_input), priority=90, match=(inport == "lrp-gtsp-186d8754-cc4b-40fd-9e5d-b0d26fc063bd"
&amp;&amp; arp.tpa == 169.254.128.2 &amp;&amp; arp.op == 1),
action=(eth.dst = eth.src; eth.src = fa:16:3e:7e:96:e7; arp.op = 2;
/* ARP reply */ arp.tha = arp.sha; arp.sha = fa:16:3e:7e:96:e7;
arp.tpa = arp.spa; arp.spa = 169.254.128.2;
outport = "lrp-gtsp-186d8754-cc4b-40fd-9e5d-b0d26fc063bd"; flags.loopback = 1; output;)
table=1 (lr_in_ip_input), priority=90, match=(ip4.dst == 169.254.128.2
&amp;&amp; icmp4.type == 8 &amp;&amp; icmp4.code == 0),
action=(ip4.dst &lt;-&gt; ip4.src; ip.ttl = 255; icmp4.type = 0; flags.loopback = 1; next; )
</code></p></li>
<li><p><strong>UNSNAT</strong> - translates the destination IP to the real address for packets coming from <strong>external</strong> networks
<code>
table=3 (lr_in_unsnat), priority=100, match=(ip &amp;&amp; ip4.dst == 169.254.0.54),
action=(ct_snat; next;)
</code></p></li>
<li><p><strong>DNAT</strong> - implements what is commonly known as static NAT, i.e. performs one-to-one destination IP translation for every configured floating IP.
<code>
table=4 (lr_in_dnat), priority=100, match=(ip &amp;&amp; ip4.dst == 169.254.0.52),
action=(flags.loopback = 1; ct_dnat(10.0.0.5);)
</code></p></li>
<li><p><strong>IP routing</strong> - implements L3 forwarding based on the destination IP address. At this stage the <code>outport</code> is decided, IP TTL is decremented and the new next-hop IP is set in register0.</p></li>
</ul>


<pre><code>table=5 (lr_in_ip_routing), priority=1, match=(ip4.dst == 0.0.0.0/0),
  action=(ip.ttl--; reg0 = 169.254.0.1; reg1 = 169.254.0.54; eth.src = fa:16:3e:39:c8:d8;
  outport = "lrp-dc1ae9e3-d8fd-4451-aed8-3d6ddc5d095b"; flags.loopback = 1; next;)
</code></pre>

<ul>
<li><p><strong>Next Hop Resolver</strong> - discovers the next-hop MAC address for a packet. This could either be a statically configured value when the next-hop is an OVN-managed router or a dynamic binding learned through ARP and stored in a special <code>MAC_Binding</code> table of Southbound DB.
<code>
table=6 (lr_in_arp_resolve), priority=100, match=(outport == "lrp-gtsp-186d8754-cc4b-40fd-9e5d-b0d26fc063bd"
&amp;&amp; reg0 == 169.254.128.1), action=(eth.dst = fa:16:3e:2a:7f:25; next;)
table=6 (lr_in_arp_resolve), priority=0, match=(ip4),
action=(get_arp(outport, reg0); next;)
</code></p></li>
<li><p><strong>SNAT</strong> - implements what is commonly known as overload NAT. Translates source IP, source UDP/TCP port number and ICMP Query ID to hide them behind a single IP address</p></li>
</ul>


<pre><code>table=0 (lr_out_snat), priority=25, match=(ip &amp;&amp; ip4.src == 10.0.0.0/24),
  action=(ct_snat(169.254.0.54);)
</code></pre>

<ul>
<li><strong>Output</strong> - send the packet out the port determined during the IP routing stage.</li>
</ul>


<pre><code>table=1 (lr_out_delivery), priority=100, match=(outport == "lrp-gtsp-186d8754-cc4b-40fd-9e5d-b0d26fc063bd"),
  action=(output;)
</code></pre>

<p>This was a very high-level, abridged and simplified version of how logical datapaths are built in OVN. Hopefully this lays enough groundwork to move on to the official <a href="http://openvswitch.org/support/dist-docs/ovn-northd.8.html">northd documentation</a> which describes both L2 and L3 datapaths in much greater detail.</p>

<p>Apart from the logical flows, Southbound DB also contains a number of tables that establish the logical-to-physical bindings. For example, the <code>Port_Binding</code> table establishes binding between logical switch, logical port, logical port overlay ID (a.k.a. tunnel key) and the unique hypervisor ID. In the next section we&rsquo;ll see how this information is used to translate logical flows into OpenFlow flows at each compute node. For full description of Southbound DB, its tables and their properties refer to the official SB <a href="http://openvswitch.org/support/dist-docs/ovn-sb.5.html">schema documentation</a>.</p>

<h2>OVN Controller - OpenFlow flows</h2>

<p><a href="http://openvswitch.org/support/dist-docs/ovn-controller.8.html">OVN Controller</a> process is the distributed part of OVN SDN controller. This process, running on each compute node, connects to Southbound DB via OVSDB and configures local OVS according to information received from it. It also uses Southbound DB to exchange the physical location information with other hypervisors. The two most important bits of information that OVN controller contributes to Southbound DB are physical location of logical ports and overlay tunnel IP address. These are the last two missing pieces to map logical flows to physical nodes and networks.</p>

<p>The whole flat space of OpenFlow tables is split into multiple areas. Tables 16 to 47 implement an ingress logical pipeline and tables 48 to 63 implement an egress logical pipeline. These tables have no notion of physical ports and are functionally equivalent to logical flows in Southbound DB. Tables 0 and 65 are responsible for mapping between the physical and logical realms. In table 0 packets are matched on the physical incoming port and assigned to a correct logical datapath as was defined by the <code>Port_Binding</code> table. In table 65 the information about the outport, that was determined during the ingress pipeline processing, is mapped to a local physical interface and the packet is sent out.</p>

<p>To demonstrate the details of OpenFlow implementation, I&rsquo;ll use the traffic flow between VM1 and external destination (8.8.8.8). For the sake of brevity I will only cover the major steps of packet processing inside OVS, omitting security checks and ARP/DHCP processing.</p>

<p><img class="center" src="/images/ovn-zoom4-openflow.png"></p>

<p>When packets traverse OpenFlow tables they get labelled or annotated with special values to simplify matching in subsequent tables. For example, when table 0 matches the incoming port, it annotates the packet with the datapath ID. Since it would have been impractical to label packets with globally unique UUIDs from Soutbound DB, these UUIDs get mapped to smaller values called <strong>tunnel keys</strong>. To make things even more confusing, each port will have a local kernel ID, unique within each hypervisor. We&rsquo;ll need both tunnel keys and local port IDs to be able to track the packets inside the OVS. The figure below depicts all port and datapath IDs that have been collected from the Soutbound DB and local OVSDB on each hypervisor. Local port numbers are attached with a dotted line to their respective tunnel keys.</p>

<p><img class="center" src="/images/ovn-zoom4-tunnelkey.png"></p>

<p>When VM1 sends the first packet to 8.8.8.8, it reaches OVS on local port 13. OVN Controller knows that this port belongs to VM1 and installs an OpenFlow rule to match all packets from this port and annotate them with datapath ID (OXM_OF_METADATA), incoming port ID (NXM_NX_REG14), conntrack zone (NXM_NX_REG13). It then moves these annotated packets to the first table of the ingress pipeline.
<code>
table=0, priority=100,in_port=13 actions=load:0x2-&gt;NXM_NX_REG13[],
  load:0x2-&gt;OXM_OF_METADATA[],load:0x2-&gt;NXM_NX_REG14[],
  resubmit(,16)
</code></p>

<p>Skipping to the L2 MAC address lookup stage, the output port (0x1) is decided based on the destination MAC address and saved in register 15.
<code>
table=29, priority=50,metadata=0x2,dl_dst=fa:16:3e:0d:df:ea
  actions=load:0x1-&gt;NXM_NX_REG15[],resubmit(,32)
</code></p>

<p>Finally, the packet reaches the last table where it is sent out the physical patch port interface towards R1.
<code>
table=65, priority=100,reg15=0x1,metadata=0x2 actions=output:1
</code></p>

<p>The other end of this patch port is connected to a local instance of distributed router R1. That means our packet, unmodified, re-enters OpenFlow table 0, only this time on a different port. Local port 2 is associated with a logical pipeline of a router, hence <code>metadata</code> for this packet is set to 4.
<code>
table=0, priority=100,in_port=2 actions=load:0x4-&gt;OXM_OF_METADATA[],
  load:0x1-&gt;NXM_NX_REG14[],resubmit(,16)
</code></p>

<p>The packet progresses through logical router datapath and finally gets to table 21 where destination IP lookup take place. It matches the catch-all <strong>default route</strong> rule and the values for its next-hop IP (0xa9fe8002), MAC address (fa:16:3e:2a:7f:25) and logical output port (0x03) are set.
<code>
table=21, priority=1,ip,metadata=0x4 actions=dec_ttl(),load:0xa9fe8002-&gt;NXM_NX_XXREG0[96..127],
  load:0xa9fe8001-&gt;NXM_NX_XXREG0[64..95],mod_dl_src:fa:16:3e:2a:7f:25,
  load:0x3-&gt;NXM_NX_REG15[],load:0x1-&gt;NXM_NX_REG10[0],resubmit(,22)
</code></p>

<p>Table 65 converts the logical output port 3 to physical port 6, which is yet another patch port connected to a transit switch.
<code>
table=65, priority=100,reg15=0x3,metadata=0x4 actions=output:6
</code></p>

<p>The packet once again re-enters OpenFlow pipeline from table 0, this time from port 5. Table 0 maps incoming port 5 to the logical datapath of a transit switch with Tunnel key 7.</p>

<pre><code>table=0, priority=100,in_port=5 actions=load:0x7-&gt;OXM_OF_METADATA[],
  load:0x1-&gt;NXM_NX_REG14[],resubmit(,16)
</code></pre>

<p>Destination lookup determines the output port (2) but this time, instead of entering the egress pipeline locally, the packet gets sent out the physical tunnel port (7) which points to the IP address of a compute node hosting the GW router. The headers of an overlay packet are populated with logical datapath ID (0x7), logical input port (copied from register 14) and logical output port (0x2).
<code>
table=29, priority=50,metadata=0x7,dl_dst=fa:16:3e:7e:96:e7
  actions=load:0x2-&gt;NXM_NX_REG15[],resubmit(,32)
table=32, priority=100,reg15=0x2,metadata=0x7 actions=load:0x7-&gt;NXM_NX_TUN_ID[0..23],
  set_field:0x2/0xffffffff-&gt;tun_metadata0,move:NXM_NX_REG14[0..14]-&gt;NXM_NX_TUN_METADATA0[16..30],
  output:7
</code></p>

<p>When packet reaches the destination node, it once again enters the OpenFlow table 0, but this time all information is extracted from the tunnel keys.</p>

<pre><code>table=0, priority=100,in_port=17 actions=move:NXM_NX_TUN_ID[0..23]-&gt;OXM_OF_METADATA[0..23],
  move:NXM_NX_TUN_METADATA0[16..30]-&gt;NXM_NX_REG14[0..14],
  move:NXM_NX_TUN_METADATA0[0..15]-&gt;NXM_NX_REG15[0..15],
  resubmit(,33)
</code></pre>

<p>At the end of the transit switch datapath the packet gets sent out port 12, whose peer is patch port 16.
<code>
table=65, priority=100,reg15=0x2,metadata=0x7 actions=output:12
</code></p>

<p>The packet re-enters OpenFlow table 0 from port 16, where it gets mapped to the logical datapath of a gateway router.
<code>
table=0, priority=100,in_port=16 actions=load:0x2-&gt;NXM_NX_REG11[],
  load:0x6-&gt;NXM_NX_REG12[],load:0x6-&gt;OXM_OF_METADATA[],
  load:0x2-&gt;NXM_NX_REG14[],resubmit(,16)
</code></p>

<p>Similar to a distributed router R1, table 21 determines the next-hop MAC address for a packet and saves the output port in register 15.
<code>
table=21, priority=1,ip,metadata=0x6 actions=dec_ttl(),load:0xa9fe0001-&gt;NXM_NX_XXREG0[96..127],
  load:0xa9fe0036-&gt;NXM_NX_XXREG0[64..95],mod_dl_src:fa:16:3e:39:c8:d8,
  load:0x1-&gt;NXM_NX_REG15[],load:0x1-&gt;NXM_NX_REG10[0],resubmit(,22)
</code></p>

<p>The first table of an egress pipeline source-NATs packets to external IP address of the GW router.
<code>
table=48, priority=33,ip,metadata=0x6,nw_src=10.0.0.2
  actions=ct(commit,table=49,zone=NXM_NX_REG12[0..15],nat(src=169.254.0.56))
</code></p>

<p>The modified packet is sent out the physical port 14 towards the external switch.
<code>
table=65, priority=100,reg15=0x1,metadata=0x6 actions=output:14
</code></p>

<p>External switch determines the output port connected to the <code>br-ex</code> on a local hypervisor and send the packet out.
<code>
table=0, priority=100,in_port=13 actions=load:0x5-&gt;NXM_NX_REG11[],
  load:0x3-&gt;NXM_NX_REG12[],load:0x3-&gt;OXM_OF_METADATA[],
  load:0x2-&gt;NXM_NX_REG14[],resubmit(,16)
table=29, priority=0,metadata=0x3 actions=load:0xfffe-&gt;NXM_NX_REG15[],resubmit(,32)
table=33, priority=100,reg15=0xfffe,metadata=0x3
  actions=load:0x1-&gt;NXM_NX_REG13[],load:0x1-&gt;NXM_NX_REG15[],
  resubmit(,34),load:0xfffe-&gt;NXM_NX_REG15[]
table=65, priority=100,reg15=0x1,metadata=0x3 actions=output:15
</code></p>

<p>As we&rsquo;ve just seen, OpenFlow repeats the logical topology by interconnecting logical datapaths of switches and routers with virtual point-to-point patch cables. This may seem like an unnecessary modelling element with a potential for a performance impact. However, when flows get installed in kernel datapath, these patch ports <a href="http://galsagie.github.io/2015/11/23/ovn-l3-deepdive">do not exist</a>, which means that there isn&rsquo;t any performance impact on packets in fastpath.</p>

<h2>Physical network - GENEVE overlay</h2>

<p>Before we wrap up, let us have a quick look at the new overlay protocol GENEVE. The goal of any overlay protocol is to transport all the necessary tunnel keys. With VXLAN the only tunnel key that could be transported is the Virtual Network Identifier (VNI). In OVN&rsquo;s case these tunnel keys include not only the logical datapath ID (commonly known as VNI) but also both input and output port IDs. You could have carved up the 24 bits of VXLAN tunnel ID to encode all this information but this would only have given you 256 unique values per key. Some other overlay protocols, like STT have even bigger tunnel ID header size but they, too, have a strict upper limit.</p>

<p>GENEVE was designed to have a variable-length header. The first few bytes are well-defined fixed size fields followed by variable-length Options. This kind of structure allows software developers to innovate at their own pace while still getting the benefits of hardware offload for the fixed-size portion of the header. OVN developers <a href="http://openvswitch.org/support/dist-docs/ovn-architecture.7.html">decided</a> to use Options header type 0x80 to store the 15-bit logical ingress port ID and a 16-bit egress port ID (an extra bit is for logical multicast groups).</p>

<p><img class="center" src="/images/ovn-zoom5-geneve.png"></p>

<p>The figure above shows the ICMP ping coming from VM1(10.0.0.2) to Google&rsquo;s DNS. As I&rsquo;ve showed in the previous section, GENEVE is used between the ingress and egress pipelines of a transit switch (SWtr), whose datapath ID is encoded in the VNI field (0x7). Packets enter the transit switch on port 1 and leave it on port 2. These two values are encoded in the <code>00010002</code> value of the <code>Options Data</code> field.</p>

<p>So now that GENEVE has taken over as the inter-hypervisor overlay protocol, does that mean that VXLAN is dead? OVN still supports VXLAN but only for interconnects with 3rd party devices like VXLAN-VLAN gateways or VXLAN TOR switches. Rephrasing the official OVN <a href="http://openvswitch.org/support/dist-docs/ovn-architecture.7.html">documentation</a>, VXLAN gateways will continue to be supported but they will have a reduced feature set due to lack of extensibility.</p>

<h2>Conclusion</h2>

<p>OpenStack networking has always been one of the first use cases of any new SDN controller. All the major SDN platforms like ACI, NSX, Contrail, VSP or ODL have some form of OpenStack integration. And it made sense, since native Neutron networking has always been one of the biggest pain points in OpenStack deployments. As I&rsquo;ve just demonstrated, OVN can now do all of the common networking functionality natively, without having to rely on 3rd party agents. In addition to that it has a fantastic <a href="http://openvswitch.org/support/dist-docs/">documentation</a>, implements all forwarding inside a single OVS bridge and it is an open-source project. As an OpenStack networking solution it is still, perhaps, a few months away from being production ready - active/active HA is not supported with OVSDB, GW router scheduling options are limited, lack of native support for DNS and Metadata proxy. However I anticipate that starting from the next OpenStack release (Ocata, Feb 2017) OVN will be ready for mass deployment even by companies without an army of OVS/OpenStack developers. And when that happens there will even less need for proprietary OpenStack SDN platforms.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenStack SDN With OVN (Part 1) - Build and Install]]></title>
    <link href="http://networkop.github.io/blog/2016/11/27/ovn-part1/"/>
    <updated>2016-11-27T00:00:00+00:00</updated>
    <id>http://networkop.github.io/blog/2016/11/27/ovn-part1</id>
    <content type="html"><![CDATA[<p>This is a first of a two-post series dedicated to project OVN. In this post I&rsquo;ll show how to build, install and configure OVN to work with a 3-node RDO OpenStack lab.</p>

<!--more-->


<p>Vanilla OpenStack networking has many functional, performance and scaling limitations. Projects like <a href="/blog/2016/05/06/neutron-l2pop/">L2 population</a>, <a href="/blog/2016/05/06/neutron-l2pop/">local ARP responder</a>, <a href="/blog/2016/05/21/neutron-l2gw/">L2 Gateway</a> and <a href="/blog/2016/10/13/os-dvr/">DVR</a> were conceived to address those issues. However good a job these projects do, they still remain a collection of separate projects, each with its own limitations, configuration options and sets of dependencies. That led to an effort outside of OpenStack to develop a special-purpose OVS-only SDN controller that would address those issues in a centralised and consistent manner. This post will be about one such SDN controller, coming directly from the people responsible for OpenvSwitch, Open Virtual Network (OVN).</p>

<h2>OVN quick introduction</h2>

<p>OVN is a distributed SDN controller implementing virtual networks with the help OVS. Even though it is positioned as a <abbr title="Cloud Management System">CMS</abbr>-independent controller, the main use case is still OpenStack. OVN was designed to address the following limitations of vanilla OpenStack networking:</p>

<ul>
<li>Security groups could not be implemented directly on OVS ports and, therefore, required a dedicated Linux bridge between the VM and the OVS integration bridge.</li>
<li>Routing and DHCP agents required dedicated network namespaces.</li>
<li>NAT was implemented using a combination of network namespaces, iptables and proxy-ARP.</li>
</ul>


<p>OVN implements security groups, distributed virtual routing, NAT and distributed DHCP server all inside a single OVS bridge. This dramatically improves performance by reducing the number of inter-process packet handling and ensures that all flows can benefit from kernel fast-path switching.</p>

<p>At a high level, OVN consists of 3 main components:</p>

<ol>
<li>OVN ML2 Plugin - performs translation between Neutron data model and OVN logical data model stored in Northbound DB.</li>
<li>OVN northd - the brains of OVN, translates the high level networking abstractions (logical switches, routers and ports) into logical flows. These <a href="https://blog.russellbryant.net/2016/11/11/ovn-logical-flows-and-ovn-trace/">logical flows</a> are not yet OpenFlow flows but similar in concept and a very powerful abstraction. All translated information is stored in Southbound DB.</li>
<li>OVN controllers - located on each compute node, receive identical copies of logical flows (centralised network view) and exchange logical port to overlay IP binding information via the central Southbound DB. This information is used to perform logical flow translation into OpenFlow which are then programmed into the local OVS instance.</li>
</ol>


<p><img class="center" src="/images/ovn-arch.png"></p>

<p>If you want to learn more about OVN architecture and use cases, <a href="http://docs.openstack.org/developer/networking-ovn/readme.html">OpenStack OVN page</a> has an excellent collection of resources for further reading.</p>

<h2>OpenStack installation</h2>

<p>I&rsquo;ll use RDO packstack to help me build a 1 controller and 2 compute nodes OpenStack lab on CentOS7. I&rsquo;ll use the master trunk to deploy the latest OpenStack Ocata packages. This is required since at the time of writing (Nov 2016) some of the OVN features were not available in OpenStack Newton.</p>

<pre><code class="bash Install latest RDO repositories on all 3 nodes">cd /etc/yum.repos.d/
wget http://trunk.rdoproject.org/centos7/delorean-deps.repo
wget https://trunk.rdoproject.org/centos7-master/current/delorean.repo
</code></pre>

<p>On the controller node, generate a sample answer file and modify settings to match the IPs of individual nodes. Optionally, you can disable some of the unused components like Nagios and Ceilometer similar to how I did it in my <a href="http://networkop.co.uk/blog/2016/04/18/os-unl-lab/">earlier post</a>.</p>

<pre><code class="bash Modify answer file and deploy OpenStack">yum install -y openstack-packstack crudini
packstack --gen-answer-file=/root/packstack.answer
crudini --set --existing defautl CONFIG_COMPUTE_HOSTS 169.254.0.12,169.254.0.13
crudini --set --existing defautl CONFIG_CONTROLLER_HOST 169.254.0.11
crudini --set --existing defautl CONFIG_NETWORK_HOSTS 169.254.0.11
packstack --answer-file=/root/packstack.answer
</code></pre>

<p>After the last step we should have a working 3-node OpenStack lab, similar to the one depicted below. If you want to learn about how to automate this process, refer to my older posts about <a href="/blog/2016/08/26/os-lab-p1/">OpenStack</a> and <a href="/blog/2016/09/09/os-lab-p2/">underlay Leaf-Spine fabric</a> build using Chef.</p>

<p><img class="center" src="/images/ovn-openstack.png"></p>

<h2>OVN Build</h2>

<p>OVN can be built directly from OVS source code. Instead of building and installing OVS on each of the OpenStack nodes individually, I&rsquo;ll build a set of RPM&rsquo;s on the Controller and will use them to install and upgrade OVS/OVN components on the remaining nodes.</p>

<p>Part of OVN build process includes building an OVS kernel module. In order to be able to use kmod RPM on all nodes we need to make sure all nodes use the same version of Linux kernel. The easiest way would be to fetch the latest updates from CentOS repos and reboot the nodes. This step should result in same kernel version on all nodes, which can be checked with <code>uname -r</code> command.</p>

<pre><code class="bash Upgrade kernel on all nodes">yum -y update kernel
reboot
</code></pre>

<p>The official <a href="https://github.com/openvswitch/ovs/blob/master/INSTALL.Fedora.rst">OVS installation procedure for CentOS7</a> is pretty accurate and requires only a few modifications to account for the packages missing in the minimal CentOS image I&rsquo;ve used as a base OS.</p>

<pre><code class="bash OVS build procedure">yum install rpm-build autoconf automake libtool systemd-units openssl openssl-devel python python-twisted-core python-zope-interface python-six desktop-file-utils groff graphviz procps-ng libcap-ng libcap-ng-devel
yum install selinux-policy-devel kernel-devel-`uname -r` git
git clone https://github.com/openvswitch/ovs.git
cd ovs
./boot.sh
./configure
make rpm-fedora RPMBUILD_OPT="--without check"
make rpm-fedora-kmod
</code></pre>

<p>At the end of the process we should have a set of rpms inside the <code>ovs/rpm/rpmbuild/RPMS/</code> directory.</p>

<h2>OVN Install</h2>

<p>Before we can begin installing OVN, we need to prepare the existing OpenStack environment by disabling and removing legacy Neutron OpenvSwitch agents. Since OVN natively implements L2 and L3 forwarding, DHCP and NAT, we won&rsquo;t need L3 and DHCP agents on any of the Compute nodes. Network node that used to provide North-South connectivity will no longer be needed.</p>

<h3>OpenStack preparation</h3>

<p>First, we need to make sure all Compute nodes have a bridge that would provide access to external provider networks. In my case, I&rsquo;ll move the <code>eth1</code> interface under the OVS <code>br-ex</code> on all Compute nodes.</p>

<pre><code class="bash /etc/sysconfig/network-scripts/ifcfg-eth1">DEVICE=eth1
NAME=eth1
DEVICETYPE=ovs
TYPE=OVSPort
OVS_BRIDGE=br-ex
ONBOOT=yes
BOOTPROTO=none
</code></pre>

<p>IP address needs to be moved to <code>br-ex</code> interface. Below example is for Compute node #2:</p>

<pre><code class="bash /etc/sysconfig/network-scripts/ifcfg-br-ex">ONBOOT=yes
DEFROUTE=yes
IPADDR=169.254.0.12
PREFIX=24
GATEWAY=169.254.0.1
DNS1=8.8.8.8
DEVICE=br-ex
NAME=br-ex
DEVICETYPE=ovs
OVSBOOTPROTO=none
TYPE=OVSBridge
</code></pre>

<p>At the same time OVS configuration on Network/Controller node will need to be completely wiped out. Once that&rsquo;s done, we can remove the Neutron OVS package from all nodes.</p>

<pre><code class="bash Remove legacy Neutron OVS agents">yum remove openstack-neutron-openvswitch
</code></pre>

<h3>OVS packages installation</h3>

<p>Now everything is ready for OVN installation. First step is to install the kernel module and upgrade the existing OVS package. Reboot may be needed in order for the correct kernel module to be loaded.</p>

<pre><code class="bash Upgrade OVS on all nodes">rpm -i openvswitch-kmod-2.6.90-1.el7.centos.x86_64.rpm
rpm -U openvswitch-2.6.90-1.el7.centos.x86_64.rpm
reboot
</code></pre>

<p>Now we can install OVN. Controllers will be running the <code>ovn-northd</code> process which can be installed as follows:</p>

<pre><code class="bash Install OVN on the Controller">rpm -i openvswitch-ovn-common-*.x86_64.rpm
rpm -i openvswitch-ovn-central-*.x86_64.rpm
systemctl start ovn-northd
</code></pre>

<p>The following packages install the <code>ovn-controller</code> on all Compute nodes:</p>

<pre><code class="bash Install OVN on Compute nodes">rpm -i openvswitch-ovn-common-*.x86_64.rpm
rpm -i openvswitch-ovn-host-*.x86_64.rpm
systemctl start ovn-controller
</code></pre>

<p>The last thing is to install the OVN ML2 plugin, a python library that allows Neutron to talk to OVN Northbound database.</p>

<pre><code class="bash Install OVN ML2 plugin on the Controller">yum install python-networking-ovn
</code></pre>

<h2>OVN Configuration</h2>

<p>Now that we have all the required packages in place, it&rsquo;s time to reconfigure Neutron to start using OVN instead of a default openvswitch plugin. The installation procedure is described in the official <a href="http://docs.openstack.org/developer/networking-ovn/index.html">Neutron integration guide</a>. At the end, once we&rsquo;ve restarted <code>ovn-northd</code> on the controller and <code>ovn-controller</code> on the compute nodes, we should see the following output on the controller node:</p>

<pre><code class="bash ovs-sbctl show">Chassis "d03bdd51-e687-4078-aa54-0ff8007db0b5"
    hostname: "compute-3"
    Encap geneve
        ip: "10.0.0.4"
        options: {csum="true"}
    Encap vxlan
        ip: "10.0.0.4"
        options: {csum="true"}
Chassis "b89b8683-7c74-43df-8ac6-1d57ddefec77"
    hostname: "compute-2"
    Encap vxlan
        ip: "10.0.0.2"
        options: {csum="true"}
    Encap geneve
        ip: "10.0.0.2"
        options: {csum="true"}
</code></pre>

<p>This means that all instances of a distributed OVN controller located on each compute node have successfully registered with Southbound OVSDB and provided information about their physical overlay addresses and supported encapsulation types.</p>

<h2>(Optional) Automating everything with Chef</h2>

<p>At this point of time there&rsquo;s no way to automate OVN deployment with Packstack (TripleO already has OVN integration templates). For those who want to bypass the manual build process I have created a new Chef cookbook, automating all steps described above. This Chef playbook assumes that OpenStack environment has been built as described in my <a href="/blog/2016/08/26/os-lab-p1/">earlier post</a>. Optionally, you can automate the build of underlay network as well by following my <a href="/blog/2016/09/09/os-lab-p2/">other post</a>. Once you&rsquo;ve got both OpenStack and underlay built, you can use the following scripts to build, install and configure OVN:</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>git clone &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://github.com/networkop/chef-unl-os.git&quot;</span>&gt;https://github.com/networkop/chef-unl-os.git&lt;/a&gt;
</span><span class='line'><span class="nb">cd </span>chef-unl-os
</span><span class='line'>chef-client -z -E lab ovn.rb
</span></code></pre></td></tr></table></div></figure></p>

<h2>Test topology setup</h2>

<p>Now we should be able to create a test topology with two tenant subnets and an external network interconnected by a virtual router.</p>

<pre><code class="bash Creating a test topology">neutron net-create NET-RED
neutron net-create NET-BLUE
neutron subnet-create --name SUB-BLUE NET-BLUE 10.0.0.0/24
neutron subnet-create --name SUB-RED NET-RED 20.0.0.0/24
neutron net-create NET-EXT --provider:network_type flat \
                           --provider:physical_network extnet \
                           --router:external --shared
neutron subnet-create --name SUB-EXT --enable_dhcp=False \
                      --allocation-pool=start=169.254.0.50,end=169.254.0.99 \
                      --gateway=169.254.0.1 NET-EXT 169.254.0.0/24
neutron router-create R1
neutron router-interface-add R1 SUB-BLUE
neutron router-interface-add R1 SUB-RED
neutron router-gateway-set R1 NET-EXT
</code></pre>

<p>When we attach a few test VMs to each subnet we should be able to successfully ping between the VMs, assuming the security groups are setup to allow ICMP/ND.</p>

<pre><code class="bash Create VMs">curl http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img | glance \
image-create --name='IMG-CIRROS' \
  --visibility=public \
  --container-format=bare \
  --disk-format=qcow2
nova aggregate-create AGG-RED AZ-RED
nova aggregate-create AGG-BLUE AZ-BLUE
nova aggregate-add-host AGG-BLUE compute-2
nova aggregate-add-host AGG-RED compute-3
nova boot --flavor m1.tiny --image 'IMG-CIRROS' \
  --nic net-name=NET-BLUE \
  --availability-zone AZ-BLUE \
  VM1

nova boot --flavor m1.tiny --image 'IMG-CIRROS' \
  --nic net-name=NET-RED \
  --availability-zone AZ-RED \
  VM2
nova boot --flavor m1.tiny --image 'IMG-CIRROS' \
  --nic net-name=NET-BLUE \
  --availability-zone AZ-RED \
  VM3
openstack floating ip create NET-EXT
openstack server add floating ip VM3 169.254.0.53
</code></pre>

<p><img class="center" src="/images/ovn-topo.png"></p>

<p>In the next post we will use the above virtual topology to explore the dataplane packet flow inside an OVN-managed OpenvSwitch and how it uses the new encapsulation protocol GENEVE to optimise egress forwarding lookups on remote compute nodes.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenStack SDN - Distributed Virtual Routing]]></title>
    <link href="http://networkop.github.io/blog/2016/10/13/os-dvr/"/>
    <updated>2016-10-13T00:00:00+01:00</updated>
    <id>http://networkop.github.io/blog/2016/10/13/os-dvr</id>
    <content type="html"><![CDATA[<p>In this post we&rsquo;ll explore how DVR is implemented in OpenStack Neutron and what are some of its benefits and shortcomings.</p>

<!--more-->


<p>To be honest I was a little hesitant to write this post because the topic of Neutron&rsquo;s DVR has already been <abbr title="beaten to death">exhaustively covered</abbr> by many, including <a href="https://assafmuller.com/category/dvr/">Assaf Muller</a>, <a href="http://blog.gampel.net/2014/12/openstack-neutron-distributed-virtual.html">Eran Gampel</a> and in the official OpenStack <a href="http://docs.openstack.org/mitaka/networking-guide/scenario-dvr-ovs.html">networking guide</a>. The coverage of the topic was so thorough that I barely had anything to add. However I still decided to write a DVR post of my own for the following two reasons:</p>

<ol>
<li>I often use my own posts as references and it&rsquo;s always easier for me to find information in my own writings.</li>
<li>I wanted to use this post as a reference platform for subsequent posts about dynamic routing and OVN project.</li>
</ol>


<p>The topic of Neutron&rsquo;s DVR is quite vast so I had to compromise between the length of this post and the level of details. In the end, I edited out most of the repeated content and replaced it with references to my older posts. I think I left everything that should be needed to follow along the narrative so hopefully it won&rsquo;t seem too patchy.</p>

<h2>Virtual topology overview</h2>

<p>Let&rsquo;s see what we&rsquo;re going to be dealing with in this post. This is a simple virtual topology with two VMs sitting in two different subnets. VM1 has a floating IP assigned that is used for external access.</p>

<p><img class="center" src="/images/dvr-topo.png"></p>

<p>Before we get to the packet walk details, let me briefly describe how to build the above topology using Neutron CLI. I&rsquo;ll assume that OpenStack has just been installed and nothing has been configured yet, effectively we&rsquo;ll pick up from where we left our lab in the <a href="http://networkop.github.io/blog/2016/09/09/os-lab-p2/">previous post</a>.</p>

<h2>Virtual topology setup</h2>

<ol>
<li>Upload Cirros Linux image to OpenStack's image repository

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img | glance \
</span><span class='line'>     image-create --name='IMG-CIRROS' \
</span><span class='line'>     --visibility=public \
</span><span class='line'>     --container-format=bare \
</span><span class='line'>     --disk-format=qcow2</span></code></pre></td></tr></table></div></figure>
</li>
<li>Create 2 virtual subnets - RED and BLUE

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>neutron net-create NET-BLUE
</span><span class='line'>neutron subnet-create --name SUB-BLUE NET-BLUE 10.0.0.0/24 \
</span><span class='line'>  --dns-nameserver 8.8.8.8
</span><span class='line'>
</span><span class='line'>neutron net-create NET-RED
</span><span class='line'>neutron subnet-create --name SUB-RED NET-RED 10.0.1.0/24 \
</span><span class='line'>  --dns-nameserver 8.8.8.8</span></code></pre></td></tr></table></div></figure>
</li>
<li>Create external network

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>neutron net-create EXT-NET --provider:network_type flat \
</span><span class='line'>  --provider:physical_network extnet  \
</span><span class='line'>  --router:external \
</span><span class='line'>  --shared
</span><span class='line'>
</span><span class='line'>neutron subnet-create --name EXT-SUB \
</span><span class='line'> --enable_dhcp=False \
</span><span class='line'> --allocation-pool=start=169.254.0.50,end=169.254.0.99 \
</span><span class='line'> --gateway=169.254.0.1 EXT-NET 169.254.0.0/24</span></code></pre></td></tr></table></div></figure>
</li>

<li>Create a router and attach it to all three networks created above

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>neutron router-create R1
</span><span class='line'>neutron router-interface-add R1 SUB-RED
</span><span class='line'>neutron router-interface-add R1 SUB-BLUE
</span><span class='line'>neutron router-gateway-set R1 EXT-NET</span></code></pre></td></tr></table></div></figure>
</li>

<li>Create two host aggregates to spread the VMs across two different hosts

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>nova aggregate-create AGG-RED AZ-RED
</span><span class='line'>nova aggregate-create AGG-BLUE AZ-BLUE
</span><span class='line'>nova aggregate-add-host AGG-BLUE compute-2
</span><span class='line'>nova aggregate-add-host AGG-RED compute-3</span></code></pre></td></tr></table></div></figure>
</li>

<li>Boot VMs on two different hypervisors

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>nova boot --flavor m1.tiny --image 'IMG-CIRROS' \
</span><span class='line'>     --nic net-name=NET-BLUE \
</span><span class='line'>    --availability-zone AZ-BLUE \
</span><span class='line'>     VM1
</span><span class='line'>
</span><span class='line'>nova boot --flavor m1.tiny --image 'IMG-CIRROS' \
</span><span class='line'>    --nic net-name=NET-RED \
</span><span class='line'>    --availability-zone AZ-RED \
</span><span class='line'>    VM2</span></code></pre></td></tr></table></div></figure>
</li>

<li>Assign a floating IP (Static NAT) to VM1

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>nova floating-ip-create EXT-NET
</span><span class='line'>nova floating-ip-associate VM1 169.254.0.55</span></code></pre></td></tr></table></div></figure>
</li>

<li>Enable ingress ICMP and SSH access

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>nova secgroup-add-rule default tcp 22 22 0.0.0.0/0
</span><span class='line'>nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0</span></code></pre></td></tr></table></div></figure>
</li>

<li>Make sure that both VMs are up and running.

<figure class='code'><figcaption><span>nova list</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>+--------------------------------------+------+--------+------------+-------------+----------------------------------+
</span><span class='line'><span class="p">|</span> ID                                   <span class="p">|</span> Name <span class="p">|</span> Status <span class="p">|</span> Task State <span class="p">|</span> Power State <span class="p">|</span> Networks                         <span class="p">|</span>
</span><span class='line'>+--------------------------------------+------+--------+------------+-------------+----------------------------------+
</span><span class='line'><span class="p">|</span> 92263ae8-43d1-4cd0-b271-2b11f0efbe7f <span class="p">|</span> VM1  <span class="p">|</span> ACTIVE <span class="p">|</span> -          <span class="p">|</span> Running     <span class="p">|</span> NET-BLUE<span class="o">=</span>10.0.0.12, 169.254.0.55 <span class="p">|</span>
</span><span class='line'><span class="p">|</span> b4562f24-2461-49fb-875b-fa1bf869dc4a <span class="p">|</span> VM2  <span class="p">|</span> ACTIVE <span class="p">|</span> -          <span class="p">|</span> Running     <span class="p">|</span> NET-RED<span class="o">=</span>10.0.1.4                 <span class="p">|</span>
</span><span class='line'>+--------------------------------------+------+--------+------------+-------------+----------------------------------+
</span></code></pre></td></tr></table></div></figure>
</li>
</ol>


<h2>non-DVR traffic flow</h2>

<p>Using the technique described in my <a href="http://networkop.github.io/blog/2016/04/22/neutron-native/">earlier post</a> I&rsquo;ve collected the dynamically allocated port numbers and created a physical representation of our virtual network.</p>

<p><img class="center" src="/images/dvr-before.png"></p>

<p>For the sake of brevity I will omit the verification commands. The traffic flow between VM1 and VM2 will follow the standard path that I&rsquo;ve explored in my <a href="http://networkop.github.io/blog/2016/04/22/neutron-native/">native Neutron SDN post</a>.<br/>
It is obvious that in this case traffic flows are suboptimal. Instead of going directly between the peer compute nodes, the packet has to hairpin through a Neutron router. This adds to the end-to-end latency and creates unnecessary load on the Network node. These are one of the main reasons why Distributed Virtual Routing was introduced in OpenStack Juno.</p>

<h2>Enabling DVR</h2>

<p>Enabling DVR requires configuration changes of multiple files on all OpenStack nodes. At a high level, all compute nodes will now run Neutron&rsquo;s L3-agent service which will be responsible for provisioning of DVR and other auxiliary namespaces. The details of specific configuration options that need to be enabled can be found in the official OpenStack <a href="http://docs.openstack.org/mitaka/networking-guide/scenario-dvr-ovs.html">Networking guide</a>. As usual, I&rsquo;ve incorporated all the necessary changes into a single Chef <a href="https://github.com/networkop/chef-unl-os/tree/master/cookbooks/neutron">cookbook</a>, so in order to enable DVR in our lab all what you need to do is run the following commands from the UNetLab VM:</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>git pull origin master
</span><span class='line'>chef-client -z -E lab neutron.rb
</span></code></pre></td></tr></table></div></figure></p>

<p>Once all changes has been made, we need to either create a new router or update the existing one to enable the DVR functionality:</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>neutron router-update <span class="p">&amp;</span>ndash<span class="p">;</span>admin-state-up False <span class="p">&amp;</span>ndash<span class="p">;</span>distributed True R1
</span><span class='line'>neutron router-update <span class="p">&amp;</span>ndash<span class="p">;</span>admin-state-up True R1
</span></code></pre></td></tr></table></div></figure></p>

<h2>DVR East-West traffic flow</h2>

<p>Now let&rsquo;s see how the traffic flows have changed with the introduction of DVR.</p>

<p><img class="center" src="/images/dvr-ew.png"></p>

<p>We&rsquo;re going to be examining the following traffic flow:</p>

<ul>
<li>From VM1 (10.0.0.12/fa:16:3e:83:92:96)</li>
<li>Via router R1 (10.0.0.1/fa:16:3e:72:7a:50; 10.0.1.1/fa:16:3e:6a:2c:8b)</li>
<li>To VM2 (10.0.1.4/fa:16:3e:76:31:68)</li>
</ul>


<p>R1 now has an instance on all compute nodes that have VMs in the BLUE or RED networks. That means that VM1 will send a packet directly to the R1&rsquo;s BLUE interface via the integration bridge.</p>

<pre><code class="bash ovs-appctl fdb/show br-int"> port  VLAN  MAC                Age
    2     1  fa:16:3e:83:92:96    1
    4     1  fa:16:3e:72:7a:50    1
    5     2  fa:16:3e:6a:2c:8b    1
</code></pre>

<p>This is dynamically populated MAC address table of the integration bridge. You can see that the MAC address of VM1 and both interfaces of R1 have been learned. That means that when VM1 sends a packet to its default gateway&rsquo;s MAC address, it will go directly to R1&rsquo;s BLUE interface on port 4.</p>

<p>In this post I will omit the details of ARP resolution process which remains the same as <a href="http://networkop.github.io/blog/2016/05/06/neutron-l2pop/">before</a>, however there&rsquo;s one interesting detail that is worth mentioning before we move on. During the initial flood-and-learn phase on the <strong>br-int</strong>, the ARP request will get flooded down to the tunnel bridge. As per the standard behaviour, the packet should get replicated to all nodes. However, in this case we don&rsquo;t want to hear responses from other nodes, since the router is hosted locally. In order to help that, tunnel bridges explicitly drop all packets coming from integration bridges and destined for MAC addresses of locally hosted routers:</p>

<p><code>bash ovs-appctl ofproto/trace br-tun in_port=1,dl_vlan=1,dl_dst=fa:16:3e:72:7a:50 | grep -E "Rule|action"
Rule: table=0 cookie=0xa3536ac94478bd1d priority=1,in_port=1
OpenFlow actions=goto_table:1
        Rule: table=1 cookie=0xa3536ac94478bd1d priority=2,dl_vlan=1,dl_dst=fa:16:3e:72:7a:50
        OpenFlow actions=drop
</code></p>

<p>Getting back to our traffic flow, once the IP packet has reached the DVR instance of R1 on compute node #2, the routing lookup occurs and the packet is sent back to the integration bridge with a new source MAC of R1&rsquo;s RED interface.</p>

<pre><code class="bash ip netns exec qrouter-uuid ip route">10.0.0.0/24 dev qr-102c4426-86  proto kernel  scope link  src 10.0.0.1
10.0.1.0/24 dev qr-3779302e-62  proto kernel  scope link  src 10.0.1.1
</code></pre>

<p>Tunnel bridge will do its usual work by locating the target compute node based on the destination MAC address of VM2 (DVR requires <a href="http://networkop.github.io/blog/2016/05/06/neutron-l2pop/">L2 population</a> to be enabled) and will send the packet directly to the compute node #3.</p>

<p><code>bash ovs-appctl ofproto/trace br-tun in_port=1,dl_vlan=2,dl_src=fa:16:3e:6a:2c:8b,dl_dst=fa:16:3e:76:31:68 | grep -E "Rule|action"
Rule: table=0 cookie=0xa3536ac94478bd1d priority=1,in_port=1
OpenFlow actions=goto_table:1
        Rule: table=1 cookie=0xa3536ac94478bd1d priority=1,dl_vlan=2,dl_src=fa:16:3e:6a:2c:8b
        OpenFlow actions=set_field:fa:16:3f:d3:10:60-&gt;eth_src,goto_table:2
                Rule: table=2 cookie=0xa3536ac94478bd1d priority=0,dl_dst=00:00:00:00:00:00/01:00:00:00:00:00
                OpenFlow actions=goto_table:20
                        Rule: table=20 cookie=0xa3536ac94478bd1d priority=2,dl_vlan=2,dl_dst=fa:16:3e:76:31:68
                        OpenFlow actions=pop_vlan,set_field:0x4d-&gt;tun_id,output:3
</code></p>

<p>Since all instances of R1 have the same set of IP/MAC addresses, the MAC address of a local router can be learned by the remote integration bridge hosting the same instance of DVR. In order to prevent that from happening, the sending <strong>br-tun</strong> replaces the source MAC address of the frame with the <code>set_field:fa:16:3f:d3:10:60-&gt;eth_src</code> action. This way the real R1&rsquo;s MAC address gets masked as the frame leaves the node. These &ldquo;mask&rdquo; MACs are generated by and learned from the Neutron server, which ensures that each node gets a unique address.</p>

<p>The receiving node&rsquo;s <strong>br-tun</strong> will swap the VXLAN header with a VLAN ID and forward the frame up to the integration bridge.</p>

<p><code>bash ovs-appctl ofproto/trace br-tun in_port=3,tun_id=0x4d,dl_dst=fa:16:3e:76:31:68 | grep -E "Rule|action"
Rule: table=0 cookie=0x8a7dedf35101427f priority=1,in_port=3
OpenFlow actions=goto_table:4
        Rule: table=4 cookie=0x8a7dedf35101427f priority=1,tun_id=0x4d
        OpenFlow actions=push_vlan:0x8100,set_field:4097-&gt;vlan_vid,goto_table:9
                Rule: table=9 cookie=0x8a7dedf35101427f priority=0
                OpenFlow actions=goto_table:10
                        Rule: table=10 cookie=0x8a7dedf35101427f priority=1
                        OpenFlow actions=learn(table=20,hard_timeout=300,priority=1,cookie=0x8a7dedf35101427f,NXM_OF_VLAN_TCI[0..11],NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],load:0-&gt;NXM_OF_VLAN_TCI[],load:NXM_NX_TUN_ID[]-&gt;NXM_NX_TUN_ID[],output:OXM_OF_IN_PORT[]),output:1
                                Rule: table=0 cookie=0x9a1e0026794eadc5 priority=1
                                OpenFlow actions=NORMAL
</code></p>

<p>Integration bridge of compute node #3 will lookup the destination MAC address and send the packet out port 2.</p>

<pre><code class="bash ovs-appctl fdb/show br-int"> port  VLAN  MAC                Age
    2     1  fa:16:3e:76:31:68    0
    4     2  fa:16:3e:72:7a:50    0
    5     1  fa:16:3e:6a:2c:8b    0
    1     1  fa:16:3e:29:de:20    0
</code></pre>

<p>The reverse packet flow is similar - the packet will get routed on the compute node #3 and sent in a BLUE network to the compute node #2.</p>

<h2>External connectivity</h2>

<p>External connectivity will be very different for VMs with and without a floating IP. We will examine each case individually.</p>

<h3>Case 1 - Overload NAT (VM2 with no FIP)</h3>

<p><img class="center" src="/images/dvr-snat.png"></p>

<p>External connectivity for VMs with no floating IP is still performed by the Network node. This time however, NATing is performed by a new element - SNAT namespace. As per the normal behaviour, VM2 will send a packet to its default gateway first. Let&rsquo;s have a closer look at the routing table of the DVR:</p>

<pre><code class="bash ip netns exec qrouter-uuid ip route">10.0.0.0/24 dev qr-102c4426-86  proto kernel  scope link  src 10.0.0.1
10.0.1.0/24 dev qr-3779302e-62  proto kernel  scope link  src 10.0.1.1
</code></pre>

<p>There&rsquo;s no default route in the main routing table, so how would it get routed out? DVRs extensively use <a href="http://linux-ip.net/html/tools-ip-rule.html">Linux routing policy database</a> (RPDB), a feature that has a lot in common with OpenFlow tables. The principle of RPDB is that every packet gets matched against a set of routing tables until there&rsquo;s a hit. The tables are checked in the order of their priority (lowest to highest). One of the main features of RPDB is the ability to perform matches based on something other than the destination IP address, which is why it&rsquo;s often referred to as policy-based routing. To view the contents of RPDB use the <code>ip rule</code> command under the DVR namespace:</p>

<pre><code class="bash ip netns exec qrouter-uuid ip rule">0:      from all lookup local
32766:  from all lookup main
32767:  from all lookup default
167772161:      from 10.0.0.1/24 lookup 167772161
167772417:      from 10.0.1.1/24 lookup 167772417
</code></pre>

<p>In our case table 167772161 matches all packets sourced from the BLUE subnet and if we examine the corresponding routing table we&rsquo;ll find the missing default route there.</p>

<pre><code class="bash ip netns exec qrouter-uuid ip route list table 167772417">default via 10.0.1.12 dev qr-3779302e-62
</code></pre>

<p>The next hop of this default route points to the SNAT&rsquo;s interface in the BLUE network. MAC address is statically programmed by the local L3-agent.</p>

<pre><code class="bash ip netns exec qrouter-uuid ip neigh | grep 10.0.1.12">10.0.1.12 dev qr-3779302e-62 lladdr fa:16:3e:29:de:20 PERMANENT
</code></pre>

<p>Integration bridge sends the packet out port 1 to the tunnel bridge.</p>

<pre><code class="bash ovs-appctl fdb/show br-int | grep fa:16:3e:29:de:20">    1     1  fa:16:3e:29:de:20    1
</code></pre>

<p>Tunnel bridge finds the corresponding match and sends the VXLAN-encapsulated packet to the Network node.</p>

<p><code>bash ovs-appctl ofproto/trace br-tun in_port=1,dl_vlan=1,dl_dst=fa:16:3e:29:de:20 | tail -n 1
Datapath actions: set(tunnel(tun_id=0x4d,src=10.0.0.4,dst=10.0.0.0,ttl=64,flags(df|key))),pop_vlan,3
</code></p>

<p>Tunnel bridge of the Network node forwards the frame up to the integration bridge.</p>

<p><code>bash ovs-appctl ofproto/trace br-tun in_port=3,tun_id=0x4d,dl_dst=fa:16:3e:29:de:20 | grep output
OpenFlow actions=learn(table=20,hard_timeout=300,priority=1,cookie=0xb9be3fe62922c800,NXM_OF_VLAN_TCI[0..11],NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],load:0-&gt;NXM_OF_VLAN_TCI[],load:NXM_NX_TUN_ID[]-&gt;NXM_NX_TUN_ID[],output:OXM_OF_IN_PORT[]),output:1
</code></p>

<p>Integration bridge sends the frame to port 10, which is where SNAT namespace is attached</p>

<pre><code class="bash ovs-appctl fdb/show br-int | grep fa:16:3e:29:de:20">   10     1  fa:16:3e:29:de:20    0
</code></pre>

<p>SNAT is a namespace with an interface in each of the subnets - BLUE, RED and External subnet</p>

<pre><code class="bash ip netns exec snat-uuid ip a | grep -E "UP|inet"">16: sg-fefd493b-a5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN
    link/ether fa:16:3e:99:5c:3a brd ff:ff:ff:ff:ff:ff
    inet 10.0.0.6/24 brd 10.0.0.255 scope global sg-fefd493b-a5
18: sg-b3d58360-b4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN
    link/ether fa:16:3e:29:de:20 brd ff:ff:ff:ff:ff:ff
    inet 10.0.1.12/24 brd 10.0.1.255 scope global sg-b3d58360-b4
19: qg-765b5aca-ce: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN
    link/ether fa:16:3e:d5:75:0e brd ff:ff:ff:ff:ff:ff
    inet 169.254.0.57/24 brd 169.254.0.255 scope global qg-765b5aca-ce
</code></pre>

<p>SNAT has a single default route pointing to the External network&rsquo;s gateway.</p>

<pre><code class="bash ip netns exec snat-uuid ip route | grep default">default via 169.254.0.1 dev qg-765b5aca-ce
</code></pre>

<p>Before sending the packet out, iptables will NAT the packet to hide it behind SNAT&rsquo;s <strong>qg</strong> external interface IP.</p>

<pre><code class="bash ip netns exec snat-uuid iptables -t nat -L | grep SNAT">SNAT       all  --  anywhere             anywhere             to:169.254.0.57
</code></pre>

<h3>Case 2 - Static NAT (VM1 with FIP)</h3>

<p><img class="center" src="/images/dvr-dnat.png"></p>

<p>The first step in this scenario is the same - VM1 sends a packet to the MAC address of its default gateway. As before, the default route is missing in the main routing table.</p>

<pre><code class="bash ip netns exec qrouter-uuid ip route list table main">10.0.0.0/24 dev qr-102c4426-86  proto kernel  scope link  src 10.0.0.1
10.0.1.0/24 dev qr-3779302e-62  proto kernel  scope link  src 10.0.1.1
169.254.106.114/31 dev rfp-e4d4897e-7  proto kernel  scope link  src 169.254.106.114
</code></pre>

<p>Looking at the <strong>ip rule</strong> configuration we can find that table 16 matches all packets from that particular VM (10.0.0.12).</p>

<pre><code class="bash ip netns exec qrouter-uuid ip rule">0:      from all lookup local
32766:  from all lookup main
32767:  from all lookup default
57481:  from 10.0.0.12 lookup 16
167772161:      from 10.0.0.1/24 lookup 167772161
167772417:      from 10.0.1.1/24 lookup 167772417
</code></pre>

<p>Routing table 16 sends the packet via a point-to-point veth pair link to the FIP namespace.</p>

<pre><code class="bash ip netns exec qrouter-uuid ip route list table 16">default via 169.254.106.115 dev rfp-e4d4897e-7
</code></pre>

<p>Before sending the packet out, DVR translates the source IP of the packet to the FIP assigned to that VM.</p>

<pre><code class="bash ip netns exec qrouter-uuid iptables -t nat -L | grep NAT">SNAT       all  --  10.0.0.12            anywhere             to:169.254.0.55
</code></pre>

<p>A FIP namespace is a simple router designed to connect multiple DVRs to external network. This way all routers can share the same &ldquo;uplink&rdquo; namespace and don&rsquo;t have to consume valuable addresses from external subnet.</p>

<pre><code class="bash ip netns exec fip-uuid ip a | grep -E "UP|inet"">2: fpr-e4d4897e-7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    inet 169.254.106.115/31 scope global fpr-e4d4897e-7
15: fg-d3bb699d-af: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN
    inet 169.254.0.58/24 brd 169.254.0.255 scope global fg-d3bb699d-af
</code></pre>

<p>Default route inside the FIP namespace points to the External subnet&rsquo;s gateway IP.</p>

<pre><code class="bash ip netns exec fip-uuid ip route | grep default">default via 169.254.0.1 dev fg-d3bb699d-af
</code></pre>

<p>The MAC address of the gateway is statically configured by the L3 agent.</p>

<pre><code class="bash ip netns exec fip-uuid ip neigh | grep 169.254.0.1">169.254.0.1 dev fg-d3bb699d-af lladdr 32:3e:7d:13:ca:78 DELAY
</code></pre>

<p>The packet is sent to the <strong>br-int</strong> with the destination MAC address of the default gateway, which is learned on port 3.</p>

<pre><code class="bash ovs-appctl fdb/show br-int | grep 32:3e:7d:13:ca:78">  3     3  32:3e:7d:13:ca:78    1
</code></pre>

<p>External bridge strips the VLAN ID of the packet coming from the <strong>br-int</strong> and does the lookup in the dynamic MAC address table.</p>

<p><code>bash ovs-appctl ofproto/trace br-ex in_port=2,dl_vlan=3 | grep actions=
OpenFlow actions=pop_vlan,NORMAL
</code></p>

<p>The frame is forwarded out the physical interface.</p>

<pre><code class="bash ovs-appctl fdb/show br-ex | grep 32:3e:7d:13:ca:78">1     0  32:3e:7d:13:ca:78    1
</code></pre>

<p>Reverse packet flow will be quite similar, however in this case FIP namespace must be able to respond to ARP requests for the IPs that only exist on DVRs. In order to do that, it uses a proxy-ARP feature. First, L3 agent installs a static route for the FIP pointing back to the correct DVR over the veth pair interface:</p>

<pre><code class="bash ip netns exec fip-uuid ip route get 169.254.0.55">169.254.0.55 via 169.254.106.114 dev fpr-e4d4897e-7  src 169.254.106.115
</code></pre>

<p>Now that the FIP namespace knows the route to the floating IP, it can respond to ARPs on behalf of DVR as long as proxy-ARP is enabled on the external <strong>fg</strong> interface:</p>

<pre><code class="bash ip netns exec fip-uuid">cat /proc/sys/net/ipv4/conf/fg-d3bb699d-af/proxy_arp
1
</code></pre>

<p>Finally, the DVR NATs the packet back to its internal IP in the BLUE subnet and forwards it straight to VM1.</p>

<pre><code class="bash ip netns exec qrouter-uuid iptables -t nat -L | grep DNAT">DNAT       all  --  anywhere             169.254.0.55         to:10.0.0.12
</code></pre>

<h2>DVR Pros and Cons</h2>

<p>Without a doubt DVR has introduced a number of much needed <strong>improvements</strong> to OpenStack networking:</p>

<ul>
<li>East-West traffic now follows the most optimal path thereby reducing the load on the Network node.</li>
<li>External connectivity to floating IPs now also follows the most optimal path directly to the compute node hosting the VM.</li>
</ul>


<p>However, there&rsquo;s a number of <strong>issues</strong> that either remain unaddressed or result directly from the current DVR architecture:</p>

<ul>
<li>DHCP and SNAT are still hosted on the Network node.</li>
<li>Asymmetric routing means that every DVR needs to have an interface in every configured subnet, even when there are no VMs that belong to those subnets on the current compute node.</li>
<li>Direct connectivity to FIP means that all compute nodes now need to have direct L2 adjacency to external subnets.</li>
<li>FIP namespace on compute nodes consumes IP addresses from external subnets which can be a problem if external subnet is in a public IPv4 address range.</li>
<li>DVR implementation as a network namespace creates additional overhead in packet processing.</li>
</ul>


<p>Some of the above issues are not critical and can be fixed with a little effort:</p>

<ul>
<li>In order to reduce the scope of a External network VLAN span inside the DC, dedicate a subset of hosts that will have a direct L2 adjacency to external networks and only deploy external-facing VMs on those hosts.</li>
<li>Since FIP namespace only requires an external IP address for <a href="http://lists.openstack.org/pipermail/openstack-dev/2016-June/096386.html">debugging purposes</a>, we can create an additional, secondary, subnet in RFC1918 space for FIP connectivity. This is enabled by a feature called subnet <a href="https://specs.openstack.org/openstack/neutron-specs/specs/newton/subnet-service-types.html">&ldquo;Service types&rdquo;</a> and is <a href="https://github.com/openstack/neutron-specs/blob/master/specs/newton/subnet-service-types.rst">available</a> in the latest Newton release.</li>
</ul>


<p>However the main issue still remains unresolved. Every North-South packet has to hop several times between the global and DVR/FIP/NAT namespaces. These kind of operations are very expensive in terms of consumed CPU and memory resources and can be very detrimental to network performance. Using namespaces may be the most straight-forward and non-disruptive way of implementing DVR, however it&rsquo;s definitely not the most optimal. Ideally we&rsquo;d like to see both L2 and L3 pipelines implemented in OpenvSwitch tables. This way all packets can benefit from OVS <a href="https://networkheresy.com/2014/11/13/accelerating-open-vswitch-to-ludicrous-speed/">fast-path flow caching</a>. But fear not, the solution to this already exists in a shape of <a href="https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/ovn-native-virtual-networking-for-open-vswitch">Open Virtual Network</a>. OVN is a project spawned from the OVS and aims to <a href="https://blog.russellbryant.net/2016/09/29/ovs-2-6-and-the-first-release-of-ovn/">address</a> a number of shortcomings existing in current implementations of virtual networks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Automating the Build of OpenStack Lab (Part 2)]]></title>
    <link href="http://networkop.github.io/blog/2016/09/09/os-lab-p2/"/>
    <updated>2016-09-09T00:00:00+01:00</updated>
    <id>http://networkop.github.io/blog/2016/09/09/os-lab-p2</id>
    <content type="html"><![CDATA[<p>In this post we&rsquo;ll use Chef, unnumbered BGP and Cumulus VX to build a massively scalable &ldquo;Lapukhov&rdquo; Leaf-Spine data centre.</p>

<!--more-->


<hr />

<p>In the <a href="http://networkop.github.io/blog/2016/08/26/os-lab-p1/">last post</a> we&rsquo;ve seen how to use Chef to automate the build of a 3-node OpenStack cloud. The only thing remaining is to build an underlay network supporting communication between the nodes, which is what we&rsquo;re going to do next. The build process will, again, be relatively simple and will include only a few manual steps, but before we get there let me go over some of the decisions and assumptions I&rsquo;ve made in my network design.</p>

<h2>High-level design</h2>

<p>The need to provide more bandwidth for East-West traffic has made the Clos Leaf-Spine architecture a de facto standard in any data centre network design. The use of virtual overlay networks has obviated the requirement to have a strict VLAN and IP numbering schemes in the underlay. The only requirement for the compute nodes now is to have any-to-any layer 3 connectivity. This is how the underlay network design has converged to a Layer 3 Leaf-Spine architecture.<br/>
The choice of a routing protocol is not so straight-forward. My fellow countryman Petr Lapukhov and co-authors of <a href="https://tools.ietf.org/html/draft-ietf-rtgwg-bgp-routi3ng-large-dc-11">RFC draft</a> claim that having a single routing protocol in your WAN and DC reduces complexity and makes interoperability and operations a lot easier. This draft presents some of the design principles that can be used to build a L3 data centre with BGP as the only routing protocol. In our lab we&rsquo;re going to implement a single &ldquo;cluster&rdquo; of the multi-tier topology proposed in that RFC.</p>

<p><img class="center" src="/images/os-lab-chef-full.png"></p>

<p>In order to help us build this in an automated and scalable way, we&rsquo;re going to use a relatively new feature called <strong>unnumbered BGP</strong>.</p>

<h2>Unnumbered BGP as a replacement for IGP</h2>

<p>As we all know, one of the main advantages of interior gateway protocols is the automatic discovery of adjacent routers which is accomplished with the help of link-local multicasts. On the other hand, BGP traditionally required you to explicitly define neighbor&rsquo;s IP address in order to establish a peering relationship with it. This is where IPv6 comes to the rescue. With the help of neighbor discovery protocol and router advertisement messages, it becomes possible to accurately determine the address of the peer BGP router on an intra-fabric link. The only question is how we would exchange IPv4 information over and IPv6-only BGP network.<br/>
<a href="https://tools.ietf.org/html/rfc5549">RFC 5549</a>, described an &ldquo;extended nexthop encoding capability&rdquo; which allows BGP to exchange routing updates with nexthops that don&rsquo;t belong to the address family of the advertised prefix. In plain English it means that BGP is now capable of advertising an IPV4 prefix with an IPv6 nexthop. This makes it possible to configure all transit links inside the Clos fabric with IPv6 link-local addresses and still maintain reachability between the edge IPv4 host networks. Since nexthop IPs will get updated at every hop, there is no need for an underlying IGP to distribute them between all BGP routers. What we see is, effectively, BGP <strong>absorbing</strong> the functions of an IGP protocol inside the data centre.</p>

<h2>Configuration example on Cumulus VX</h2>

<p>In order to implement BGP unnumbered on Cumulus Linux all you need to is:</p>

<ol>
<li>Enable IPv6 router advertisements on all transit links</li>
<li>Enable BGP on the same interfaces</li>
</ol>


<p>Example Quagga configuration snippet will look like this:</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>interface swp1
</span><span class='line'>  ipv6 nd ra-interval 5
</span><span class='line'>  no ipv6 nd suppress-ra&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;rouer bgp &lt;ASN&gt;
</span><span class='line'>  neighbor swp1 interface
</span><span class='line'>  neighbor swp1 external
</span></code></pre></td></tr></table></div></figure></p>

<p>As you can see, Cumulus simplifies it even more by allowing you to only specify the BGP peering type (external/internal) and learning the value of peer BGP AS dynamically from a neighbor.</p>

<h2>Design assumptions and caveats</h2>

<p>With all the above in mind, this is the list of decisions I&rsquo;ve made while building the fabric configuration:</p>

<ul>
<li>All switches inside the fabric will be running BGP peerings using <strong>IPv6 link-local</strong> addresses</li>
<li><strong>eBGP</strong> will be used throughout to simplify configuration automation (all peers will be external)</li>
<li>Each Leaf/Spine switch will have a <strong>unique IPv4 loopback</strong> address assigned for management purposes (ICMP, SSH)</li>
<li>On each Leaf switch <strong>all directly connected IPv4</strong> prefixes will get redistributed into BGP</li>
<li>BGP multipath rule will be &ldquo;relaxed&rdquo; to allow for different AS-PATHs. This is not used in our current topology but is required in an HA Leaf switch design (same IPv4 prefix will be advertised from two Leaf switches with different ASN)</li>
<li>Loop prevention on Leaf switches will also be &ldquo;relaxed&rdquo;. This, again, is not used in our single &ldquo;cluster&rdquo; topology, however it will allow same Leaf ASNs to be reused in a different cluster.</li>
</ul>


<h2>Implementation steps</h2>

<p>Picking up where we left off after the OpenStack node provisioning described in the <a href="http://networkop.github.io/blog/2016/08/26/os-lab-p1/">previous post</a></p>

<ol>
<li><p>Get the latest <a href="https://github.com/networkop/chef-unl-os">OpenStack lab cookbooks</a></p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> git clone &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://github.com/networkop/chef-unl-os.git&quot;</span>&gt;https://github.com/networkop/chef-unl-os.git&lt;/a&gt;
</span><span class='line'> <span class="nb">cd </span>chef-unl-os
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p><a href="https://cumulusnetworks.com/cumulus-vx/">Download</a> and import Cumulus VX image similar to how it&rsquo;s described <a href="http://www.unetlab.com/2015/06/adding-cisco-asav-images/">here</a>.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> /opt/unetlab/addons/qemu/cumulus-vx/hda.qcow2
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Build the topology inside UNL. Make sure that Node IDs inside UNL match the ones in <strong>chef-unl-os/environment/lab.rb</strong> file and that interfaces are connected as shown in the diagram below</p>

<p> <img class="center" src="/images/os-lab-unl.png"></p></li>
<li><p>Re-run UNL self-provisioning cookbook to create a <a href="https://github.com/networkop/chef-unl-os/blob/master/cookbooks/pxe/templates/ztp.erb">zero touch provisioning</a> file and update DHCP server configuration with static entries for the switches.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> chef-client -z -E lab -o pxe
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Cumulus <a href="https://docs.cumulusnetworks.com/display/DOCS/Zero+Touch+Provisioning+-+ZTP">ZTP</a> allows you to run a predefined script on the first boot of the operating system. In our case we inject a UNL VM&rsquo;s public key and enable passwordless <strong>sudo</strong> for cumulus user.</p></li>
<li><p>Kickoff Chef provisioning to bootstrap and configure the DC fabric.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> chef-client -z -E lab fabric.rb
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> This command instructs Chef provisioning to connect to each switch, download and install the Chef client and run a simple recipe to create quagga configuration file from a template.</p></li>
</ol>


<p>At the end of step 5 we should have a fully functional BGP-only fabric and all 3 compute nodes should be able to reach each other in at most 4 hops.</p>

<p><code>bash [root@controller-1 ~]# traceroute 10.0.0.4
traceroute to 10.0.0.4 (10.0.0.4), 30 hops max, 60 byte packets
 1  10.0.0.1 (10.0.0.1)  0.609 ms  0.589 ms  0.836 ms
 2  10.255.255.7 (10.255.255.7)  0.875 ms  2.957 ms  3.083 ms
 3  10.255.255.6 (10.255.255.6)  3.473 ms  5.486 ms  3.147 ms
 4  10.0.0.4 (10.0.0.4)  4.231 ms  4.159 ms  4.115 ms
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Automating the Build of OpenStack Lab (Part 1)]]></title>
    <link href="http://networkop.github.io/blog/2016/08/26/os-lab-p1/"/>
    <updated>2016-08-26T00:00:00+01:00</updated>
    <id>http://networkop.github.io/blog/2016/08/26/os-lab-p1</id>
    <content type="html"><![CDATA[<p>In this post we will explore what&rsquo;s required to perform a zero-touch deployment of an OpenStack cloud. We&rsquo;ll get a 3-node lab up and running inside UNetLab with just a few commands.</p>

<!--more-->


<hr />

<p>Now that I&rsquo;m finally beginning to settle down at my new place of residence I can start spending more time on research and blogging. I have left off right before I was about to start exploring the native OpenStack distributed virtual routing function. However as I&rsquo;d started rebuilding my OpenStack lab from scratch I realised that I was doing a lot of repetitive tasks which can be easily automated. Couple that with the fact that I needed to learn Chef for my new work and you&rsquo;ve got this blogpost describing a few Chef <a href="https://github.com/networkop/chef-unl-os.git">cookbooks</a> (similar to Ansible&rsquo;s playbook) automating all those manual steps described in my earlier blogposts <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">1</a> and <a href="http://networkop.github.io/blog/2016/04/18/os-unl-lab/">2</a>.<br/>
In addition to that in this post I&rsquo;ll show how to build a very simple OpenStack baremetal provisioner and installer. Some examples of production-grade baremetal provisioners are <a href="https://wiki.openstack.org/wiki/Ironic">Ironic</a>, <a href="http://crowbar.github.io/">Crowbar</a> and <a href="http://maas.io/">MAAS</a>. In our case we&rsquo;ll turn UNetLab VM into an <strong>undercloud</strong>, a server used to provision and deploy our OpenStack lab, an <strong>overcloud</strong>. To do that we&rsquo;ll first install and configure DHCP, TFTP and Apache servers to PXE-boot our UNL OpenStack nodes. Once all the nodes are bootstrapped, we&rsquo;ll use Chef to configure the server networking and kickoff the packstack OpenStack installer.</p>

<p><img class="center" src="/images/os-lab-chef.png"></p>

<p>In this post I&rsquo;ll try to use Chef recipes that I&rsquo;ve written as much as possible, therefore you won&rsquo;t see the actual configuration commands, e.g. how to configure Apache or DHCP servers. However I will try to describe everything that happens at each step and hopefully that will provide enough incentive for the curious to look into the Chef code and see how it&rsquo;s done. To help with the Chef code understanding let me start with a brief overview of what to look for in a cookbook.</p>

<h2>How to read a Chef cookbook (Optional)</h2>

<p>A cookbook directory (<strong>/cookbooks/[cookbook_name]</strong>) contains all its configuration scripts in <strong>/recipes</strong>. Each file inside a recipe contains a list of steps to be performed on a server. Each step is an operation (add/delete/update) on a <strong>resource</strong>. Here are some of the common Chef resources:</p>

<ul>
<li>Package - allows you to add, remove or update a package</li>
<li>Template - creates a file from an <strong>erb</strong>-formatted template</li>
<li>Execute - runs an ad-hoc CLI command</li>
</ul>


<p>Just these three basic resources allow you to do 95% of administrative tasks on any server. Most importantly they do it in platform-independent (any flavour of Linux) and idempotent (only make changes if current state is different from a desired state) way. Other directories you might want to explore are:</p>

<ul>
<li>/templates - contains all the <strong>erb</strong>-formatted templates</li>
<li>/attributes - contains recipe variables (file paths, urls etc.)</li>
<li>/files - contains the non-template files, i.e. files with static content</li>
</ul>


<h2>Bootstrapping the OpenStack nodes</h2>

<ol>
<li><p>If you haven&rsquo;t done it yet, download a copy of the <strong>UNetLab VM</strong> from the <a href="http://www.unetlab.com/">official website</a>. Set it up inside your hypervisor so that you can access Internet through the first interface <strong>pnet0</strong> (i.e. connect the first NIC of the VM to hypervisor&rsquo;s NAT interface). Make sure the VM has got at least 6GB of RAM and VT-x support enabled for nested virtualization.</p></li>
<li><p>Follow the official <a href="https://downloads.chef.io/chef-dk/">installation instructions</a> to <strong>install Chef Development Kit</strong> inside UNetLab VM.</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>wget &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://packages.chef.io/stable/ubuntu/12.04/chefdk_0.16.28-1_amd64.deb&quot;</span>&gt;https://packages.chef.io/stable/ubuntu/12.04/chefdk_0.16.28-1_amd64.deb&lt;/a&gt;
</span><span class='line'>dpkg -i chefdk_0.16.28-1_amd64.deb
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p><strong>Install git</strong> and clone <a href="https://github.com/networkop/chef-unl-os.git">chef cookbooks</a>.</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>apt-get -y update
</span><span class='line'>apt-get -y install git
</span><span class='line'>git clone &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://github.com/networkop/chef-unl-os.git&quot;</span>&gt;https://github.com/networkop/chef-unl-os.git&lt;/a&gt;
</span><span class='line'><span class="nb">cd </span>chef-unl-os
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Examine the lab <strong>environment settings</strong> to see what values are going to be used. You can modify that file to your liking.</p>

<blockquote><p>Note that the OpenStack node IDs (keys of <em>os_lab</em> hash) MUST have one to one correspondence with the UNL node IDs which will be created at step 5</p></blockquote>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>cat environment/lab.rb
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Run Chef against a local server to setup the <strong>baremetal provisioner</strong>. This step installs and configures DHCP, TFTP and Apache servers. It also creates all the necessary PXE-boot and kickstart files based on our environment settings. Note that a part of the process is the download of a 700MB CentOS image so it might take a while to complete.</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>chef-client -z -E lab -o pxe
</span></code></pre></td></tr></table></div></figure></p>

<p>At the start of the PXE-boot process, DCHP server sends an OFFER which, along with the standard IP information, includes the name of the PXE boot image and the IP address of TFTP server where to get it from. A server loads this image and then searches the TFTP server for the boot configuration file which tells it what kernel to load and where to get a kickstart file. Both kickstart and the actual installation files are accessed via HTTP and served by the same Apache server that runs UNL GUI.</p></li>
<li><p>From <strong>UNL GUI</strong> create a new lab, add 3 OpenStack nodes and connect them all to <strong>pnet10</strong> interface as described in <a href="http://www.unetlab.com/2014/11/using-cloud-devices/">this guide</a>. Note that the <strong>pnet10</strong> interface has already been created by Chef so you don&rsquo;t have to re-create it again.</p>

<blockquote><p>Make sure that the UNL node IDs match the ones defined in the environment setting file</p></blockquote></li>
<li><p>Fire-up the nodes and watch them being bootstrapped by our UNL VM.</p></li>
</ol>


<h2>Server provisioning</h2>

<p>Next step is to configure the server networking and kickoff the OpenStack installer. These steps will also be done with a single command:</p>

<p>   <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>   chef-client -z -E lab lab.rb
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>
The first part of this script will connect to each prospective OpenStack node and setup its network interfaces and hostnames. The second part of this script will generate a packstack answer file and modify its settings to exclude some of the components we&rsquo;re not going to use (like Nagios, Ceph and Ceilometer). Have a look at <strong>cookbooks/packstack/recipe/default.rb</strong> for the list of modifications. The final step is a command to kickoff the packstack installer which will use another configuration management system, Puppet, to install and configure OpenStack according to the provided answer file.</p>

<p>At the end of these steps you should have a fully functional 3-node OpenStack environment.</p>

<h2>To be continued&hellip;</h2>

<p>This is a part of a 2-post series. In the next post we&rsquo;ll look into how to use the same tools to perform the baremetal provisioning of our physical underlay network.</p>
]]></content>
  </entry>
  
</feed>
