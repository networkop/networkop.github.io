<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Openstack | Network-oriented programming]]></title>
  <link href="http://networkop.github.io/blog/categories/openstack/atom.xml" rel="self"/>
  <link href="http://networkop.github.io/"/>
  <updated>2016-05-10T23:16:51-07:00</updated>
  <id>http://networkop.github.io/</id>
  <author>
    <name><![CDATA[Michael Kashin]]></name>
    <email><![CDATA[mmkashin@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Openstack SDN - Extending a L2 Provider Network Over a L3 Fabric]]></title>
    <link href="http://networkop.github.io/blog/2016/05/11/neutron-routed-extnet/"/>
    <updated>2016-05-11T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/05/11/neutron-routed-extnet</id>
    <content type="html"><![CDATA[<p>In the this post we&rsquo;ll tackle yet another Neutron scalability problem identified in my <a href="http://networkop.github.io/blog/2016/04/22/neutron-native/">earlier post</a> - a requirement to have a direct L2 adjacency between the external provider network and the network node.</p>

<!--more-->


<h2>Provider vs Tenant networks</h2>

<p>Before we start, let&rsquo;s recap the difference between the <a href="http://docs.openstack.org/mitaka/networking-guide/intro-os-networking-overview.html">two major</a> Neutron network types:</p>

<ul>
<li>Tenant networks are:

<ul>
<li>provisioned by tenants</li>
<li>used for inter-VM (east-west) communication</li>
<li>use Neutron virtual router as their default gateway</li>
</ul>
</li>
<li>Provider networks are:

<ul>
<li>provisioned by OpenStack administrator(for use by tenants)</li>
<li>match existing physical networks</li>
<li>can be either flat (untagged VLAN) or VLAN-based (multiple VLANs)</li>
<li>need to be L2 adjacent to network and/or compute nodes</li>
</ul>
</li>
</ul>


<p>These two network types are not mutually exclusive. In our case the <strong>admin tenant</strong> network is implemented as a VXLAN-based overlay whose only requirement is to have a layer-3 reachability in the underlay. However tenant network could also have been implemented using a VLAN-based provider network in which case a set of dot1Q tags pre-provisioned in the underlay would have been used for tenant network segregation.</p>

<h2>External provider network</h2>

<p>External network is used by VMs to communicate with the outside world (north-south). Since default gateway is located outside of OpenStack environment this, by definition, is a provider network. Normally, tenant networks will use the non-routable address space and will rely on a Neutron virtual router to perform some form of NAT translation. As we&rsquo;ve seen in the <a href="http://networkop.github.io/blog/2016/04/22/neutron-native/">earlier post</a>, Neutron virtual router is directly connected to the external bridge which allows it to &ldquo;borrow&rdquo; ip address from the external provider network to use for two types of NAT operations:</p>

<ul>
<li>SNAT - a source-based port address translation performed by the Neutron virtual router</li>
<li>DNAT - a static NAT created for every <a href="https://www.rdoproject.org/networking/difference-between-floating-ip-and-private-ip/">floating ip address</a> configured for a VM</li>
</ul>


<p>In default deployments all NATing functionality is performed by a network node, so external provider network only needs to be L2 adjacent with a limited number of physical hosts. In deployments where <abbr title=" Distributed Virtual Router">DVR</abbr> is used, the virtual router and NAT functionality gets distributed among all compute hosts which means that they, too, now need to be layer-2 adjacent to the external network.</p>

<h2>Solutions overview</h2>

<p>The direct adjacency requirement presents a big problem for deployments where layer-3 routed underlay is used for the tenant networks. There is a limited number of ways to satisfy this requirements, for example:</p>

<ul>
<li>Span a L2 segment across the whole DC fabric. This means that the fabric needs to be converted to layer-2, reintroducing spanning-tree and all the unique vendor solutions to overcome STP limitations(e.g. TRILL, Fabripath, SPB).</li>
<li>Build a dedicated physical network. This may not always be feasible, especially considering that it needs to be delivered to all compute hosts.</li>
<li>Extend the provider network over an existing L3 fabric with VXLAN overlay. This can easily be implemented with just a few commands, however it requires a border leaf switch capable of performing VXLAN-VLAN translation.</li>
</ul>


<h2>Detailed design</h2>

<p>As I&rsquo;ve said in my <a href="http://networkop.github.io/blog/2016/04/18/os-unl-lab/">earlier post</a>, I&rsquo;ve built the leaf-spine fabric out of Cisco IOU virtual switches, however the plan was to start introducing other vendors later in the series. So this time for the border leaf role I&rsquo;ve chosen Arista vEOS switch, however, technically, it could have been any other vendor capable of doing VXLAN-VLAN bridging (e.g. any hardware switch with <a href="http://blog.ipspace.net/2014/06/trident-2-chipset-and-nexus-9500.html">Trident 2</a> or similar ASIC).</p>

<p><img class="center" src="/images/neutron-extnet-l3.png"></p>

<h3>Arista vEOS configuration</h3>

<p>Configuration of Arista switches is very similar to Cisco IOS. In fact, I was able to complete all interface and OSPF routing configuration only with the help of CLI context help. The only bit that was new to me and that I had to lookup in the official guide was the <a href="https://eos.arista.com/vxlan-with-mlag-configuration-guide/">VXLAN configuration</a>. These similarities makes the transition from Cisco to Arista very easy and I can understand (but not approve!) why Cisco would file a lawsuit against Arista for copying its &ldquo;industry-standard CLI&rdquo;.</p>

<pre><code class="text L4 configuration">interface Ethernet1
   description SPINE-1:Eth0/3
   no switchport
   ip address 169.254.41.4/24
   ip ospf network point-to-point
!
interface Ethernet2
   description SPINE-2:Eth0/3
   no switchport
   ip address 169.254.42.4/24
   ip ospf network point-to-point
!
interface Ethernet3
   description VM-HOST-ONLY:PNET1
   switchport access vlan 100
   spanning-tree portfast
!
interface Loopback0
   ip address 10.0.0.4/32
!
interface Vxlan1
   vxlan source-interface Loopback0
   vxlan udp-port 4789
   vxlan vlan 100 vni 1000
   vxlan vlan 100 flood vtep 10.0.3.10
!
router ospf 1
   router-id 10.0.0.4
   passive-interface default
   no passive-interface Ethernet1
   no passive-interface Ethernet2
   network 0.0.0.0/0 area 0.0.0.0
!
</code></pre>

<p>Interface VXLAN1 sets up VXLAN-VLAN bridging between VNI 1000 and VLAN 100. VLAN 100 is used to connect to VMware Workstation&rsquo;s host-only interface, the one that was <a href="http://networkop.github.io/blog/2016/04/18/os-unl-lab/">previously</a> connected directly to the L3 leaf switch. VXLAN interface does the multicast source replication by flooding unknown packets over the layer 3 fabric to the network node (10.0.3.10).</p>

<h3>OpenStack network node configuration</h3>

<p>Since we don&rsquo;t yet have the distributed routing feature enabled, the only OpenStack component that requires any changes is the network node. First, let&rsquo;s remove the physical interface from the external bridge, since it will no longer be used to connect to the external provider network.</p>

<pre><code class="bash Remove the physical interface from the external bridge">$ ovs-vsctl del-port br-ex eth1.300
</code></pre>

<p>Next let&rsquo;s add the VXLAN interface towards the Loopback IP address of the Arista border leaf switch. The key option sets the VNI which must be equal to the VNI defined on the border leaf.</p>

<pre><code class="bash add the VXLAN interface towards the Arista switch">$ ovs-vsctl add-port br-ex vxlan1 \
-- set interface vxlan1 \
type=vxlan \
options:remote_ip=10.0.0.4 \
options:key=1000
</code></pre>

<p>Without any physical interfaces attached to the external bridge, the OVS will use the Linux network stack to find the outgoing interface. When a packet hits the <strong>vxlan1</strong> interface of the br-ex, it will get encapsulated in a VXLAN header and passed on to the OS network stack where it will follow the <a href="http://networkop.github.io/blog/2016/04/18/os-unl-lab/">pre-configured</a> static route forwarding all 10/8 traffic towards the leaf-spine fabric. Check out <a href="http://blog.scottlowe.org/2013/05/15/examining-open-vswitch-traffic-patterns/">this article</a> if you want to learn more about different types of interfaces and traffic forwarding behaviours in OpenvSwitch.</p>

<h3>Cleanup</h3>

<p>In order to make changes persistent and prevent the static interface configuration from interfering with OVS, remove all OVS-related configuration and shutdown interface eth1.300.</p>

<pre><code class="bash /etc/sysconfig/network-scripts/ifcfg-eth1.300">ONBOOT=no
VLAN=yes
</code></pre>

<h2>Change in the packet flow</h2>

<p>None of the packet flows have changed as the result of this modification. All VMs will still use NAT to break out of the private environment, the NAT&rsquo;d packets will reach the external bridge <strong>br-ex</strong> as described in my <a href="http://networkop.github.io/blog/2016/04/22/neutron-native/">earlier post</a>. However this time <strong>br-ex</strong> will forward the packets out the <strong>vxlan1</strong> port which will deliver them to the Arista switch over the same L3 fabric used for east-west communication.</p>

<p>If we did a capture on the fabric-facing interface <strong>eth1</strong> of the control node while running a ping from one of the VMs to the external IP address, we would see a VXLAN-encapsulated packet destined for the Loopback IP of L4 leaf switch.</p>

<p><img class="center" src="/images/neutron-provider-vxlan.png"></p>

<h2>Coming Up</h2>

<p>In the next post we&rsquo;ll examine the L2 gateway feature that allows tenant networks to communicate with physical servers through yet another VXLAN-VLAN hardware gateway.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Openstack SDN - L2 Population and ARP Proxy]]></title>
    <link href="http://networkop.github.io/blog/2016/05/06/neutron-l2pop/"/>
    <updated>2016-05-06T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/05/06/neutron-l2pop</id>
    <content type="html"><![CDATA[<p>In the <a href="http://networkop.github.io/blog/2016/04/22/neutron-native/">previous post</a> we&rsquo;ve had a look at how native OpenStack SDN works and what are some of its limitations. In this post we&rsquo;ll tackle the first one of them - overhead created by multicast source replication.</p>

<!--more-->


<h2>MAC learning in a controller-less VXLAN overlay</h2>

<p>VXLAN <a href="https://tools.ietf.org/html/rfc7348">standard</a> does not specify any control plane protocol to exchange MAC-IP bindings between VTEPs. Instead it relies on data plane flood-and-learn behaviour, just like a normal switch. To force this behaviour in an underlay, the standard stipulates that each VXLAN network should be mapped to its own multicast address and each VTEP participating in a network should join the corresponding multicast group. That multicast group would be used to flood the <abbr title="Broadcast Unknown unicast and Multicast">BUM</abbr> traffic in an underlay to all subscribed VTEPs thereby populating dynamic MAC address tables.</p>

<p>Default OpenvSwitch implementation <a href="https://github.com/openvswitch/ovs/blob/master/FAQ.md#q-how-much-of-the-vxlan-protocol-does-open-vswitch-currently-support">does not support</a> VXLAN multicast flooding and uses unicast source replication instead. This decision comes with a number of tradeoffs:</p>

<ul>
<li>Duplicate packets consume additional bandwidth. Extra 100 bytes exchanged every 3 minutes in a 100-nodes environment generate around 500 kbit/s of traffic on average. This can be considered negligible inside modern high-speed DC fabrics.</li>
<li>Hardware VTEP gateways rely on multicast for MAC learning and VTEP discovery. As we&rsquo;ll see later in the series, these gateways can now be controlled by Neutron just like a normal OVS inside a compute host.</li>
<li>Duplicate packets are processed by hosts that do not need them, e.g. ARP request is processed by tunnel and integration bridges of all hosts that have VMs in the same broadcast domain. This presents some serious scaling limitation and is addressed by the L2 population feature described in this post.</li>
</ul>


<p>Despite all the tradeoffs, OVS with unicast source replication has become a de-facto standard in most recent OpenStack implementations. The biggest advantage of such approach is the lack of requirement for multicast in the underlay network.</p>

<h2>VXLAN MAC learning with an SDN controller</h2>

<p>Neutron server is aware of all active MAC and IP addresses within the environment. This information can be used to prepopulate forwarding entries on all tunnel bridges. This is accomplished by a <a href="https://github.com/openstack/neutron/tree/master/neutron/plugins/ml2/drivers/l2pop">L2 population</a> driver. However that in itself isn&rsquo;t enough. Whenever a VM doesn&rsquo;t know the destination MAC address, it will send a broadcast ARP request which needs to be intercepted and responded by a local host to stop it from being flooded in the network. The latter is accomplished by a feature called <a href="https://assafmuller.com/2014/05/21/ovs-arp-responder-theory-and-practice/">ARP responder</a> which simulates the functionality commonly known as <strong>ARP proxy</strong> inside the tunnel bridge.</p>

<p><img class="center" src="/images/neutron-l2-arp.png"></p>

<h3>Configuration</h3>

<p>Configuration of these two features is <a href="https://kimizhang.wordpress.com/2014/04/01/how-ml2vxlan-works/">fairly straight-forward</a>. First, we need to add L2 population to the list of supported mechanism drivers on our control node and restart the neutron server.</p>

<pre><code class="bash Update on control node">$ sed -ri 's/(mechanism_drivers.*)/\1,l2population/' /etc/neutron/plugin.ini
$ service neutron-server restart  
</code></pre>

<p>Next we need to enable L2 population and ARP responder features on all 3 compute nodes.</p>

<pre><code class="bash Updates on all 3 compute nodes">$ sed -ri 's/.*(arp_responder).*/\1 = true/' /etc/neutron/plugins/ml2/openvswitch_agent.ini
$ sed -ri 's/.*(l2_population).*/\1 = true/' /etc/neutron/plugins/ml2/openvswitch_agent.ini
$ service neutron-openvswitch-agent restart
</code></pre>

<p>Since L2 population is triggered by the <a href="https://assafmuller.com/2014/02/23/ml2-address-population/">port_up</a> messages, we might need to restart both our VMs for the change to take effect.</p>

<h3>BUM frame from VM-1 for MAC address of VM-2 (Revisited)</h3>

<p>Now let&rsquo;s once again examine what happens when VM-1 issues an ARP request for VM-2&rsquo;s MAC address (1a:bf).</p>

<p>First, the frame hits the flood-and-learn rule of the integration bridge and gets flooded down to the tunnel bridge as desribed in the <a href="http://networkop.github.io/blog/2016/04/22/neutron-native/">previous post</a>. Once in the br-tun, the frames gets matched by the incoming port and resubmitted to table 2. In addition to a default unicast/multicast bit match, table 2 now also matches all ARP requests and resubmitts them to the new table 21. Note how the ARP entry has a higher priority to always match before the default catch-all multicast rule.</p>

<pre><code class="bash ovs-ofctl dump-flows br-tun">table=0, priority=1,in_port=1 actions=resubmit(,2)
table=2, priority=1,arp,dl_dst=ff:ff:ff:ff:ff:ff actions=resubmit(,21)
table=2, priority=0,dl_dst=00:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,20)
table=2, priority=0,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,22)
</code></pre>

<p>Inside table 21 are the entries created by the ARP responder feature. The following is an example entry that matches all ARP requests where target IP address field equals the IP of VM-2(10.0.0.9).</p>

<pre><code class="bash ovs-ofctl dump-flows br-tun"> table=21, priority=1,arp,dl_vlan=1,arp_tpa=10.0.0.9 
 actions=move:NXM_OF_ETH_SRC[]-&gt;NXM_OF_ETH_DST[],
 mod_dl_src:fa:16:3e:ab:1a:bf,
 load:0x2-&gt;NXM_OF_ARP_OP[],
 move:NXM_NX_ARP_SHA[]-&gt;NXM_NX_ARP_THA[],
 move:NXM_OF_ARP_SPA[]-&gt;NXM_OF_ARP_TPA[],
 load:0xfa163eab1abf-&gt;NXM_NX_ARP_SHA[],
 load:0xa000009-&gt;NXM_OF_ARP_SPA[],
 IN_PORT
</code></pre>

<p>The resulting action builds an ARP response by modifying the fields and headers on the original ARP request message, specifically OVS:</p>

<ol>
<li>Copies the source MAC address (VM-1) to the destination MAC address header</li>
<li>Spoofs the source MAC address to make it look like it comes from VM-2</li>
<li>Modifies the operation code of ARP message to 0x2, meaning <strong>reply</strong></li>
<li>Overwrites the target IP and MAC address fields inside the ARP packet with VM-1&rsquo;s values</li>
<li>Overwrites the source hardware address with VM-2&rsquo;s MAC</li>
<li>Overwrites the source IP address with the address of VM-2(0xa000009)</li>
<li>Sends the packet out the port from which it was received</li>
</ol>


<h3>Unicast frame from VM-1 to VM-2 (Revisited)</h3>

<p>Now that VM-1 has learned the MAC address of VM-2 it can start sending the unicast frames. The first few steps will again be the same. The frame hits the tunnel bridge, gets classified as a unicast and resubmitted to table 20. Table 20 will still have an entry generated by a <strong>learn</strong> action triggered by a packet coming from VM-2, however now it also has and identical entry with a higher priority(priority=2), which was preconfigured by a L2 population feature.</p>

<pre><code class="bash">table=0, priority=1,in_port=1 actions=resubmit(,2)
table=2, priority=0,dl_dst=00:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,20)
table=20, priority=2,dl_vlan=1,dl_dst=fa:16:3e:ab:1a:bf actions=strip_vlan,set_tunnel:0x54,output:2
table=20, priority=1,vlan_tci=0x0001/0x0fff,dl_dst=fa:16:3e:ab:1a:bf actions=load:0-&gt;NXM_OF_VLAN_TCI[],load:0x54-&gt;NXM_NX_TUN_ID[],output:2
</code></pre>

<h2>Other BUM traffic</h2>

<p>The two features described in this post only affect the ARP traffic to VMs <strong>known</strong> to the Neutron server. All the other BUM traffic will still be flooded as described in the <a href="http://networkop.github.io/blog/2016/04/22/neutron-native/">previous post</a>.</p>

<h2>Results</h2>

<p>As the result of enabling L2 population and ARP responder features we were able to reduce the amount of BUM traffic in the overlay network and reduce the eliminate processing on compute hosts incurred by ARP request flooding.</p>

<p>However one downside of this approach is the increased number of flow entries in tunnel bridges of compute hosts. Specifically, for each known VM there now will be two entries in the tunnel bridge with different priorities. This may have negative impact on performance and is something to keep in mind when designing OpenStack solutions for scale.</p>

<h2>Coming Up</h2>

<p>In the next post I&rsquo;ll show how to overcome the requirement of a direct L2 adjacency between the network node and external subnet. Specifically, I&rsquo;ll use Arista switch to extend a L2 provider network over a L3 leaf-spine Cisco fabric.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network Engineering Analysis of OpenStack SDN]]></title>
    <link href="http://networkop.github.io/blog/2016/04/22/neutron-native/"/>
    <updated>2016-04-22T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/04/22/neutron-native</id>
    <content type="html"><![CDATA[<p>Now that we have our 3-node OpenStack lab <a href="http://networkop.github.io/blog/2016/04/18/os-unl-lab/">up and running</a> we can start exploring how virtual networks are implemented under the hood.</p>

<!--more-->


<h2>Intro</h2>

<p>This is going to be quite a lengthy blogpost so I&rsquo;ll try to explain its structure first. I&rsquo;ll start with a high level overview of components used to build virtual networks by examining 3 types of traffic:</p>

<ul>
<li>Unicast traffic between VM1 and VM2</li>
<li>Unicast traffic between VM1 and the outside world (represented by an external subnet)</li>
<li>Broadcast, Unknown unicast and Multicast or BUM traffic from VM1</li>
</ul>


<p>Following that I&rsquo;ll give a brief overview of how to interpret the configuration and dynamic state of OpenvSwitch to manually trace the path of a packet. This will be required for the next section where I&rsquo;ll go over the same 3 types of traffic but this time corroborating every step with the actual outputs collected from the virtual switches. For the sake of brevity I&rsquo;ll abridge a lot of the output to only contain the relevant information.</p>

<h2>High Level Overview</h2>

<p><a href="http://www.innervoice.in/blogs/2015/03/31/openstack-neutron-plugins-and-agents/">Neutron</a> server, residing in a control node, is responsible for orchestrating and provisioning of all virtual networks within an OpenStack environment. Its goal is to enable end-to-end reachability among the VMs and between the VMs and external subnets. To do that, Neutron uses concepts that should be very familiar to every network engineer like subnet, router, firewall, DHCP and NAT. In the <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous post</a> we&rsquo;ve seen how to create a virtual router and attach it to public and private networks. We&rsquo;ve also attached both of our VMs to a newly created private network and verified connectivity by logging into those virtual machines. Now let&rsquo;s see how exactly these VMs communicate with each other and the outside world.</p>

<p><img class="center" src="/images/neutron-high-level.png"></p>

<h3>Unicast frame between VM1 and VM2</h3>

<ol>
<li><p>As soon as the frame leaves the vNIC of VM1 it hits the firewall. The firewall is implemented on a <a href="http://www.innervoice.in/blogs/2013/12/08/tap-interfaces-linux-bridge/">tap interface</a> of the integration bridge. A set of <abbr title="Access Control List">ACL</abbr> rules, defined in a <strong>Security Group</strong> that VM belongs to, gets translated into Linux <a href="https://www.frozentux.net/iptables-tutorial/iptables-tutorial.html#TRAVERSINGOFTABLES">iptables</a> rules and attached to this tap interface. These simple reflexive access lists are what VMware and Cisco are calling microsegmentation and touting as one of the main use case of their SDN solutions.</p></li>
<li><p>Next our frame enters the integration bridge implemented using <a href="http://openvswitch.org/">OpenvSwitch</a>. Its primary function is to interconnect all virtual machines running on the host. Its secondary function is to provide isolation between different subnets and tenants by keeping them in different VLANs. VLAN IDs used for this are locally significant and don&rsquo;t propagate outside of the physical host.</p></li>
<li><p>A dot1q-tagged packet is forwarded down a layer 2 trunk to the tunnel bridge, also implemented using OpenvSwitch. It is programmed to strip the dot1q tags, replace them with VXLAN headers and forward an IP/UDP packet with VXLAN payload on to the physical network.</p></li>
<li><p>Our simple routed underlay delivers the packets to the destination host, where the tunnel bridge swaps the <abbr title=" VXLAN Network ID">VNI</abbr> with a dot1q tag and forwards the packet up to the integration bridge.</p></li>
<li><p>Integration bridge consults the local MAC table, finds the output interface, clears the dot1q tag and send the frame up to the VM.</p></li>
<li><p>The frame gets screened by incoming iptables rules and gets delivered to the VM2.</p></li>
</ol>


<h3>Unicast frame between VM1 and External host</h3>

<ol>
<li><p>The first 3 steps will still be the same. VM1 sends a frame with destination MAC address of a virtual router. This packet will get encapsulated in a VXLAN header and forwarded to the Network node.</p></li>
<li><p>The tunnel and integration bridges of the network node deliver the packet to the private interface of a virtual router. This virtual router lives in a <a href="http://blog.scottlowe.org/2013/09/04/introducing-linux-network-namespaces/">linux network namespaces</a> (similar to <abbr title="Virtual Routing and Forwarding">VRF</abbr>s) used to provide isolation between OpenStack tenants.</p></li>
<li><p>The router finds the outgoing interface (a port attached to the external bridge), and a next-hop IP which we have set when we configured a public subnet <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">earlier</a>.</p></li>
<li><p>The router then performs a source NAT on the packet before forwarding it out. This way the private IP of the VM stays completely hidden and hosts outside of OpenStack can talk back to the VM by sending packets to (publicly routable) external subnet.</p></li>
<li><p>External bridge (also an OpenvSwitch) receives the packet and forwards it out the attached physical interface (eth1.300).</p></li>
</ol>


<h3>BUM frame from VM1 for MAC address of VM2</h3>

<ol>
<li><p>VM1 sends a multicast frame, which gets examined by the iptables rules and enters the integration bridge.</p></li>
<li><p>The integration bridge follows the same process as for the unicast frame to assign the dot1q tag and floods the frame to the tunnel bridge.</p></li>
<li><p>The tunnel bridge sees the multicast bit in the destination MAC address and performs source replication by sending a duplicate copy of the frame to both compute host #2 and the network node.</p></li>
<li><p>Tunnel bridges of both receiving hosts strip the VXLAN header, add the dot1q tag and flood the frame to their respective integration bridges.</p></li>
<li><p>Integration bridges flood the frame within the VLAN identified by the dot1q header.</p></li>
<li><p>The response from VM2 follows the same process as the unicast frame.</p></li>
</ol>


<p>One thing worth noting is when an ARP packet enters the integration bridge, its source IP address (in case of IPv4) or source MAC address (in case of IPv6) gets examined to make sure it belong to that VM. This is how <strong>ARP spoofing protection</strong> is implemented in OpenStack.</p>

<h2>OpenvSwitch quick intro</h2>

<p>Before we dive deeper into the details of the packet flows inside OVS let me give a brief overview of how it works. There are two main protocols to configure OVS:</p>

<ul>
<li><p>OVSDB - a management protocol used to configure bridges, ports, VLANs, QoS, monitoring etc.</p></li>
<li><p>OpenFlow - used to install flow entries for traffic switching, similar to how you would configure a static route but allowing you to match on most of the L2-L4 protocol headers.</p></li>
</ul>


<p>Control node instructs all <a href="http://www.innervoice.in/blogs/2015/03/31/openstack-neutron-plugins-and-agents/">local OVS agents</a> about how to configure virtual networks. Each local OVS agent then uses these two protocols to configure OVS and install all the required forwarding entries. Each entry contains a set of matching fields (e.g. incoming port, MAC/IP addresses) and an action field which determines what to do with the packet. These forwarding entries are implemented as <a href="https://wiki.openstack.org/wiki/Ovs-flow-logic">tables</a>. This is how a packet traverses these tables:</p>

<ol>
<li><p>First packet always hits table 0. The entries are examined in order of their priority (highest first) to find the first match. Note that it&rsquo;s the first and not necessarily the more specific match. It&rsquo;s the responsibility of a controller to build tables so that more specific flows are matched first. Normally this is done by assigning a higher priority to a more specific flow. Exact matches (where all L2-L4 fields are specified) implicitly have the highest priority value of 65535.</p></li>
<li><p>When a flow is matched, the <strong>action</strong> field of that flow is examined. Here are some of the most commonly used actions:</p>

<table>
<thead>
<tr>
<th> Action </th>
<th></th>
<th> Description </th>
</tr>
</thead>
<tbody>
<tr>
<td> resubmit(X,) </td>
<td></td>
<td> Resubmit a packet to table X </td>
</tr>
<tr>
<td> output:Y </td>
<td></td>
<td> Send a packet out port Y </td>
</tr>
<tr>
<td> NORMAL </td>
<td></td>
<td> Use a standard flood-and-learn behaviour of a switch to populate a local dynamic MAC address table </td>
</tr>
<tr>
<td> learn(table=Z) </td>
<td></td>
<td> Create an exact-match entry for the matched flow in table Z (we&rsquo;ll see how its used later on) </td>
</tr>
</tbody>
</table>


<p> These actions can be combined in a sequence to create complex behaviours like sending the same packet to multiple ports for multicast source replication.</p></li>
<li><p>OVS also <a href="https://networkheresy.com/2014/11/13/accelerating-open-vswitch-to-ludicrous-speed/">implements</a> what Cisco calls <strong>Fast switching</strong>, where the first packet lookup triggers a cache entry to be installed in the kernel-space process to be used by all future packets from the same flow.</p></li>
</ol>


<h2>Detailed packet flow analysis</h2>

<p>Let&rsquo;s start by recapping what we know about our private virtual network. All these values can be obtained from Horizon GUI by examining the private network configuration under Project -> Network -> Networks -> private_network:</p>

<ul>
<li>VM1, IP=10.0.0.8, MAC=fa:16:3e:19:e4:91, port id = <strong>258336bc-4f</strong>38-4bec-9229-4bc76e27f568</li>
<li>VM2, IP=10.0.0.9, MAC=fa:16:3e:ab:1a:bf, port id = <strong>e5f7eaca-1a</strong>36-4b08-aa9b-14e9787f80b0</li>
<li>Router, IP=10.0.0.1, MAC=fa:16:3e:cf:89:47, port id = <strong>96dfc1d3-d2</strong>3f-4d28-a461-fa2404767df2</li>
</ul>


<p>The first 11 characters of port id will be used inside an integration bridge to build the port names, e.g.:</p>

<ul>
<li>tap258336bc-4f - interface connected to VM1</li>
<li>qr-96dfc1d3-d2 - interface connected to the router</li>
</ul>


<h3>Enumerating OVS ports</h3>

<p>In its forwarding entries OVS uses internal port numbers a lot, therefore it would make sense to collect all port number information before we start. This is how it can be done:</p>

<ul>
<li>Use <code>ovs-vsctl show</code> command to collect information about existing port names and their attributes (e.g. dot1q tag, VXLAN tunnel IPs, etc). This is the output collected on compute host #1:</li>
</ul>


<pre><code class="bash ovs-vsctl show | grep -E "Bridge|Port|tag|options"">    Bridge br-tun
        Port br-tun
        Port patch-int
                options: {peer=patch-tun}
        Port "vxlan-0a00020a"
                options: {df_default="true", in_key=flow, local_ip="10.0.1.10", out_key=flow, remote_ip="10.0.2.10"}
        Port "vxlan-0a00030a"
                options: {df_default="true", in_key=flow, local_ip="10.0.1.10", out_key=flow, remote_ip="10.0.3.10"}
    Bridge br-int
        Port br-int
        Port "tap258336bc-4f"
            tag: 5
        Port patch-tun
                options: {peer=patch-int}
</code></pre>

<ul>
<li>Use <code>ovs-ofctl dump-ports-desc &lt;bridge_ID&gt;</code> command to correlate port names and numbers. Example below is for integration bridge of compute host #1:</li>
</ul>


<pre><code class="bash ovs-ofctl dump-ports-desc br-int | grep addr"> 2(patch-tun): addr:5a:c5:44:fc:ac:72
 7(tap258336bc-4f): addr:fe:16:3e:19:e4:91
 LOCAL(br-int): addr:46:fe:10:de:1b:4f
</code></pre>

<p>I&rsquo;ve put together a diagram showing all the relevant integration bridge (<strong>br-int</strong>) and tunnel bridge (<strong>br-tun</strong>) ports on all 3 hosts.</p>

<p><img class="center" src="/images/neutron-port-details.png"></p>

<h3>Unicast frame between VM1 and VM2</h3>

<ol>
<li> Frame enters the br-int on port 7. Default iptables rules allow all outbound traffic from a VM.</li>

<li> Inside the br-int our frame is matched by the "catch-all" rule which triggers the flood-and-learn behaviour:

<figure class='code'><figcaption><span>ovs-ofctl dump-flows br-int</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">table</span><span class="o">=</span>0, <span class="nv">priority</span><span class="o">=</span>10,arp,in_port<span class="o">=</span><span class="m">7</span> <span class="nv">actions</span><span class="o">=</span>resubmit<span class="o">(</span>,24<span class="o">)</span>
</span><span class='line'><span class="nv">table</span><span class="o">=</span>0, <span class="nv">priority</span><span class="o">=</span><span class="m">0</span> <span class="nv">actions</span><span class="o">=</span>NORMAL
</span></code></pre></td></tr></table></div></figure></li>

<li> Since it's a unicast frame the MAC address table is already populated by ARP:

<figure class='code'><figcaption><span>ovs-appctl fdb/show br-int</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> port  VLAN  MAC                Age
</span><span class='line'>    <span class="m">2</span>     <span class="m">5</span>  fa:16:3e:ab:1a:bf    1
</span></code></pre></td></tr></table></div></figure></li>

Target IP address is behind port 2 which is where the frame gets forwarded next.

<li> Inside the tunnel bridge the frame will match three different tables. The first table simply matches the incoming port and resubmits the frame to table 2. Table 2 will match the unicast bit of the MAC address (the least significant bit of the first byte) and resubmit the frame to unicast table 20:

<figure class='code'><figcaption><span>ovs-ofctl dump-flows br-tun</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">table</span><span class="o">=</span>0, <span class="nv">priority</span><span class="o">=</span>1, <span class="nv">in_port</span><span class="o">=</span><span class="m">1</span> <span class="nv">actions</span><span class="o">=</span>resubmit<span class="o">(</span>,2<span class="o">)</span>
</span><span class='line'><span class="nv">table</span><span class="o">=</span>2, <span class="nv">priority</span><span class="o">=</span>0, <span class="nv">dl_dst</span><span class="o">=</span>00:00:00:00:00:00/01:00:00:00:00:00 <span class="nv">actions</span><span class="o">=</span>resubmit<span class="o">(</span>,20<span class="o">)</span>
</span><span class='line'><span class="nv">table</span><span class="o">=</span>20, <span class="nv">priority</span><span class="o">=</span>1, <span class="nv">vlan_tci</span><span class="o">=</span>0x0005/0x0fff,dl_dst<span class="o">=</span>fa:16:3e:ab:1a:bf <span class="nv">actions</span><span class="o">=</span>load:0-&gt;NXM_OF_VLAN_TCI<span class="o">[]</span>,load:0x54-&gt;NXM_NX_TUN_ID<span class="o">[]</span>,output:2
</span></code></pre></td></tr></table></div></figure>

The final match is done on a VLAN tag and destination MAC address. Resulting action is a combination of three consecutive steps:
<ol>
  <li> Clear the dot1q tag - load:0->NXM_OF_VLAN_TCI[]</li>
  <li> Tag the frame with VNI 0x54 - load:0x54->NXM_NX_TUN_ID[]</li>
  <li> Send the frame to compute host 2 - output:2</li>
</ol>
This last match entry is quite interesting in a way that it contains the destination MAC address of VM2, which means this entry was created <b>after</b> the ARP process. In fact, as we'll see in the next step, this entry is populated by a <b>learn</b> action triggered by the ARP response coming from VM2.
</li>

<li>A VXLAN packet arrives at compute host 2 and enters the tunnel bridge through port 2. It's matched on the incoming port and resubmitted to a table where it is assigned with an internal VLAN ID 3 based on the matched tunnel id 0x54. 

<figure class='code'><figcaption><span>ovs-ofctl dump-flows br-tun</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">table</span><span class="o">=</span>0, <span class="nv">priority</span><span class="o">=</span>1,in_port<span class="o">=</span><span class="m">2</span> <span class="nv">actions</span><span class="o">=</span>resubmit<span class="o">(</span>,4<span class="o">)</span>
</span><span class='line'><span class="nv">table</span><span class="o">=</span>4, <span class="nv">priority</span><span class="o">=</span>1,tun_id<span class="o">=</span>0x54 <span class="nv">actions</span><span class="o">=</span>mod_vlan_vid:3,resubmit<span class="o">(</span>,10<span class="o">)</span>
</span><span class='line'><span class="nv">table</span><span class="o">=</span>10, <span class="nv">priority</span><span class="o">=</span><span class="m">1</span> <span class="nv">actions</span><span class="o">=</span>learn<span class="o">(</span><span class="nv">table</span><span class="o">=</span>20,hard_timeout<span class="o">=</span>300,priority<span class="o">=</span>1,NXM_OF_VLAN_TCI<span class="o">[</span>0..11<span class="o">]</span>,NXM_OF_ETH_DST<span class="o">[]=</span>NXM_OF_ETH_SRC<span class="o">[]</span>,load:0-&gt;NXM_OF_VLAN_TCI<span class="o">[]</span>,load:NXM_NX_TUN_ID<span class="o">[]</span>-&gt;NXM_NX_TUN_ID<span class="o">[]</span>,output:NXM_OF_IN_PORT<span class="o">[])</span>,output:1
</span></code></pre></td></tr></table></div></figure>

The last match does two things:
<ol>
  <li> Creates a mirroring entry in table 20 for the reverse packet flow. This is the entry similar to the one we've just seen in step 4.  </li>
  <li> Sends the packet towards the integration bridge.</li>
</ol>
</li>  
<li>The integration bridge checks the local dynamic MAC address table to find the MAC address of VM2(1a:bf).

<figure class='code'><figcaption><span>ovs-appctl fdb/show br-int</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>port  VLAN  MAC                Age
</span><span class='line'>  <span class="m">5</span>     <span class="m">3</span>  fa:16:3e:ab:1a:bf    0
</span></code></pre></td></tr></table></div></figure>
</li>

<li>The frame is checked against the iptables rules configured on port 5 and gets sent up to VM2</li>
</ol>


<h3>Unicast frame to external host (192.168.247.1)</h3>

<ol>
  <li>
    The first 5 steps will be similar to the previous section. The only exception will be that the tunnel bridge of compute host 1 will send the VXLAN packet out port 3 towards the network node.
  </li>
  <li>
    The integration bridge of the network node consults the MAC address table to find the location of the virtual router (89:47) and forwards the packet out port 6.
<figure class='code'><figcaption><span>ovs-appctl fdb/show br-int</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> port  VLAN  MAC                Age
</span><span class='line'>    <span class="m">6</span>     <span class="m">1</span>  fa:16:3e:cf:89:47    1
</span></code></pre></td></tr></table></div></figure>
  </li>
  <li> The virtual router does the route lookup to find the outgoing interface (qg-18bff97b-57)
<figure class='code'><figcaption><span>ip route get 192.168.247.1</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>192.168.247.1 dev qg-18bff97b-57  src 192.168.247.90
</span></code></pre></td></tr></table></div></figure>
Remember that since our virtual router resides in a network namespace all commands must be prepended with <code>ip netns exec qrouter-uuid</code>
  
  </li>
  <li> The virtual router performs the source IP translation to hide the private IP address: 
<figure class='code'><figcaption><span>iptables -t nat -S | grep qg-18bff97b-57</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>-A neutron-l3-agent-POSTROUTING ! -i qg-18bff97b-57 ! -o qg-18bff97b-57 -m conntrack ! --ctstate DNAT -j ACCEPT
</span><span class='line'>-A neutron-l3-agent-snat -o qg-18bff97b-57 -j SNAT --to-source 192.168.247.90
</span></code></pre></td></tr></table></div></figure>
By default all packets will get translated to the external address of the router. For each assigned floating IP address there will be a pair of source/destination NAT entries created in the same table. 

  </li>
  <li> The router consults its local ARP table to find the MAC address of the next hop:
<figure class='code'><figcaption><span>ip neigh show 192.168.247.1</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>192.168.247.1 dev qg-18bff97b-57 lladdr 00:50:56:c0:00:01 DELAY
</span></code></pre></td></tr></table></div></figure>
  </li>
  <li> External bridge receives the frame from the virtual router on port 4, consults its own MAC address table built by ARP and forwards the packet to the final destination.
<figure class='code'><figcaption><span>ovs-appctl fdb/show br-ex</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> port  VLAN  MAC                Age
</span><span class='line'>    <span class="m">4</span>     <span class="m">0</span>  fa:16:3e:5c:90:e0    1
</span><span class='line'>    <span class="m">1</span>     <span class="m">0</span>  00:50:56:c0:00:01    1
</span></code></pre></td></tr></table></div></figure></li>
</ol>


<h3>BUM frame from VM1 for MAC address of VM2</h3>

<ol>
  <li>The integration bridge of the sending host will check the source IP of the ARP packet to make sure it hasn't been spoofed before flooding the packet within the local broadcast domain (VLAN 5).

<figure class='code'><figcaption><span>ovs-ofctl dump-flows br-int</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">table</span><span class="o">=</span>0, <span class="nv">priority</span><span class="o">=</span>10,arp,in_port<span class="o">=</span><span class="m">7</span> <span class="nv">actions</span><span class="o">=</span>resubmit<span class="o">(</span>,24<span class="o">)</span>
</span><span class='line'><span class="nv">table</span><span class="o">=</span>24, <span class="nv">priority</span><span class="o">=</span>2,arp,in_port<span class="o">=</span>7,arp_spa<span class="o">=</span>10.0.0.8 <span class="nv">actions</span><span class="o">=</span>NORMAL
</span></code></pre></td></tr></table></div></figure>
</li>
<li> The flooded packet reaches the tunnel bridge where it goes through 3 different tables. The first table matches the incoming interface, the second table matches the multicast bit of the MAC address.

<figure class='code'><figcaption><span>ovs-ofctl dump-flows br-tun</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">table</span><span class="o">=</span>0, <span class="nv">priority</span><span class="o">=</span>1,in_port<span class="o">=</span><span class="m">1</span> <span class="nv">actions</span><span class="o">=</span>resubmit<span class="o">(</span>,2<span class="o">)</span>
</span><span class='line'><span class="nv">table</span><span class="o">=</span>2, <span class="nv">priority</span><span class="o">=</span>0,dl_dst<span class="o">=</span>01:00:00:00:00:00/01:00:00:00:00:00 <span class="nv">actions</span><span class="o">=</span>resubmit<span class="o">(</span>,22<span class="o">)</span>
</span><span class='line'><span class="nv">table</span><span class="o">=</span>22, <span class="nv">dl_vlan</span><span class="o">=</span><span class="m">5</span> <span class="nv">actions</span><span class="o">=</span>strip_vlan,set_tunnel:0x54,output:3,output:2
</span></code></pre></td></tr></table></div></figure>

The final table swaps the dot1q and VXLAN identifiers and does the source replication by forwarding the packet out ports 2 and 3.
</li>
<li> The following steps are similar to the unicast frame propagation with the exception that the local MAC table of the integration bridges will flood the packet to all interfaces in the same broadcast domain. That means that duplicate ARP requests will reach both the private interface of the virtual router and VM2. The latter, recognising its own IP, will send a unicast ARP response whose source IP will be verified by the ARP spoofing rule of the integration bridge. As the result of that process, both integration bridges on compute host 1 and 2 will populate their local MAC tables with addresses of VM1 and VM2.
</li>
</ol>


<h2>Native OpenStack SDN advantages and limitation</h2>

<p>Current implementation of OpenStack networking has several advantages compared to the traditional SDN solutions:</p>

<ul>
<li>Data-plane learning allows network to function even in the absence of the controller node</li>
<li>Multicast source replication does not rely on multicast support in the underlay network</li>
<li>ARP spoofing protection is the default security setting</li>
</ul>


<p>However at this point it should also be clear that there a number of limitations that can impact the overall network scalability and performance:</p>

<ul>
<li>Multicast source replication creates unnecessary overhead by flooding ARP packets to all hosts</li>
<li>All routed traffic has to go through the network node which becomes a bottleneck for the whole network</li>
<li>There is no ability to control physical devices, even the ones that support OVSDB/Openflow</li>
<li>Network node must be layer 2 adjacent with the external network segment</li>
</ul>


<h2>Things to explore next</h2>

<p>In the following posts I&rsquo;ll continue poking around Neutron and explore a number of features designed to address some of the limitations described above:</p>

<ul>
<li><a href="http://networkop.github.io/blog/2016/05/06/neutron-l2pop/">L2 population</a></li>
<li><a href="https://wiki.openstack.org/wiki/Neutron/DVR">Distributed Virtual Router</a></li>
<li><a href="https://wiki.openstack.org/wiki/Neutron/L2-GW">L2 hardware gateway</a></li>
<li><a href="https://wiki.openstack.org/wiki/Neutron/L3_High_Availability_VRRP">Network High Availability</a></li>
<li><a href="https://wiki.openstack.org/wiki/Neutron/LBaaS">Load-Balancing-as-a-Service</a></li>
<li><a href="https://github.com/openstack/networking-ovn">Open Virtual Network for OVS</a></li>
<li><a href="http://specs.openstack.org/openstack/neutron-specs/specs/mitaka/bgp-dynamic-routing.html">Neutron&rsquo;s dynamic BGP routing</a></li>
</ul>


<h2>C<sub>2</sub>O</h2>

<p>While writing this post I&rsquo;ve compiled a list of commands most useful to query the state of OpenvSwitch. So now, inspired by a similar  IOS to JUNOS (I2J) command <a href="http://www.net-gyver.com/?page_id=1166">conversion tables</a>, I&rsquo;ve put together my own Cisco to OVS conversion table, just for fun.</p>

<table>
<thead>
<tr>
<th> Action </th>
<th> Cisco </th>
<th> OpenvSwitch </th>
</tr>
</thead>
<tbody>
<tr>
<td> Show MAC address table </td>
<td> show mac address-table dynamic   </td>
<td> <code>ovs-appctl fdb/show &lt;bridge_id&gt;</code> </td>
</tr>
<tr>
<td> Clear MAC address table </td>
<td> clear mac address-table dynamic </td>
<td> <code>ovs-appctl fdb/flush &lt;bridge_id&gt;</code> </td>
</tr>
<tr>
<td> Show port numbers </td>
<td> show interface status </td>
<td> <code>ovs-ofctl dump-ports-desc &lt;bridge_ID&gt;</code> </td>
</tr>
<tr>
<td> Show OVS configuration </td>
<td> show run </td>
<td> <code>ovs-vsctl show</code> </td>
</tr>
<tr>
<td> Show packet forwarding rules </td>
<td> show ip route  </td>
<td> <code>ovs-ofctl dump-flows &lt;bridge_id&gt;</code> </td>
</tr>
<tr>
<td> Simulate packet flow </td>
<td> packet-tracer   </td>
<td> <code>ovs-appctl ofproto/trace &lt;bridge_id&gt; in_port=1</code> </td>
</tr>
<tr>
<td> View command history </td>
<td> show archive log config </td>
<td> <code>ovsdb-tool show-log -m</code> </td>
</tr>
</tbody>
</table>


<h2>References</h2>

<p>In this post I have glossed over some details like iptables and DHCP for the sake of brevity and readability. However this post wouldn&rsquo;t be complete if I didn&rsquo;t include references to other resources that contain a more complete, even if at times outdated, overview of OpenStack networking. This is also a way to pay tribute to blogs where I&rsquo;ve learned most of what I was writing about here:</p>

<ul>
<li><a href="https://www.rdoproject.org/networking/networking-in-too-much-detail/">Networking in too much detail</a></li>
<li><a href="http://www.opencloudblog.com/?p=300">Neutron using VXLAN</a></li>
<li><a href="http://docs.ocselected.org/openstack-manuals/kilo/networking-guide/content/under_the_hood_openvswitch.html">Under the hood of OVS</a></li>
<li><a href="https://kimizhang.wordpress.com/2014/04/01/how-ml2vxlan-works/">How ML2/VXLAN works</a></li>
<li><a href="http://docs.openstack.org/openstack-ops/content/network_troubleshooting.html">Official network troubleshooting guide</a></li>
<li><a href="http://therandomsecurityguy.com/openvswitch-cheat-sheet/">OVS commands cheat sheet</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building a Multi-node OpenStack Lab in UNetLab]]></title>
    <link href="http://networkop.github.io/blog/2016/04/18/os-unl-lab/"/>
    <updated>2016-04-18T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/04/18/os-unl-lab</id>
    <content type="html"><![CDATA[<p>In the <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous post</a> I&rsquo;ve demonstrated how to get a working instance of a single-node OpenStack inside <a href="http://www.unetlab.com/">UNetLab</a>. In this post we&rsquo;ll continue building on that by adding two new compute nodes and redesigning our network to resemble something you might actually see in a real life.</p>

<!--more-->


<h2>OpenStack network requirements</h2>

<p>Depending on the number of deployed <a href="https://www.openstack.org/software/project-navigator/">components</a>, OpenStack physical network requirements could be different. In our case we&rsquo;re not going to deploy any storage solution and simply use the <strong>ephemeral</strong> storage, i.e. hard disk that&rsquo;s a part of a virtual machine. However, even in minimal installations, there are a number of networks that should be considered individually due to different connectivity requirements:</p>

<ul>
<li><p>Server <abbr title=" Out-Of-Band">OOB</abbr> <strong>management</strong> network - this is usually a dedicated physical network used mainly for server bootstrapping and OS deployment. It is a Layer 3 network with DHCP relays configured at each edge L3 interface and access to Internet package repositories.</p></li>
<li><p><strong>API</strong> network - used for internal communication between various OpenStack services. This can be a routed network without Internet access. The only requirement is any-to-any reachability within a single OpenStack environment.</p></li>
<li><p><strong>External</strong> network - used for public access to internal OpenStack virtual machines. This is the <em>outside</em> of OpenStack, with a pool of IP addresses used to NAT the internal IPs of public-facing virtual machines. This network <strong>must</strong> be Layer 2 adjacent <strong>only</strong> with a network control node.</p></li>
<li><p><strong>Tenant</strong> network - used for communication between virtual machines within OpenStack environment. Thanks to the use of VXLAN overlay, this can be a simple routed network that has any-to-any reachability between all Compute and Network nodes.</p></li>
</ul>


<h2>Building a lab network</h2>

<p>For labbing purposes it&rsquo;s possible to relax some of the above network requirements without seriously affecting the outcomes of our simulation. For example, it&rsquo;s possible to combine some of the networks and still satisfy the requirements stated above. These are the networks that will be configured inside UNetLab:</p>

<ul>
<li><p><strong>Management</strong> - this network will combine the functions of OOB and API networks. To isolate it from our data centre underlay I&rsquo;ll be using separate interfaces on virtual machines and connect them directly to Workstation&rsquo;s NAT interface (192.168.91.0/24 in my case)  to give them direct access to Internet.</p></li>
<li><p><strong>External</strong> - this network will be connected to Workstation&rsquo;s host-only NIC (192.168.247.0/24) through Vlan300 configured on one of the leaf switches. Since it must be L2 adjacent with the network control node our leaf switch will not perform any routing for this subnet.</p></li>
<li><p><strong>Tenant</strong> - this will be a routed leaf/spine <a href="https://en.wikipedia.org/wiki/Clos_network">Clos</a> fabric comprised of 3 leaf and 2 spine switches running a single-area OSPF process on all their links. Each server will have its own unique tenant subnet (Vlan100) terminated on the leaf switch and subnet injected into OSPF. The subnet used for this Vlan is going to be <code>10.0.X.0/24</code>, where X is the number of the leaf switch terminating the vlan.</p>

<p>  The links between switches are all L3 point-to-point with addresses borrowed from 169.254.0.0/16 range specifically to emphasize the fact that the internal addressing does not need to be known or routed outside of the fabric. The <strong>sole function of the fabric</strong> is to provide multiple equal cost paths between any pair of leafs, thereby achieving maximum link utilisation. Here&rsquo;s an example of a traceroute between Vlan100&rsquo;s of Leaf #1 and Leaf #3.</p></li>
</ul>


<pre><code class="text Traceroute inside an ECMP routed Clos fabric">L3#traceroute 10.0.1.1 source 10.0.3.1
Type escape sequence to abort.
Tracing the route to 10.0.1.1
VRF info: (vrf in name/id, vrf out name/id)
  1 169.254.31.111 1 msec
    169.254.32.222 0 msec
    169.254.31.111 0 msec
  2 169.254.12.1 1 msec
    169.254.11.1 1 msec *
</code></pre>

<h2>Building lab servers</h2>

<p>Based on my experience a standard server would have at least 3 physical interfaces - one for OOB management and a pair of interfaces for application traffic. The two application interfaces will normally be combined in a single <abbr title=" Link Aggregation Group">LAG</abbr> and connected to a pair of MLAG-capable TOR switches. Multi-chassis LAG or <a href="http://blog.ipspace.net/2010/10/multi-chassis-link-aggregation-basics.html">MLAG</a> is a pretty old and well-understood technology so I&rsquo;m not going to try and simulate it in the lab. Instead I&rsquo;ll simply assume that a server will be connected to a TOR switch via a single physical link. That link will be setup as a dot1q trunk to allow for multiple subnets to share it.</p>

<h2>Physical lab topology</h2>

<p>All the above requirements and assumptions result in the following topology that we need to build inside UNetLab:</p>

<p><img class="center" src="/images/neutron-native.png"></p>

<p>For servers I&rsquo;ll be using OpenStack node type that I&rsquo;ve described in my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous post</a>. The two compute nodes do not need as much RAM as the control node, so I&rsquo;ll reduce it to just 2GB.</p>

<p>For switches I&rsquo;ll be using a Cisco&rsquo;s <a href="http://www.unetlab.com/2014/11/adding-cisco-iouiol-images/">L2 IOU</a> image for now, mainly due to the low resource requirements. In the future I&rsquo;ll try and swap it for something else. As you can see from the sample config below, fabric configuration is very basic and can be easily replaced by any other solution:</p>

<pre><code class="text Leaf 1 configuration">interface Ethernet0/0
 no switchport
 ip address 169.254.11.1 255.255.255.0
 ip ospf network point-to-point
 duplex auto
!
interface Ethernet0/1
 no switchport
 ip address 169.254.12.1 255.255.255.0
 ip ospf network point-to-point
 duplex auto
!
interface Ethernet0/2
 switchport trunk allowed vlan 100
 switchport trunk encapsulation dot1q
 switchport mode trunk
!
interface Vlan100
 ip address 10.0.1.1 255.255.255.0
!
router ospf 1
 network 0.0.0.0 255.255.255.255 area 0
</code></pre>

<h2>Server configuration and OpenStack installation</h2>

<p>Refer to my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous blogpost</a> for instructions on how to install OpenStack and follow the first 5 steps from &ldquo;Installing CentOS and Openstack&rdquo; section. Before doing the final step, we need to configure our VMs' new interfaces:</p>

<ul>
<li>Remove any IP configuration from <strong>eth1</strong> interface to make it look like this:</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/ifcfg-eth1">TYPE=Ethernet
BOOTPROTO=none
DEVICE=eth1
ONBOOT=yes
</code></pre>

<ul>
<li>Configure <strong>Tenant network</strong>:</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/ifcfg-eth1.100">VLAN=yes
DEVICE=eth1.100
BOOTPROTO=none
IPADDR=10.0.X.10
PREFIX=24
ONBOOT=yes
</code></pre>

<ul>
<li>Setup a <strong>static route</strong> to all other leaf nodes:</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/route-eth1.100">10.0.0.0/8 via 10.0.X.1
</code></pre>

<ul>
<li>On Control node setup <strong>External network</strong> interface</li>
</ul>


<pre><code class="ini /etc/sysconfig/network-scripts/route-eth1.300">DEVICE=eth1.300
IPADDR=192.168.247.100
PREFIX=24
ONBOOT=yes
BOOTPROTO=none
VLAN=yes
</code></pre>

<p>Now we&rsquo;re ready to kick off OpenStack installation. This can be done with a single command that needs to be executed on the Control node. Note that <code>eth1.100</code> interface is spelled as <code>eth1_100</code> in the last line.</p>

<pre><code class="bash Controller">packstack --allinone \
    --os-cinder-install=n \
    --os-ceilometer-install=n \
    --os-trove-install=n \
    --os-ironic-install=n \
    --nagios-install=n \
    --os-swift-install=n \
    --os-gnocchi-install=n \
    --os-aodh-install=n \
    --os-neutron-ovs-bridge-mappings=extnet:br-ex \
    --os-neutron-ovs-bridge-interfaces=br-ex:eth1.300 \
    --os-neutron-ml2-type-drivers=vxlan,flat \
    --provision-demo=n \
    --os-compute-hosts=192.168.91.10,192.168.91.11,192.168.91.12 \
    --os-neutron-ovs-tunnel-if=eth1_100
</code></pre>

<h2>Creating a virtual network for a pair of VMs</h2>

<p>Once again, follow all steps from &ldquo;Configuring Openstack networking&rdquo; section of my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous blogpost</a>. Only this time when setting up a public subnet, update the subnet details to match our current environment:</p>

<pre><code class="bash Step 3 - Creating a public subnet">  neutron subnet-create --name public_subnet \
    --enable_dhcp=False \
    --allocation-pool=start=192.168.247.90,end=192.168.247.126 \
    --gateway=192.168.247.1 external_network 192.168.247.0/24
</code></pre>

<h2>Nova-scheduler and setting up host aggregates</h2>

<p>OpenStack&rsquo;s Nova project is responsible for managing virtual machines. Nova controller views all available compute nodes as a single pool of resources. When a new VM is to be instantiated, a special process called nova-scheduler examines all available compute nodes and selects the &ldquo;best&rdquo; one based on a special algorithm, which normally takes into account amount of RAM, CPU and other host capabilities.</p>

<p>To make our host selection a little bit more deterministic, we can define a group of compute servers via <strong>host aggregates</strong>, which will be used by nova-scheduler in its selection algorithm. Normally it could include all servers in a single rack or a row of racks. In our case we&rsquo;ll setup two host aggregates each with a single compute host. This way we&rsquo;ll be able to select exactly which compute host to use when instantiating a new virtual machine.</p>

<p>To setup it up, from Horizon&rsquo;s dashboard navigate to Admin -> System and create two host aggregates <strong>comp-1</strong> and <strong>comp-2</strong>, each including a single compute host.</p>

<h2>Creating workloads and final testing</h2>

<p>Using a process described in &ldquo;Spinning up a VM&rdquo; section of my <a href="http://networkop.github.io/blog/2016/04/04/openstack-unl/">previous blogpost</a>, create a couple of virtual machines assigning them to different host aggregates created earlier.</p>

<h2>Security and Remote access</h2>

<p>To access these virtual machines we need to give them a <a href="https://www.rdoproject.org/networking/floating-ip-range/">floating</a> ip address from the External subnet range. To do that navigate to Project -> Compute -> Instances and select <strong>Associate Floating IP</strong> from the Actions drop-down menu.</p>

<p>The final steps is to allow remote SSH access. Each new VM inherits ACLs from a default security group. So the easiest way to allow SSH is to go to Project -> Compute -> Access &amp; Security and add a rule to allow inbound SSH connections for the default security group.</p>

<h2>Verification</h2>

<p>At this stage you should be able to SSH into the floating IP addresses assigned to the two new VMs using the default credentials. Feel free to poke around and explore Horizon&rsquo;s interface a bit more. For example, try setting up an SSH key pair and re-build our two VMs to allow passwordless SSH access.</p>

<h2>What to expect next</h2>

<p>In the next post we&rsquo;ll explore some of the basic concepts of OpenStack&rsquo;s SDN. We&rsquo;ll peak inside the internal implementation of virtual networks and see what are some of their limitations and drawbacks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Openstack on UNetlab]]></title>
    <link href="http://networkop.github.io/blog/2016/04/04/openstack-unl/"/>
    <updated>2016-04-04T00:00:00-07:00</updated>
    <id>http://networkop.github.io/blog/2016/04/04/openstack-unl</id>
    <content type="html"><![CDATA[<p>In this post I&rsquo;m going to show how to get a running instance of Openstack inside a UNetLab virtual machine.</p>

<!--more-->


<p><img class="center" src="/images/unl-os.png"></p>

<h2>What the hell am I trying to do?</h2>

<p>I admit that running Openstack on anything other than baremetal is nonsense. So why would anyone want to run it with two layers of virtualisation underneath? My goal is to explore some of the new SDN/NFV technologies without leaving the confines on my home area network and/or racking up a triple-digit electricity bill. I also wanted to be able to swap underlay networks without spending hours trying to plumb together virtualized switches and servers from multiple vendors. That&rsquo;s why I&rsquo;ve decided to use UNetLab VM as a host for my Openstack lab. This would allow me to easily assemble any type of underlay, WAN or DCI network and with hardware virtualisation support I can afford to run Openstack double-nested inside Workstation and Qemu on my dual-core i7 without too much of a performance penalty. After all, <a href="https://www.ravellosystems.com/technology/hvx">some companies</a> even managed to turn similar things into a commercial product.</p>

<p>My interest in Openstack is strictly limited by networking, that&rsquo;s why a lot of the things you&rsquo;ll see in this and following posts will not be applicable to a real-life production environment. However, as far as networking is concerned, I&rsquo;ll try to stick as close to the official Openstack <a href="http://docs.openstack.org/openstack-ops/content/example_architecture.html">network design</a> as possible. I&rsquo;ll be using <a href="https://www.rdoproject.org">RDO</a> to deploy Openstack. The specific method will be Packstack which is a collection of Puppet modules used to deploy Openstack components.</p>

<p>Why have I not went the OpenDaylight/Mininet way if I wanted to play with SDN/NFV? Because I wanted something more realistic to play with, that wouldn&rsquo;t feel like vendor&rsquo;s powerpoint presentation. Plus there&rsquo;s plenty of resources on the &lsquo;net about it anyway.</p>

<p>So, without further ado, let&rsquo;s get cracking.</p>

<h2>Setting the scene</h2>

<p>On my Windows 8 laptop I&rsquo;ve got a UNL virtual machine running inside a VMWare Workstation.. I&rsquo;ve <a href="http://www.unetlab.com/download/">downloaded</a> and <a href="http://www.unetlab.com/2014/11/upgrade-unetlab-installation/">upgraded</a> a pre-built UNL VM image. I&rsquo;ve also downloaded a copy of the <a href="http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.iso">Centos 7 minimal ISO image</a> and copied it over to my UNL VM&rsquo;s home directory.</p>

<p>For network access I&rsquo;ll be using VMWware Workstation&rsquo;s NAT interface. It&rsquo;s currently configured with <code>192.168.91.0/24</code> subnet with DHCP range of <code>.128-.254</code>. Therefore I&rsquo;ll be using <code>.10-.126</code> to allocate IPs to my Openstack servers.</p>

<h2>Creating a custom node type in UNL</h2>

<p>Every node type inside UNL has its own unique settings. Some settings, like amount of RAM, CPU or number of network interfaces, can be changed during node instantiation, while some of them remain &ldquo;baked in&rdquo;. Say, for example, the default &ldquo;Linux&rdquo; template creates nodes with default <strong>Qemu Virtual CPU</strong> which doesn&rsquo;t support the hardware virtualisation (<strong>VT-X/AMD-V</strong>) <a href="http://docs.openstack.org/liberty/config-reference/content/kvm.html">required</a> by Openstack. In order to change that you can either edit the existing node template or follow these steps to create a new one:</p>

<ol>
<li><p>Add Openstack node definition to initialization file <code>/opt/unetlab/html/includes/init.php</code>.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='php'><span class='line'> <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">linux</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;</span>                 <span class="o">=&gt;</span>      <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">Linux</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;,</span>
</span><span class='line'> <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">openstack</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;</span>             <span class="o">=&gt;</span>      <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">Openstack</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;,</span>
</span><span class='line'> <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">mikrotik</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;</span>              <span class="o">=&gt;</span>      <span class="o">&amp;</span><span class="nx">lsquo</span><span class="p">;</span><span class="nx">MikroTik</span> <span class="nx">RouterOS</span><span class="o">&amp;</span><span class="nx">rsquo</span><span class="p">;,</span>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a new Openstack node template based on existing linux node template.</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>cp /opt/unetlab/html/templates/linux.php /opt/unetlab/html/templates/openstack.php
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Edit the template file replacing all occurences of &lsquo;Linux&rsquo; with &lsquo;Openstack&rsquo;</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/Linux/Openstack/g<span class="p">;</span> s/linux/openstack/g<span class="err">&#39;</span> /opt/unetlab/html/templates/openstack.php
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Edit the template file to double the RAM and CPU and pass all host&rsquo;s CPU instructions to Openstack nodes</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/2048/4096/<span class="p">;</span> s/<span class="o">(</span>cpu.*<span class="o">)</span> <span class="o">=</span> 1/<span class="se">\1</span> <span class="o">=</span> 2/<span class="p">;</span> s/<span class="o">(</span><span class="nv">order</span><span class="o">=)</span>dc/<span class="se">\1</span>cd -cpu host/<span class="p">&amp;</span>lsquo<span class="p">;</span> /opt/unetlab/html/templates/openstack.php
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<p>At this point you should be able to navigate to UNL&rsquo;s web interface and find a new node of type Openstack. However you won&rsquo;t be able to create it until you have at least one image, which is what we&rsquo;re going to build next.</p>

<h2>Building a Linux VM inside UNetLab</h2>

<p>Now we need to create a CentOS image inside a UNL. One way to do it is build it inside a VMWare Workstation, copy it to UNL and convert the <strong>.vmdk</strong> to <strong>.qcow2</strong>. However, when I tried doing this I ran into a problem with CentOS not finding the correct disk partitions during bootup. The workaround was to boot into rescue mode and rebuild the initramfs. For those feeling adventurous, I would recommend checking out the following links [<a href="https://wiki.centos.org/TipsAndTricks/CreateNewInitrd">1</a>, <a href="http://advancelinux.blogspot.com.au/2013/06/how-to-rebuild-initrd-or-initramfs-in.html">2</a>, <a href="http://forums.fedoraforum.org/showthread.php?t=288020">3</a>] before trying this option.<br/>
The other option is to build CentOS inside UNL from scratch. This is how you can do it:</p>

<ol>
<li><p>Create a new directory for Openstack image</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>mkdir -p /opt/unetlab/addons/qemu/openstack-1
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a link to CentOS ISO boot image from our new directory</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>ln -s ~/CentOS-7-x86_64-Minimal-1511.iso /opt/unetlab/addons/qemu/openstack-1/cdrom.iso
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a blank 6Gb disk image</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>/opt/qemu/bin/qemu-img create -f qcow2 -o <span class="nv">preallocation</span><span class="o">=</span>metadata /opt/unetlab/addons/qemu/openstack-1/virtioa.qcow2 6G
</span></code></pre></td></tr></table></div></figure></p>

<p>If you want to create <strong>snapshots</strong> at any stage of the process you&rsquo;d need to use a copy of this file under /opt/unetlab/tmp/pod_id/lab_uuid/node_id/ directory</p></li>
</ol>


<p>Now you should be able to successfully create an Openstack node and connect it to Internet. Create a new network that would have Internet connectivity (in my case it&rsquo;s <strong>pnet0</strong>) and connect it to Openstack&rsquo;s <strong>eth0</strong>.  At this stage we have everything ready to start installing Openstack, but before we move on let me take a quick detour to tell you about my ordeals with VNC integration.</p>

<h2>Optional: Integrating TightVNC with UNL</h2>

<p>For some unknown reason UltraVNC does not work well on my laptop. My sessions would often crash or start minimised with the only option to close the window. That&rsquo;s not the only thing not working properly on my laptop thanks to the corporate policies with half of the sh*t locked down for <em>security</em> reasons.<br/>
So instead of mucking around with <strong>Ultra</strong> I decided to give me old pal <strong>Tight</strong>VNC a go. The setup process is very similar to the <a href="http://www.unetlab.com/2015/03/url-telnet-ssh-vnc-integration-on-windows/">official VNC integration guide</a> with the following exceptions:</p>

<ol>
<li><p>The wrapper file simply strips the leading &lsquo;vnc://&rsquo; and trailing &lsquo;/&rsquo; off the passed argument</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='winbatch'><span class='line'><span class="k">SET</span> <span class="nv">arg</span><span class="o">=</span><span class="nv">%1</span>
</span><span class='line'><span class="k">SET</span> <span class="nv">arg</span><span class="o">=</span><span class="nv">%arg:~6%</span>
</span><span class='line'><span class="k">SET</span> <span class="nv">arg</span><span class="o">=</span><span class="nv">%arg:~0</span><span class="p">,</span><span class="m">-1</span>%
</span><span class='line'>start &amp;ldquo;&amp;rdquo; &amp;ldquo;c:\Program Files\TightVNC\tvnviewer.exe&amp;rdquo; <span class="nv">%arg%</span>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>The registry entry now points to the TightVNC wrapper</p>

<p><figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='registry'><span class='line'><span class="k">[</span><span class="nb">HKEY_CLASSES_ROOT</span><span class="k">\vnc\shell\open\command]</span>
</span><span class='line'><span class="na">@</span><span class="o">=</span><span class="s">&amp;ldquo;\&quot;c:\Program Files\TightVNC\wrapper.bat\&amp;rdquo; %1&quot;</span>
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<h2>Installing CentOS and Openstack</h2>

<p>Finally, we&rsquo;ve got all our ducks lined up in a row and we&rsquo;re ready to shoot. Fire up the Openstack node inside UNL and click on it to open a vnc session. Proceed to install CentOS with default options. You need to confirm which <strong>hard disk</strong> to use and setup the <strong>hostname</strong> and the <strong>root password</strong> during installation process.
As I mentioned earlier, we&rsquo;ll be using RDO&rsquo;s Packstack to deploy all the necessary Openstack components. The whole installation process will be quite simple and can be found on the RDO&rsquo;s <a href="https://www.rdoproject.org/install/quickstart/">quickstart page</a>. Here is my slightly modified version of installation process:</p>

<ol>
<li><p>Disable Network Manager and SELinux.</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>service NetworkManager stop
</span><span class='line'> <span class="nv">$ </span>systemctl disable NetworkManager.service
</span><span class='line'> <span class="nv">$ </span>setenforce 0
</span><span class='line'> <span class="nv">$ </span>sed -i <span class="p">&amp;</span>rsquo<span class="p">;</span>s/enforcing/permissive/<span class="p">&amp;</span>lsquo<span class="p">;</span> /etc/selinux/config
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Configure static IP address <code>192.168.91.10</code> on the network interface. <br/>
 Assuming your interface name is <code>eth0</code> make sure you /etc/sysconfig/network-scripts/ifcfg-eth0 looks something like this:</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">TYPE</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>Ethernet<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">BOOTPROTO</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>static<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">IPADDR</span><span class="o">=</span>192.168.91.10
</span><span class='line'> <span class="nv">PREFIX</span><span class="o">=</span>24
</span><span class='line'> <span class="nv">GATEWAY</span><span class="o">=</span>192.168.91.2
</span><span class='line'> <span class="nv">DNS1</span><span class="o">=</span>192.168.91.2
</span><span class='line'> <span class="nv">NAME</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>eth0<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">DEVICE</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>eth0<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'> <span class="nv">ONBOOT</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>yes<span class="p">&amp;</span>rdquo<span class="p">;</span>&lt;br/&gt;
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Issue a <code>service network restart</code> and reconnect to the new static IP address. Make sure that you still have access to Internet after making this change.</p></li>
<li><p>Setup RDO repositories</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>sudo yum install -y &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://rdoproject.org/repos/rdo-release.rpm&quot;</span>&gt;https://rdoproject.org/repos/rdo-release.rpm&lt;/a&gt;
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Update your current packages</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>sudo yum update -y
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Install Packstack</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>sudo yum install -y openstack-packstack
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> That&rsquo;s where it&rsquo;d make sense to take a snapshot with <code>qemu-img snapshot -c pre-install virtioa.qcow2</code> command</p></li>
<li><p>Deploy a single-node Openstack environment</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span>packstack --allinone <span class="se">\</span>
</span><span class='line'> --os-cinder-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-ceilometer-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-trove-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-ironic-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --nagios-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-swift-install<span class="o">=</span>n <span class="se">\</span>
</span><span class='line'> --os-neutron-ovs-bridge-mappings<span class="o">=</span>extnet:br-ex <span class="se">\</span>
</span><span class='line'> --os-neutron-ovs-bridge-interfaces<span class="o">=</span>br-ex:eth0 <span class="se">\</span>
</span><span class='line'> --os-neutron-ml2-type-drivers<span class="o">=</span>vxlan,flat <span class="se">\</span>
</span><span class='line'> --provision-demo<span class="o">=</span>n
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Here we&rsquo;re overriding some of the default Packstack options. We&rsquo;re not installing some of the components we&rsquo;re not going to use and setting up a name (<strong>extnet</strong>) for our external physical segment, which we&rsquo;ll use in the next section.</p></li>
</ol>


<p>At the end of these 4 steps you should be able to navigate to Horizon (Openstack&rsquo;s dashboard) by typing <code>http://192.168.91.10</code> in your browser. You can find login credentials in the <code>~/keystonerc_admin</code> file.</p>

<h2>Configuring Openstack networking</h2>

<p>At this stage we need to setup virtual networking infrastructure inside Openstack. This will be almost the same as described in RDO&rsquo;s external network <a href="https://www.rdoproject.org/networking/neutron-with-existing-external-network/">setup guide</a>. The only exceptions will be the <a href="https://www.rdoproject.org/networking/difference-between-floating-ip-and-private-ip/">floating IP range</a>, which will match our existing environment, and the fact that we&rsquo;re no going to setup any additional tenants yet. This is how our topology will look like:</p>

<p><img class="center" src="/images/os-net-1.png"></p>

<ol>
<li><p>Switch to Openstack&rsquo;s <code>admin</code> user
 <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> <span class="nv">$ </span><span class="nb">source</span> ~/keystonerc_admin
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create external network
 <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron net-create external_network --provider:network_type flat <span class="se">\</span>
</span><span class='line'> --provider:physical_network extnet  <span class="se">\</span>
</span><span class='line'> --router:external <span class="se">\</span>
</span><span class='line'> --shared
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a public subnet</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron subnet-create --name public_subnet <span class="se">\</span>
</span><span class='line'> --enable_dhcp<span class="o">=</span>False <span class="se">\</span>
</span><span class='line'> --allocation-pool<span class="o">=</span><span class="nv">start</span><span class="o">=</span>192.168.91.90,end<span class="o">=</span>192.168.91.126 <span class="se">\</span>
</span><span class='line'> --gateway<span class="o">=</span>192.168.91.2 external_network 192.168.91.0/24
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p> Default gateway is VMware&rsquo;s NAT IP address</p></li>
<li><p>Create a private network and subnet</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron net-create private_network
</span><span class='line'> neutron subnet-create --name private_subnet private_network 10.0.0.0/24 <span class="se">\</span>
</span><span class='line'> --dns-nameserver 8.8.8.8
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p>

<p>  This network is not routable outside of Openstack and is used for inter-VM communication</p></li>
<li><p>Create a virtual router and attach it to both networks</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> neutron router-create router
</span><span class='line'> neutron router-gateway-set router external_network
</span><span class='line'> neutron router-interface-add router private_subnet
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<p>Make sure to check out the visualisation of our newly created network topology in Horizon, it&rsquo;s amazing.</p>

<h2>Spinning up a VM</h2>

<p>There&rsquo;s no point in installing Openstack just for the sake of it. Our final step would be to create a working virtual machine that would be able to connect to Internet.</p>

<ol>
<li><p>Download a test linux image</p>

<p> <figure class='code panel panel-default'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> curl &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img&quot;</span>&gt;http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img&lt;/a&gt; <span class="p">|</span> glance <span class="se">\</span>
</span><span class='line'> image-create --name<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>cirros image<span class="p">&amp;</span>rsquo<span class="p">;</span> <span class="se">\</span>
</span><span class='line'> --visibility<span class="o">=</span>public <span class="se">\</span>
</span><span class='line'> --container-format<span class="o">=</span>bare <span class="se">\</span>
</span><span class='line'> --disk-format<span class="o">=</span>qcow2
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>From Horizon&rsquo;s home page navigate to Project -> Compute -> Images.</p></li>
<li><p>Click on <code>Launch Instance</code> and give the new VM a name.</p></li>
<li><p>Make sure it&rsquo;s attached to <code>private_network</code> under the Networking tab.</p></li>
<li><p>Less then a minute later the status should change to <code>Active</code> and you can navigate to VM&rsquo;s console by clicking on its name and going to <code>Console</code> tab.</p></li>
<li><p>Login using the default credentials (<strong>cirros/cubswin:)</strong>) and verify Internet access by pinging google.com.</p></li>
</ol>


<p>Congratulations, we have successfully created a VM running inside a KVM inside a KVM inside a VMWare Workstation inside Windows!</p>

<h2>What to expect next</h2>

<p>Unlike my other post series, I don&rsquo;t have a clear goal at this stage so I guess I&rsquo;ll continue playing around with different underlays for multi-node Openstack and then move on to various SDN solutions available like OpenDayLight and OpenContrail. Unless I lose interest half way through, which happened in the past. But until that happens, stay tuned for more.</p>
]]></content>
  </entry>
  
</feed>
